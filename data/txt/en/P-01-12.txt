Towards Scene Text Translation for Complex Writing Systems

Hour Kaing

1

Haiyue Song

1

Chenchen Ding

1

Jiannan Mao

2

Hideki Tanaka

1

Masao Utiyama

11

ASTREC, UCRI, NICT, Japan

2

Gifu University, Gifu, Japan

1

{hour kaing, haiyue.song, chenchen.ding, hideki.tanaka, mutiyama}@nict.go.jp

2

mao@mat.info.gifu-u.ac.jp



Abstract

Scene text translation aims to automatically translate textin images or videos while preserving its visual features.
Inthis work, we focus on scene text translation for complexwriting system by taking Japanese as a typical example.
We build a pipeline to translate from English to Japanese,leveraging publicly available modules for text detection,recognition, and translation, and train our own text replace-ment model specialized for English-to-Japanese transfor-mations.
Experiments show that the system can eﬀectivelygenerate translated text in Japanese while retaining muchof the original style, although background regeneration andhandling of Kanji remain open challenges.


1 Introduction

When watching a foreign movie, untranslated on-screentext in the background often hinders understanding of thescene, as shown in Figure 1.
Translating such text andplacing it in the correct position with similar style typicallyrequires signiﬁcant manual eﬀort.
Scene text translation systems
[1](also referred to ascross-language text editing systems
[2]) provide an au-tomatic solution by translating the source text in videoscenes into the target language while preserving the vi-sual features of the original text, such as its location, font,and background.
This is typically achieved by integratingscene text detection and recognition, machine translation,and scene text replacement modules.
However, translating scene text into complex writingsystems is challenging.
Japanese can be regarded as atypical example of a complex writing system, which en-compasses thousands of distinct characters across multipleforms (e.g., Kanji, Hiragana, Katakana).To address this, we train a specialized Japanese text re-placement module by 1) synthesizing 100k cross-lingualFigure 1
An example of translating scene text from Englishto Japanese in videos, while preserving the original position andstyle.text images from English to Japanese, and 2) ﬁne-tuningthe English SRNet model
[2] on our synthetic dataset.
Forother modules in our English-to-Japanese scene text trans-lation system, we leverage publicly available models in-cluding the FAST text detection model [3], the CRNN textrecognition model [4], and the NLLB200 machine transla-tion model [5].Experiments show that our model performs better thanbaselines in generating Japanese text but underperformsthem in background regeneration.
We also found that gen-erating Kanji characters is more challenging than generat-ing Katakana or Hiragana characters.


2 Background

We introduce the modules involved in our scene texttranslation system, including detecting and recognizingthe English text in each frame (Section 2.1), translatingit into the target language (Section 2.2), and placing itback into the original position with similar visual features(Section 2.3).
Previous scene text translation systems areintroduced in Section 2.4.

2.1 Text Detection and Recognition

Scene text detection aims to identify text regions in nat-ural (often noisy) scenes
[3, 6, 7, 8].
We adopt the FAST(faster arbitrarily-shaped text detector)[3] system, whichachieves real-time, high-accuracy detection for curved textand supports multiple languages including Japanese.― 234 ―

Towards Scene Text Translation for Complex Writing Systems

Hour Kaing

1

Haiyue Song

1

Chenchen Ding

1

Jiannan Mao

2

Hideki Tanaka

1

Masao Utiyama

11

ASTREC, UCRI, NICT, Japan

2

Gifu University, Gifu, Japan

1

{hour kaing, haiyue.song, chenchen.ding, hideki.tanaka, mutiyama}@nict.go.jp

2

mao@mat.info.gifu-u.ac.jp



Abstract

Scene text translation aims to automatically translate textin images or videos while preserving its visual features.
Inthis work, we focus on scene text translation for complexwriting system by taking Japanese as a typical example.
We build a pipeline to translate from English to Japanese,leveraging publicly available modules for text detection,recognition, and translation, and train our own text replace-ment model specialized for English-to-Japanese transfor-mations.
Experiments show that the system can eﬀectivelygenerate translated text in Japanese while retaining muchof the original style, although background regeneration andhandling of Kanji remain open challenges.


1 Introduction

When watching a foreign movie, untranslated on-screentext in the background often hinders understanding of thescene, as shown in Figure 1.
Translating such text andplacing it in the correct position with similar style typicallyrequires signiﬁcant manual eﬀort.
Scene text translation systems
[1](also referred to ascross-language text editing systems
[2]) provide an au-tomatic solution by translating the source text in videoscenes into the target language while preserving the vi-sual features of the original text, such as its location, font,and background.
This is typically achieved by integratingscene text detection and recognition, machine translation,and scene text replacement modules.
However, translating scene text into complex writingsystems is challenging.
Japanese can be regarded as atypical example of a complex writing system, which en-compasses thousands of distinct characters across multipleforms (e.g., Kanji, Hiragana, Katakana).To address this, we train a specialized Japanese text re-placement module by 1) synthesizing 100k cross-lingualFigure 1
An example of translating scene text from Englishto Japanese in videos, while preserving the original position andstyle.text images from English to Japanese, and 2) ﬁne-tuningthe English SRNet model
[2] on our synthetic dataset.
Forother modules in our English-to-Japanese scene text trans-lation system, we leverage publicly available models in-cluding the FAST text detection model [3], the CRNN textrecognition model [4], and the NLLB200 machine transla-tion model [5].Experiments show that our model performs better thanbaselines in generating Japanese text but underperformsthem in background regeneration.
We also found that gen-erating Kanji characters is more challenging than generat-ing Katakana or Hiragana characters.


2 Background

We introduce the modules involved in our scene texttranslation system, including detecting and recognizingthe English text in each frame (Section 2.1), translatingit into the target language (Section 2.2), and placing itback into the original position with similar visual features(Section 2.3).
Previous scene text translation systems areintroduced in Section 2.4.

2.1 Text Detection and Recognition

Scene text detection aims to identify text regions in nat-ural (often noisy) scenes
[3, 6, 7, 8].
We adopt the FAST(faster arbitrarily-shaped text detector)[3] system, whichachieves real-time, high-accuracy detection for curved textand supports multiple languages including Japanese.― 234 ―Tex t  Dete ct ionTex t  Reco gn
it
ionMachine TranslationTex t ReplacementTex t  Inser ti onTransformationFigure 2 Scene Text Translation Pipeline.
Scene text recognition then extracts text from the de-tected sub-images.
This is typically done with opticalcharacter recognition (OCR) models such as CRNN [4],SSDAN [9], or PaddleOCR.1）In our work, we use theCRNN
[4] system because it handles sequences withoutexplicit character segmentation and supports non-Latinscripts such as Japanese or Chinese.


2.2 Machine Translation

Next, the extracted source text is translated into the targetlanguage through a separate neural machine translation(NMT) system [5, 10, 11].
We employ the NLLB200
[5]model, a state-of-the-art multilingual NMT system thatprovides high-quality English to Japanese translation.
Although multimodal machine translation (MMT) thatuses images [12, 13] or videos
[14, 15] as assisting infor-mation can yield better performance, we adopt an eﬃcienttext-to-text MT approach for real-time video translation.


2.3 Scene Text Replacement

Scene text replacement aims to edit the text in an imageby replacing it with new text while preserving the originaltext style and background information.
The model typi-cally takes two inputs: a scene text image to be edited andthe new text to be inserted.
It then outputs a new image con-taining the new text.
Previous works include SRNet
[2],STEFANN [16], SwapText [17], and STRIVE [18].
Weapply the SRNet architecture, which comprise a text con-version module, a background inpainting module, and afusion module, since its code is publicly available.2）1） https://github.com/PaddlePaddle/PaddleOCR2）
https://github.com/lksshw/SRNet

2.4 Scene Text Translation

Scene text translation has been explored in both re-search [1, 19, 20] and commercial applications (e.g.,Google Translate’s Camera mode).
However, previous sys-tems [19, 20] typically focus on image translation withoutadapting text style, or are limited to Indic scripts such asEnglish to Hindi [1].Our paper presents the ﬁrst study to explore English toJapanese translation in video scenes, addressing the chal-lenge of the large number of characters and complex scriptsin Japanese.


3 Method

Our pipeline consists of six key modules―text detection,transformation, text recognition, machine translation, textreplacement, and text inser tion ― as shown in Figure 2.The process begins with an input image containing text,from which the pipeline detects text regions, crops them,and transforms these regions into a rectangular shape.
Thetransformed text image is then recognized and translatedfrom English to Japanese.
Next, given the transformed textimage and the translated Japanese text, the text replacementmodule generates a new text image in Japanese with thesame background and a visually similar text style.
Finally,the translated text image is inserted back into the originalimage, replacing the corresponding English text region.
In this work, we employed publicly available mod-els wherever possible.
Speciﬁcally, we used the FASTmodel
[3] for text detection,
the CRNN model [4] fortext recognition, and the NLLB200 model [5] for machinetranslation.
For the transformation module, we adoptedperspective transformation, a geometric function that re-shapes a quadrilateral image into a rectangular one.
Con-― 235 ―

Towards Scene Text Translation for Complex Writing Systems

Hour Kaing

1

Haiyue Song

1

Chenchen Ding

1

Jiannan Mao

2

Hideki Tanaka

1

Masao Utiyama

11

ASTREC, UCRI, NICT, Japan

2

Gifu University, Gifu, Japan

1

{hour kaing, haiyue.song, chenchen.ding, hideki.tanaka, mutiyama}@nict.go.jp

2

mao@mat.info.gifu-u.ac.jp



Abstract

Scene text translation aims to automatically translate textin images or videos while preserving its visual features.
Inthis work, we focus on scene text translation for complexwriting system by taking Japanese as a typical example.
We build a pipeline to translate from English to Japanese,leveraging publicly available modules for text detection,recognition, and translation, and train our own text replace-ment model specialized for English-to-Japanese transfor-mations.
Experiments show that the system can eﬀectivelygenerate translated text in Japanese while retaining muchof the original style, although background regeneration andhandling of Kanji remain open challenges.


1 Introduction

When watching a foreign movie, untranslated on-screentext in the background often hinders understanding of thescene, as shown in Figure 1.
Translating such text andplacing it in the correct position with similar style typicallyrequires signiﬁcant manual eﬀort.
Scene text translation systems
[1](also referred to ascross-language text editing systems
[2]) provide an au-tomatic solution by translating the source text in videoscenes into the target language while preserving the vi-sual features of the original text, such as its location, font,and background.
This is typically achieved by integratingscene text detection and recognition, machine translation,and scene text replacement modules.
However, translating scene text into complex writingsystems is challenging.
Japanese can be regarded as atypical example of a complex writing system, which en-compasses thousands of distinct characters across multipleforms (e.g., Kanji, Hiragana, Katakana).To address this, we train a specialized Japanese text re-placement module by 1) synthesizing 100k cross-lingualFigure 1
An example of translating scene text from Englishto Japanese in videos, while preserving the original position andstyle.text images from English to Japanese, and 2) ﬁne-tuningthe English SRNet model
[2] on our synthetic dataset.
Forother modules in our English-to-Japanese scene text trans-lation system, we leverage publicly available models in-cluding the FAST text detection model [3], the CRNN textrecognition model [4], and the NLLB200 machine transla-tion model [5].Experiments show that our model performs better thanbaselines in generating Japanese text but underperformsthem in background regeneration.
We also found that gen-erating Kanji characters is more challenging than generat-ing Katakana or Hiragana characters.


2 Background

We introduce the modules involved in our scene texttranslation system, including detecting and recognizingthe English text in each frame (Section 2.1), translatingit into the target language (Section 2.2), and placing itback into the original position with similar visual features(Section 2.3).
Previous scene text translation systems areintroduced in Section 2.4.

2.1 Text Detection and Recognition

Scene text detection aims to identify text regions in nat-ural (often noisy) scenes
[3, 6, 7, 8].
We adopt the FAST(faster arbitrarily-shaped text detector)[3] system, whichachieves real-time, high-accuracy detection for curved textand supports multiple languages including Japanese.― 234 ―Tex t  Dete ct ionTex t  Reco gn
it
ionMachine TranslationTex t ReplacementTex t  Inser ti onTransformationFigure 2 Scene Text Translation Pipeline.
Scene text recognition then extracts text from the de-tected sub-images.
This is typically done with opticalcharacter recognition (OCR) models such as CRNN [4],SSDAN [9], or PaddleOCR.1）In our work, we use theCRNN
[4] system because it handles sequences withoutexplicit character segmentation and supports non-Latinscripts such as Japanese or Chinese.


2.2 Machine Translation

Next, the extracted source text is translated into the targetlanguage through a separate neural machine translation(NMT) system [5, 10, 11].
We employ the NLLB200
[5]model, a state-of-the-art multilingual NMT system thatprovides high-quality English to Japanese translation.
Although multimodal machine translation (MMT) thatuses images [12, 13] or videos
[14, 15] as assisting infor-mation can yield better performance, we adopt an eﬃcienttext-to-text MT approach for real-time video translation.


2.3 Scene Text Replacement

Scene text replacement aims to edit the text in an imageby replacing it with new text while preserving the originaltext style and background information.
The model typi-cally takes two inputs: a scene text image to be edited andthe new text to be inserted.
It then outputs a new image con-taining the new text.
Previous works include SRNet
[2],STEFANN [16], SwapText [17], and STRIVE [18].
Weapply the SRNet architecture, which comprise a text con-version module, a background inpainting module, and afusion module, since its code is publicly available.2）1） https://github.com/PaddlePaddle/PaddleOCR2）
https://github.com/lksshw/SRNet

2.4 Scene Text Translation

Scene text translation has been explored in both re-search [1, 19, 20] and commercial applications (e.g.,Google Translate’s Camera mode).
However, previous sys-tems [19, 20] typically focus on image translation withoutadapting text style, or are limited to Indic scripts such asEnglish to Hindi [1].Our paper presents the ﬁrst study to explore English toJapanese translation in video scenes, addressing the chal-lenge of the large number of characters and complex scriptsin Japanese.


3 Method

Our pipeline consists of six key modules―text detection,transformation, text recognition, machine translation, textreplacement, and text inser tion ― as shown in Figure 2.The process begins with an input image containing text,from which the pipeline detects text regions, crops them,and transforms these regions into a rectangular shape.
Thetransformed text image is then recognized and translatedfrom English to Japanese.
Next, given the transformed textimage and the translated Japanese text, the text replacementmodule generates a new text image in Japanese with thesame background and a visually similar text style.
Finally,the translated text image is inserted back into the originalimage, replacing the corresponding English text region.
In this work, we employed publicly available mod-els wherever possible.
Speciﬁcally, we used the FASTmodel
[3] for text detection,
the CRNN model [4] fortext recognition, and the NLLB200 model [5] for machinetranslation.
For the transformation module, we adoptedperspective transformation, a geometric function that re-shapes a quadrilateral image into a rectangular one.
Con-― 235 ―Figure 3
Examples of synthetic data.
From top to bottom:background, text skeleton, foreground text, target image withJapanese text, source image with English text.versely, for the insertion module, we utilized inverse per-spective transformation to accurately position the trans-lated text image within the original parent image.
For text replacement, we trained our own model becauseno pre-existing cross-lingual text replacement model (fromEnglish to Japanese) was available.
Speciﬁcally, we syn-thesized 100k sets of images as the training data of ourtext replacement model.
Since SRNet is trained on mul-tiple objectives to optimize text conversion, backgroundinpainting, and the fusion module, we synthesize the fol-lowing for each set: two style images (one for English andone for Japanese),3）a background image, a foregroundJapanese text image, and a Japanese text skeleton image,as shown in Figure 3.
The style of text is generated byrandomly selecting text fonts, color, and parameters of de-formation, and background image is randomly chosen froma background set.
This data synthetic process follows thedesign of the English text replacement approach describedin
[2].
The primary diﬀerence is that our target texts arein Japanese, randomly selected from a set of 45k Japanesewords.4）Additionally, we leveraged an existing Englishmodel by ﬁne-tuning SRNet𝑒𝑛[2] on our synthetic dataset,where the input is English style image and the output isthe paired Japanese style image, resulting in xSRNet𝑒𝑛 𝑗𝑎.Since
the original SRNet𝑒𝑛weights were not publicly re-leased, we ﬁne-tuned a reproduced version of the modelinstead.
Both the synthetic tool and the SRNet𝑒𝑛modelare available in a public repository.5）3）
They are not necessary to be parallel words.4） https://github.com/hingston/japanese5） https://github.com/lksshw/SRNetModel MSE↓ PSNR↑ SSIM↑EnSource𝑖𝑛29.
79 33.
65 0.
61Background𝑟𝑒 𝑓18.71 35.91 0.75Foreground𝑟𝑒 𝑓95.
65 28.
60 0.
46SRNet𝑒𝑛54.
55 31.
01 0.
57xSRNet𝑒𝑛 𝑗𝑎(ours) 73. 94 29.
66 0.
63Table 1 Quality assessment of cross-lingual text replacement.
Bold and underlined scores are ﬁrst and second best, respectively.


4 Evaluation

In this section, we assess our pipeline by ﬁrst examiningthe quality of text replacement and then evaluating theoverall performance of our scene text translation pipeline.


4.1 Quality of Text Replacement

We measured the quality of text replacement using MeanSquared Error (MSE), Peak Signal-to-Noise Ratio (PSNR),and Structural Similarity (SSIM) metrics [21].
To computethese metrics, the evaluation dataset must comprise of En-glish text images as model inputs and Japanese text imagesas references, of which English and Japanese texts are mu-tually translated.
Due to the absence of such dataset for theEnglish-Japanese pairs, we synthesized 1k images using adistinct set of background images6）and pairwise English-Japanese text data,7）all of which diﬀer from our synthetictraining dataset.
Table 1 summarizes the performance ofour xSRNet𝑒𝑛 𝑗𝑎model, evaluated via MSE, PSNR, andSSIM.
We compare our model against SRNet𝑒𝑛, which wastrained solely for English text replacement and applied herefor cross-lingual text replacement.
We further include threestrong baselines that share part of reference by computingevaluation metrics on ground-truth Japanese text imagesagainst their respective: (1) source image with English text(EnSource𝑖𝑛), (2) ground-truth background images with-out text on it (Background𝑟𝑒 𝑓), and (3) forground Japanesetext with a grey background (Foreground𝑟𝑒 𝑓).
The sam-ples of (1), (2), and (3) are similar to those in Figure 3 atline ﬁve, one, and three, respectively.
As a result, Background𝑟𝑒 𝑓achieves the best perfor-mance across all metrics, which is not surprising given thatthe background in the evaluation dataset is challenging, andneither SRNet𝑒𝑛nor xSRNet𝑒𝑛 𝑗𝑎is expected to perfectlyregenerate the background.
Our model, xSRNet𝑒𝑛 𝑗𝑎, un-6） https://github.com/clovaai/synthtiger7） https://github.com/facebookresearch/MUSE― 236 ―

Towards Scene Text Translation for Complex Writing Systems

Hour Kaing

1

Haiyue Song

1

Chenchen Ding

1

Jiannan Mao

2

Hideki Tanaka

1

Masao Utiyama

11

ASTREC, UCRI, NICT, Japan

2

Gifu University, Gifu, Japan

1

{hour kaing, haiyue.song, chenchen.ding, hideki.tanaka, mutiyama}@nict.go.jp

2

mao@mat.info.gifu-u.ac.jp



Abstract

Scene text translation aims to automatically translate textin images or videos while preserving its visual features.
Inthis work, we focus on scene text translation for complexwriting system by taking Japanese as a typical example.
We build a pipeline to translate from English to Japanese,leveraging publicly available modules for text detection,recognition, and translation, and train our own text replace-ment model specialized for English-to-Japanese transfor-mations.
Experiments show that the system can eﬀectivelygenerate translated text in Japanese while retaining muchof the original style, although background regeneration andhandling of Kanji remain open challenges.


1 Introduction

When watching a foreign movie, untranslated on-screentext in the background often hinders understanding of thescene, as shown in Figure 1.
Translating such text andplacing it in the correct position with similar style typicallyrequires signiﬁcant manual eﬀort.
Scene text translation systems
[1](also referred to ascross-language text editing systems
[2]) provide an au-tomatic solution by translating the source text in videoscenes into the target language while preserving the vi-sual features of the original text, such as its location, font,and background.
This is typically achieved by integratingscene text detection and recognition, machine translation,and scene text replacement modules.
However, translating scene text into complex writingsystems is challenging.
Japanese can be regarded as atypical example of a complex writing system, which en-compasses thousands of distinct characters across multipleforms (e.g., Kanji, Hiragana, Katakana).To address this, we train a specialized Japanese text re-placement module by 1) synthesizing 100k cross-lingualFigure 1
An example of translating scene text from Englishto Japanese in videos, while preserving the original position andstyle.text images from English to Japanese, and 2) ﬁne-tuningthe English SRNet model
[2] on our synthetic dataset.
Forother modules in our English-to-Japanese scene text trans-lation system, we leverage publicly available models in-cluding the FAST text detection model [3], the CRNN textrecognition model [4], and the NLLB200 machine transla-tion model [5].Experiments show that our model performs better thanbaselines in generating Japanese text but underperformsthem in background regeneration.
We also found that gen-erating Kanji characters is more challenging than generat-ing Katakana or Hiragana characters.


2 Background

We introduce the modules involved in our scene texttranslation system, including detecting and recognizingthe English text in each frame (Section 2.1), translatingit into the target language (Section 2.2), and placing itback into the original position with similar visual features(Section 2.3).
Previous scene text translation systems areintroduced in Section 2.4.

2.1 Text Detection and Recognition

Scene text detection aims to identify text regions in nat-ural (often noisy) scenes
[3, 6, 7, 8].
We adopt the FAST(faster arbitrarily-shaped text detector)[3] system, whichachieves real-time, high-accuracy detection for curved textand supports multiple languages including Japanese.― 234 ―Tex t  Dete ct ionTex t  Reco gn
it
ionMachine TranslationTex t ReplacementTex t  Inser ti onTransformationFigure 2 Scene Text Translation Pipeline.
Scene text recognition then extracts text from the de-tected sub-images.
This is typically done with opticalcharacter recognition (OCR) models such as CRNN [4],SSDAN [9], or PaddleOCR.1）In our work, we use theCRNN
[4] system because it handles sequences withoutexplicit character segmentation and supports non-Latinscripts such as Japanese or Chinese.


2.2 Machine Translation

Next, the extracted source text is translated into the targetlanguage through a separate neural machine translation(NMT) system [5, 10, 11].
We employ the NLLB200
[5]model, a state-of-the-art multilingual NMT system thatprovides high-quality English to Japanese translation.
Although multimodal machine translation (MMT) thatuses images [12, 13] or videos
[14, 15] as assisting infor-mation can yield better performance, we adopt an eﬃcienttext-to-text MT approach for real-time video translation.


2.3 Scene Text Replacement

Scene text replacement aims to edit the text in an imageby replacing it with new text while preserving the originaltext style and background information.
The model typi-cally takes two inputs: a scene text image to be edited andthe new text to be inserted.
It then outputs a new image con-taining the new text.
Previous works include SRNet
[2],STEFANN [16], SwapText [17], and STRIVE [18].
Weapply the SRNet architecture, which comprise a text con-version module, a background inpainting module, and afusion module, since its code is publicly available.2）1） https://github.com/PaddlePaddle/PaddleOCR2）
https://github.com/lksshw/SRNet

2.4 Scene Text Translation

Scene text translation has been explored in both re-search [1, 19, 20] and commercial applications (e.g.,Google Translate’s Camera mode).
However, previous sys-tems [19, 20] typically focus on image translation withoutadapting text style, or are limited to Indic scripts such asEnglish to Hindi [1].Our paper presents the ﬁrst study to explore English toJapanese translation in video scenes, addressing the chal-lenge of the large number of characters and complex scriptsin Japanese.


3 Method

Our pipeline consists of six key modules―text detection,transformation, text recognition, machine translation, textreplacement, and text inser tion ― as shown in Figure 2.The process begins with an input image containing text,from which the pipeline detects text regions, crops them,and transforms these regions into a rectangular shape.
Thetransformed text image is then recognized and translatedfrom English to Japanese.
Next, given the transformed textimage and the translated Japanese text, the text replacementmodule generates a new text image in Japanese with thesame background and a visually similar text style.
Finally,the translated text image is inserted back into the originalimage, replacing the corresponding English text region.
In this work, we employed publicly available mod-els wherever possible.
Speciﬁcally, we used the FASTmodel
[3] for text detection,
the CRNN model [4] fortext recognition, and the NLLB200 model [5] for machinetranslation.
For the transformation module, we adoptedperspective transformation, a geometric function that re-shapes a quadrilateral image into a rectangular one.
Con-― 235 ―Figure 3
Examples of synthetic data.
From top to bottom:background, text skeleton, foreground text, target image withJapanese text, source image with English text.versely, for the insertion module, we utilized inverse per-spective transformation to accurately position the trans-lated text image within the original parent image.
For text replacement, we trained our own model becauseno pre-existing cross-lingual text replacement model (fromEnglish to Japanese) was available.
Speciﬁcally, we syn-thesized 100k sets of images as the training data of ourtext replacement model.
Since SRNet is trained on mul-tiple objectives to optimize text conversion, backgroundinpainting, and the fusion module, we synthesize the fol-lowing for each set: two style images (one for English andone for Japanese),3）a background image, a foregroundJapanese text image, and a Japanese text skeleton image,as shown in Figure 3.
The style of text is generated byrandomly selecting text fonts, color, and parameters of de-formation, and background image is randomly chosen froma background set.
This data synthetic process follows thedesign of the English text replacement approach describedin
[2].
The primary diﬀerence is that our target texts arein Japanese, randomly selected from a set of 45k Japanesewords.4）Additionally, we leveraged an existing Englishmodel by ﬁne-tuning SRNet𝑒𝑛[2] on our synthetic dataset,where the input is English style image and the output isthe paired Japanese style image, resulting in xSRNet𝑒𝑛 𝑗𝑎.Since
the original SRNet𝑒𝑛weights were not publicly re-leased, we ﬁne-tuned a reproduced version of the modelinstead.
Both the synthetic tool and the SRNet𝑒𝑛modelare available in a public repository.5）3）
They are not necessary to be parallel words.4） https://github.com/hingston/japanese5） https://github.com/lksshw/SRNetModel MSE↓ PSNR↑ SSIM↑EnSource𝑖𝑛29.
79 33.
65 0.
61Background𝑟𝑒 𝑓18.71 35.91 0.75Foreground𝑟𝑒 𝑓95.
65 28.
60 0.
46SRNet𝑒𝑛54.
55 31.
01 0.
57xSRNet𝑒𝑛 𝑗𝑎(ours) 73. 94 29.
66 0.
63Table 1 Quality assessment of cross-lingual text replacement.
Bold and underlined scores are ﬁrst and second best, respectively.


4 Evaluation

In this section, we assess our pipeline by ﬁrst examiningthe quality of text replacement and then evaluating theoverall performance of our scene text translation pipeline.


4.1 Quality of Text Replacement

We measured the quality of text replacement using MeanSquared Error (MSE), Peak Signal-to-Noise Ratio (PSNR),and Structural Similarity (SSIM) metrics [21].
To computethese metrics, the evaluation dataset must comprise of En-glish text images as model inputs and Japanese text imagesas references, of which English and Japanese texts are mu-tually translated.
Due to the absence of such dataset for theEnglish-Japanese pairs, we synthesized 1k images using adistinct set of background images6）and pairwise English-Japanese text data,7）all of which diﬀer from our synthetictraining dataset.
Table 1 summarizes the performance ofour xSRNet𝑒𝑛 𝑗𝑎model, evaluated via MSE, PSNR, andSSIM.
We compare our model against SRNet𝑒𝑛, which wastrained solely for English text replacement and applied herefor cross-lingual text replacement.
We further include threestrong baselines that share part of reference by computingevaluation metrics on ground-truth Japanese text imagesagainst their respective: (1) source image with English text(EnSource𝑖𝑛), (2) ground-truth background images with-out text on it (Background𝑟𝑒 𝑓), and (3) forground Japanesetext with a grey background (Foreground𝑟𝑒 𝑓).
The sam-ples of (1), (2), and (3) are similar to those in Figure 3 atline ﬁve, one, and three, respectively.
As a result, Background𝑟𝑒 𝑓achieves the best perfor-mance across all metrics, which is not surprising given thatthe background in the evaluation dataset is challenging, andneither SRNet𝑒𝑛nor xSRNet𝑒𝑛 𝑗𝑎is expected to perfectlyregenerate the background.
Our model, xSRNet𝑒𝑛 𝑗𝑎, un-6） https://github.com/clovaai/synthtiger7） https://github.com/facebookresearch/MUSE― 236 ―Input ImageGround TruthSRNetxSRNetenjaFigure 4 Samples generated by text replacement modules.
Input ImageTranslated ImageFigure 5 Samples generated by our scene text translation pipeline.derperforms compared to enSceneImg𝑖𝑛and even SRNet𝑒𝑛in terms of MSE and PSNR, indicating that further im-provements in background regeneration are needed.
How-ever, instead of focusing solely on absolute errors measuredby MSE and PSNR, our model outperforms SRNet𝑒𝑛whenmeasured by structural similarity (SSIM).
This suggeststhat our model tends to generate Japanese text images thatare more structurally similar to the g round truth, as illus-trated in Figure 4.
Additionally, we observe that generat-ing Kanji characters is more challenging than generatingKatakana characters.


4.2 Quality of Scene Text Translation

To this end, we assessed the quality of our pipeline whenall modules were used together.
We applied our approachto the ICDAR 2003 scene text dataset [22].
Figure 5 illus-trates several selected samples that were translated usingour pipeline.
These samples are relatively straightforwardfor our text replacement module because the detected textregions include fewer noise elements.
However, errors stilloccur due to other modules.
For instance, some text re-mained in English because the text detection module failedto detect it; other text was mistranslated owing to stylisticfeatures of the original text image, such as “STANFORDS”with a mixed font style between the letter “S” and the rest.
All these observations suggest that fur ther improvementsare needed not only in the text replacement module but alsoin other components, including text detection, recognition,and translation.


5 Conclusion and Future Works

We have presented a pipeline to translate scene text im-ages from English to Japanese, utilizing open models, ex-cept for the text replacement model, which was trained byourselves.
We demonstrated that our pipeline is capable oftranslating English scene text images, though it has somelimitations, such as diﬃculty in generating Kanji charac-ters, inability to detect all text, and restricting detectionand translation to words rather than phrases or sentences.
There is plenty of room to improve our pipeline.
First,we aim to enhance the text replacement module by usingmore synthesized training images with diverse backgroundscenes and Japanese Kanji characters.
For text detection,we will consider combining multiple text detection modelsto ensure that all texts are translated.
Furthermore, sincescene text is not always limited to individual words, we willalso explore contextual translation of phrases or sentences.
Finally, although our pipeline can be used to translatetext in videos frame-by-frame, the translation in the result-ing video may appear inconsistent.
This occurs becausecertain frames may be blurry or contain non-frontal text,posing particular challenges for text detection.
Additionalmodules, such as reference frame selection and text prop-agation
[18], are needed to achieve more consistent andﬂuid translations in video.
Addressing these challenges ispart of our future work.― 237 ―



References


[1]Shreyas Vaidya, Ar vind Kumar Sharma, Prajwal Gatti,and Anand Mishra. Show me the world in my language:Establishing the ﬁrst baseline for scene-text to scene-text translation. In ICPR, 2024.
[2]Liang Wu, Chengquan Zhang, Jiaming Liu, JunyuHan, Jingtuo Liu, Errui Ding, and Xiang Bai. Editingtext in the wild. In Proc ACM Int Conf Multimed,pp. 1500–1508, 2019.
[3]Zhe Chen, Jiahao Wang, Wenhai Wang, Guo Chen,Enze Xie, Ping Luo, and Tong Lu. Fast: Fasterarbitrarily-shaped text detector with minimalist kernelrepresentation. arXiv:2111.02394, 2021.
[4]Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based sequencerecognition and its application to scene text recogni-tion. IEEE PAMI, Vol. 39, No. 11, pp. 2298–2304,2016.
[5]NLLB Teamg. No language left behind: Scalinghuman-centered machine translation, 2022.
[6]Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang,Shuchang Zhou, Weiran He, and Jiajun Liang. EAST:An eﬃcient and accurate scene text detector. In CVPR,pp. 5551–5560, 2017.
[7]Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, TongLu, Gang Yu, and Shuai Shao. Shape robust text de-tection with progressive scale expansion network. InCVPR, pp. 9336–9345, 2019.
[8]Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, andXiang Bai. Real-time scene text detection with diﬀer-entiable binarization. In AAAI, Vol. 34, pp. 11474–11481, 2020.
[9]Yaping Zhang, Shuai Nie, Wenju Liu, Xing Xu,Dongxiang Zhang, and Heng Tao Shen. Sequence-to-sequence domain adaptation network for robust textimage recognition. In CVPR, pp. 2735–2744, 2019.
[10]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio. Neural machine translation by jointly learning toalign and translate, 2014.
[11]Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. Attention is all you need,2017.
[12]Desmond Elliott, Stella Frank, Khalil Sima’an, and Lu-cia Specia. Multi30K: Multilingual English-Germanimage descriptions. In VL, pp. 70–74, 2016.
[13]Lucia Specia, Stella Frank, Khalil Sima’an, andDesmond Elliott. A shared task on multimodal ma-chine translation and crosslingual image description.In WMT, pp. 543–553, 2016.
[14]Tosho Hirasawa, Zhishen Yang, Mamoru Komachi,and Naoaki Okazaki. Keyframe segmentation and po-sitional encoding for video-guided machine translationchallenge 2020, 2020.
[15]Weiqi Gu, Haiyue Song, Chenhui Chu, andSadao Kurohashi. Video-guided machine transla-tion with spatial hierarchical attention network. InACL—IJCNLP, pp. 87–92, 2021.
[16]Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh,and Umapada Pal. Stefann: Scene text editor usingfont adaptive neural network. In CVPR, June 2020.
[17]Qiangpeng Yang, Hongsheng Jin, Jun Huang, and WeiLin. Swaptext: Image based texts transfer in scenes,2020.
[18]Vijay Kumar BG, Jeyasri Subramanian, Varnith Chor-dia, Eugene Bart, Shaobo Fang, Kelly Guan, and RajaBala. Strive: scene text replacement in videos. InICCV, pp. 14529–14538. IEEE, 2021.
[19]Puneet Jain, Orhan Firat, Qi Ge, and Sihang Liang.Image translation network. 2021.
[20]Zhibin Lan, Jiawei Yu, Xiang Li, Wen Zhang, JianLuan, Bin Wang, Degen Huang, and Jinsong Su. Ex-ploring better text image translation with multimodalcodebook. In ACL, pp. 3479–3491, 2023.
[21]Zhou Wang, Alan C Bovik, Hamid R Sheikh, andEero P Simoncelli. Image quality assessment: fromerror visibility to structural similarity. IEEE trans-actions on image processing, Vol. 13, No. 4, pp.600–612, 2004.
[22]Simon M Lucas, Alex Panaretos, Luis Sosa, An-thony Tang, Shirley Wong, Robert Young, KazukiAshida, Hiroki Nagai, Masayuki Okamoto, HiroakiYamamoto, et al. Icdar 2003 robust reading competi-tions: entries, results, and future directions. IJDAR,Vol. 7, pp. 105–122, 2005.― 238 ―