Dispersion Measures as Predictors of Lexical Decision Time,Word Familiarity, and Lexical Complexity

Adam Nohejlâ€ƒTaro Watanabe



Nara Institute of Science and Technology



{nohejl.adam.mt3, taro}@is.naist.jp



Abstract

Various measures of dispersion have been proposed topaint a fuller picture of a wordâ€™s distribution in a corpus,but only little has been done to validate them externally.
Weevaluate a wide range of dispersion measures as predictorsof lexical decision time, word familiarity, and lexical com-plexity in ï¬ve diverse languages.
We ï¬nd that the logarithmof range is not only a better predictor than log-frequencyacross all tasks and languages, but that it is also the mostpowerful additional variable to log-frequency, consistentlyoutperforming the more complex dispersion measures.
Wediscuss the eï¬€ects of corpus part granularity and logarith-mic transformation, shedding light on contradictory resultsof previous studies.


1 Introduction

Measures of dispersion have been proposed in corpuslinguistics to complement frequency, a measure of centraltendency.
While a wordâ€™s frequency tells us how commonthe word is in the whole corpus, its dispersion tells us howevenly it is spread.
For instance, the words very and yeah,or came and data may have similar overall frequencies, butyeah and data would likely have lower dispersions, as theyare speciï¬c to a certain register or domain.
The conceptually simplest dispersion measure is therange: the number of corpus parts in which a word occurs.
The parts may be of diï¬€erent granularity and function,e.g. individual texts, authors, domains, or registers.
In theTUBELEX corpus [1], which is based on YouTube videos,our example words have the following frequencies (in thou-sands of occurrences) and ranges (in thousands of YouTubechannels): very: 332 and 35; yeah:
333 and 19; came: 64and 17; data: 64 and 7, conï¬rming our expectations.
The number of texts in which a word appears was usedto organize pedagogical word lists as early as in 1920by Keniston
[2] and mentioned as â€œrangeâ€ by Thorndike(2021)[3].
Gries (2008)[4] lists thirteen more advanceddispersion measures that have been proposed over thedecades, often theoretically motivated or considering in-tuitive interpretability.
What is critically missing, as Gries[4] also argues, is external validation.
We aim to bridge this gap between theoretical corpus dis-persion research, psycholinguistics, and NLP applications,with a comprehensive evaluation of dispersion measureson ï¬ve languages, three tasks, and three levels of corpuspart granularity.
Two of the tasks predict psycholinguisticdata, lexical decision time (LDT) and word familiarity, andone is an NLP task, lexical complexity prediction.


2 Related Research

Adelman et al.
(2006)[5] evaluated log-range on Englishword naming and LDT, concluding that log-range1ï¼‰is abetter predictor than log-frequency.
Brysbaert and New (2009)[8] replicated the results ofAdelman et al.
on the SUBTLEX-US subtitle corpus.
Mostlater studies on ï¬lm subtitles reached similar conclusions[9, 10, 11, 12, 13], but a few of them did not ï¬nd a statisticalsigniï¬cant diï¬€erence on individual datasets [14, 15, 16].All of these studies investigated only log-range across sub-title ï¬les (typically thousands of ï¬les corresponding toï¬lms or show episodes) as a dispersion measure, and eval-uated it on LDT or word naming times, essentially repli-cating Brysbaert and Newâ€™s study [8] on other languages.
Gries (2010)[17] evaluated multiple dispersion mea-sures on English word naming and LDT data.
The studydid not reach a conclusive result on the two datasets.
Gries (2021)[18] experimented with log-frequency,1ï¼‰
Adelman et al. call range â€œcontextual diversityâ€, avoiding theterms â€œdispersionâ€ and â€œrangeâ€, which is arguably confusing [6, 7],but does not detract anything from the practical value of their study.

Dispersion Measures as Predictors of Lexical Decision Time,Word Familiarity, and Lexical Complexity

Adam Nohejlâ€ƒTaro Watanabe



Nara Institute of Science and Technology



{nohejl.adam.mt3, taro}@is.naist.jp



Abstract

Various measures of dispersion have been proposed topaint a fuller picture of a wordâ€™s distribution in a corpus,but only little has been done to validate them externally.
Weevaluate a wide range of dispersion measures as predictorsof lexical decision time, word familiarity, and lexical com-plexity in ï¬ve diverse languages.
We ï¬nd that the logarithmof range is not only a better predictor than log-frequencyacross all tasks and languages, but that it is also the mostpowerful additional variable to log-frequency, consistentlyoutperforming the more complex dispersion measures.
Wediscuss the eï¬€ects of corpus part granularity and logarith-mic transformation, shedding light on contradictory resultsof previous studies.


1 Introduction

Measures of dispersion have been proposed in corpuslinguistics to complement frequency, a measure of centraltendency.
While a wordâ€™s frequency tells us how commonthe word is in the whole corpus, its dispersion tells us howevenly it is spread.
For instance, the words very and yeah,or came and data may have similar overall frequencies, butyeah and data would likely have lower dispersions, as theyare speciï¬c to a certain register or domain.
The conceptually simplest dispersion measure is therange: the number of corpus parts in which a word occurs.
The parts may be of diï¬€erent granularity and function,e.g. individual texts, authors, domains, or registers.
In theTUBELEX corpus [1], which is based on YouTube videos,our example words have the following frequencies (in thou-sands of occurrences) and ranges (in thousands of YouTubechannels): very: 332 and 35; yeah:
333 and 19; came: 64and 17; data: 64 and 7, conï¬rming our expectations.
The number of texts in which a word appears was usedto organize pedagogical word lists as early as in 1920by Keniston
[2] and mentioned as â€œrangeâ€ by Thorndike(2021)[3].
Gries (2008)[4] lists thirteen more advanceddispersion measures that have been proposed over thedecades, often theoretically motivated or considering in-tuitive interpretability.
What is critically missing, as Gries[4] also argues, is external validation.
We aim to bridge this gap between theoretical corpus dis-persion research, psycholinguistics, and NLP applications,with a comprehensive evaluation of dispersion measureson ï¬ve languages, three tasks, and three levels of corpuspart granularity.
Two of the tasks predict psycholinguisticdata, lexical decision time (LDT) and word familiarity, andone is an NLP task, lexical complexity prediction.


2 Related Research

Adelman et al.
(2006)[5] evaluated log-range on Englishword naming and LDT, concluding that log-range1ï¼‰is abetter predictor than log-frequency.
Brysbaert and New (2009)[8] replicated the results ofAdelman et al.
on the SUBTLEX-US subtitle corpus.
Mostlater studies on ï¬lm subtitles reached similar conclusions[9, 10, 11, 12, 13], but a few of them did not ï¬nd a statisticalsigniï¬cant diï¬€erence on individual datasets [14, 15, 16].All of these studies investigated only log-range across sub-title ï¬les (typically thousands of ï¬les corresponding toï¬lms or show episodes) as a dispersion measure, and eval-uated it on LDT or word naming times, essentially repli-cating Brysbaert and Newâ€™s study [8] on other languages.
Gries (2010)[17] evaluated multiple dispersion mea-sures on English word naming and LDT data.
The studydid not reach a conclusive result on the two datasets.
Gries (2021)[18] experimented with log-frequency,1ï¼‰
Adelman et al. call range â€œcontextual diversityâ€, avoiding theterms â€œdispersionâ€ and â€œrangeâ€, which is arguably confusing [6, 7],but does not detract anything from the practical value of their study.word length, and multiple dispersion measures as featuresfor a random forest model of English auditory LDT.


3 Examined Measures

As we have noted, both frequency and range are com-monly log-transformed to achieve better correlation withpsycholinguistic variables.
Since our evaluation will in-clude words not present in the corpus, which would resultin undeï¬ned values (log 0), we take the following steps toexamine the log-transformation of all examined measuresand frequency: Using ğ‘› = number of corpus parts, we applysmoothing in the form (ğ‘‘ğ‘› + 1)/(ğ‘› + 1) to each dispersionmeasure ğ‘‘ if we log-transform it.
Therefore, with a slightabuse of notation, we always use â€œlog ğ‘‘â€ in the followingtext to refer to log((ğ‘‘ğ‘› +1)/(ğ‘›
+ 1)).
For (log-)frequency,we always use Laplace-smoothed frequency [19].We use all measures in forms appropriate for unequallysized corpus parts and normalized to the range
[0, 1],adapting Gini index and Griesâ€™s DP to ğ‘‘ = 1 âˆ’ ğ‘‘âˆ—fromtheir original formulas ğ‘‘âˆ—, so that high values indicate highdispersion.
We use the following variables, given a wordğ‘¤: ğ‘› is the number of corpus parts; ğ‘£ğ‘–is the number ofoccurrences of ğ‘¤ in part ğ‘–; ğ‘˜ğ‘–is the number of tokens inpart ğ‘–; ğ‘ ğ‘–= ğ‘˜ğ‘–/Ãk is the proportion of part ğ‘–; ğ‘Ÿğ‘–= ğ‘£ğ‘–/Ãvis the proportion of occurrences of ğ‘¤ in part ğ‘–; ğ‘ğ‘–= ğ‘£ğ‘–/ğ‘˜ğ‘–is the relative frequency of ğ‘¤ in part ğ‘–, i.e. frequency nor-malized per part; ğ‘ğ‘–= ğ‘ğ‘–/Ãp is the frequency normalizedper part and per word.
For each variable ğ‘¥ğ‘–indexed bycorpus part, we understand x
= (ğ‘¥ğ‘–)ğ‘›ğ‘–=1as the correspond-ing vector with sumÃx =Ãğ‘›ğ‘–=1ğ‘¥ğ‘–, mean ğœ‡x=Ãx/ğ‘›, andstandard deviation ğœx=pÃğ‘›ğ‘–=1(ğ‘¥ğ‘–âˆ’ ğœ‡x)2/ğ‘›.We examine the following dispersion measures:Range ğ‘… =Ãğ‘›ğ‘–=1[ğ‘£ğ‘–> 0]ğ‘›(1)Gini index ğº = 1 âˆ’Ãğ‘›ğ‘–=1Ãğ‘›ğ‘—=1î˜Œî˜Œğ‘ğ‘–âˆ’ ğ‘ğ‘—î˜Œî˜Œ2ğ‘›(2)Juillandâ€™s [20] ğ· = 1 âˆ’ğœpğœ‡pâˆšğ‘› âˆ’ 1(3)Lyneâ€™s
[21] ğ·3= 1 âˆ’Ãğ‘›ğ‘–=1(ğ‘Ÿğ‘–âˆ’ ğ‘ ğ‘–)24(4)Griesâ€™s [4] DP = 1 âˆ’Ãğ‘›ğ‘–=1|ğ‘Ÿğ‘–âˆ’ ğ‘ ğ‘–|2(5)Rosengrenâ€™s
[22] ğ‘† =î˜€Ãğ‘›ğ‘–=1âˆšğ‘ğ‘–î˜2ğ‘›(6)Carrollâ€™s
[23] ğ·2=âˆ’Ãğ‘›ğ‘–=1ğ‘ğ‘–log ğ‘ğ‘–log ğ‘›(7)Regardless of the formulas above, we deï¬ne each mea-sure as 0 for words missing from the corpus.2ï¼‰Gini index is the discrete variant of the well-know in-dex of inequality.
It was proposed as a dispersion mea-sure independently by Murayama et al.
(2018)[24](asWord GINI = âˆ’log ğº) and Burch et al.
(2017)[25](asğ·ğ´, later adjusted to unequally sized parts, with a slightlydiï¬€erent normalization from our ğº
[26]).
We investigateğº and log ğº using the formula given by Glasser (1962)[27], which reduces computation time to ğ‘‚ (ğ‘› log ğ‘›).3ï¼‰As far as we can tell, â€œdistributional consistencyâ€ pro-posed by Zhang et al.
(2004)[28], is simply equal to Rosen-grenâ€™s ğ‘† (6).4ï¼‰Finally, we observe that inverse document frequency(idf) and variation coeï¬ƒcient (vc) can be expressed as lin-ear functions of log-range (1) and Juillandâ€™s D (3), respec-tively, and therefore do not need to be examined separatelyin terms of linear correlation:idf = log1ğ‘…ğ‘  = âˆ’log ğ‘… (8)vc =ğœpğœ‡p=âˆšğ‘› âˆ’ 1 (1 âˆ’ ğ·)(9)

4 Evaluation

We evaluate the measures on TUBELEX [1], a largeYouTube subtitle corpus for English, Chinese, Spanish, In-donesian, and Japanese.
Word frequency in TUBELEXwas already demonstrated to achieve correlation with psy-cholinguistic variables on par with or superior to ï¬lm sub-title corpora
[1].
TUBELEX also provides three levelsof linguistically valid corpus parts: videos, channels, andcategories (tens of thousands, thousands, and 15 parts re-spectively).
We use TUBELEX in its default tokenization.
For evaluation, we use the same datasets for LDT (3 lan-guages), word familiarity (5 languages), and lexical com-plexity (3 languages) as were used for extrinsic evaluationof TUBELEX log-frequency by Nohejl et al.
(2024)[1].Word familiarity and lexical complexity diï¬€er from thecommonly employed LDT or word naming tasks by being2ï¼‰
This is in line with the formulas for range, ğ‘†, and ğ·2, whichwould give 0 for a zero frequency word.
ğ·, ğ·3, and DP would beundeï¬ned, and Gini index would be 1.3ï¼‰
We also use sparse arrays to represent q, resulting in ğ‘‚(ğ‘š log ğ‘š)time, where ğ‘š is the number of non-zero elements of q.
The sparse-ness of frequency vectors q grows with the number of corpus parts,keeping computation time reasonable.4ï¼‰ This seems to have escaped the attention of Zhang et al.
[28] andGries.
Gries only noted that it gives the same numerical result in anexample scenario [4] and appears similar in cluster analysis [17].

Dispersion Measures as Predictors of Lexical Decision Time,Word Familiarity, and Lexical Complexity

Adam Nohejlâ€ƒTaro Watanabe



Nara Institute of Science and Technology



{nohejl.adam.mt3, taro}@is.naist.jp



Abstract

Various measures of dispersion have been proposed topaint a fuller picture of a wordâ€™s distribution in a corpus,but only little has been done to validate them externally.
Weevaluate a wide range of dispersion measures as predictorsof lexical decision time, word familiarity, and lexical com-plexity in ï¬ve diverse languages.
We ï¬nd that the logarithmof range is not only a better predictor than log-frequencyacross all tasks and languages, but that it is also the mostpowerful additional variable to log-frequency, consistentlyoutperforming the more complex dispersion measures.
Wediscuss the eï¬€ects of corpus part granularity and logarith-mic transformation, shedding light on contradictory resultsof previous studies.


1 Introduction

Measures of dispersion have been proposed in corpuslinguistics to complement frequency, a measure of centraltendency.
While a wordâ€™s frequency tells us how commonthe word is in the whole corpus, its dispersion tells us howevenly it is spread.
For instance, the words very and yeah,or came and data may have similar overall frequencies, butyeah and data would likely have lower dispersions, as theyare speciï¬c to a certain register or domain.
The conceptually simplest dispersion measure is therange: the number of corpus parts in which a word occurs.
The parts may be of diï¬€erent granularity and function,e.g. individual texts, authors, domains, or registers.
In theTUBELEX corpus [1], which is based on YouTube videos,our example words have the following frequencies (in thou-sands of occurrences) and ranges (in thousands of YouTubechannels): very: 332 and 35; yeah:
333 and 19; came: 64and 17; data: 64 and 7, conï¬rming our expectations.
The number of texts in which a word appears was usedto organize pedagogical word lists as early as in 1920by Keniston
[2] and mentioned as â€œrangeâ€ by Thorndike(2021)[3].
Gries (2008)[4] lists thirteen more advanceddispersion measures that have been proposed over thedecades, often theoretically motivated or considering in-tuitive interpretability.
What is critically missing, as Gries[4] also argues, is external validation.
We aim to bridge this gap between theoretical corpus dis-persion research, psycholinguistics, and NLP applications,with a comprehensive evaluation of dispersion measureson ï¬ve languages, three tasks, and three levels of corpuspart granularity.
Two of the tasks predict psycholinguisticdata, lexical decision time (LDT) and word familiarity, andone is an NLP task, lexical complexity prediction.


2 Related Research

Adelman et al.
(2006)[5] evaluated log-range on Englishword naming and LDT, concluding that log-range1ï¼‰is abetter predictor than log-frequency.
Brysbaert and New (2009)[8] replicated the results ofAdelman et al.
on the SUBTLEX-US subtitle corpus.
Mostlater studies on ï¬lm subtitles reached similar conclusions[9, 10, 11, 12, 13], but a few of them did not ï¬nd a statisticalsigniï¬cant diï¬€erence on individual datasets [14, 15, 16].All of these studies investigated only log-range across sub-title ï¬les (typically thousands of ï¬les corresponding toï¬lms or show episodes) as a dispersion measure, and eval-uated it on LDT or word naming times, essentially repli-cating Brysbaert and Newâ€™s study [8] on other languages.
Gries (2010)[17] evaluated multiple dispersion mea-sures on English word naming and LDT data.
The studydid not reach a conclusive result on the two datasets.
Gries (2021)[18] experimented with log-frequency,1ï¼‰
Adelman et al. call range â€œcontextual diversityâ€, avoiding theterms â€œdispersionâ€ and â€œrangeâ€, which is arguably confusing [6, 7],but does not detract anything from the practical value of their study.word length, and multiple dispersion measures as featuresfor a random forest model of English auditory LDT.


3 Examined Measures

As we have noted, both frequency and range are com-monly log-transformed to achieve better correlation withpsycholinguistic variables.
Since our evaluation will in-clude words not present in the corpus, which would resultin undeï¬ned values (log 0), we take the following steps toexamine the log-transformation of all examined measuresand frequency: Using ğ‘› = number of corpus parts, we applysmoothing in the form (ğ‘‘ğ‘› + 1)/(ğ‘› + 1) to each dispersionmeasure ğ‘‘ if we log-transform it.
Therefore, with a slightabuse of notation, we always use â€œlog ğ‘‘â€ in the followingtext to refer to log((ğ‘‘ğ‘› +1)/(ğ‘›
+ 1)).
For (log-)frequency,we always use Laplace-smoothed frequency [19].We use all measures in forms appropriate for unequallysized corpus parts and normalized to the range
[0, 1],adapting Gini index and Griesâ€™s DP to ğ‘‘ = 1 âˆ’ ğ‘‘âˆ—fromtheir original formulas ğ‘‘âˆ—, so that high values indicate highdispersion.
We use the following variables, given a wordğ‘¤: ğ‘› is the number of corpus parts; ğ‘£ğ‘–is the number ofoccurrences of ğ‘¤ in part ğ‘–; ğ‘˜ğ‘–is the number of tokens inpart ğ‘–; ğ‘ ğ‘–= ğ‘˜ğ‘–/Ãk is the proportion of part ğ‘–; ğ‘Ÿğ‘–= ğ‘£ğ‘–/Ãvis the proportion of occurrences of ğ‘¤ in part ğ‘–; ğ‘ğ‘–= ğ‘£ğ‘–/ğ‘˜ğ‘–is the relative frequency of ğ‘¤ in part ğ‘–, i.e. frequency nor-malized per part; ğ‘ğ‘–= ğ‘ğ‘–/Ãp is the frequency normalizedper part and per word.
For each variable ğ‘¥ğ‘–indexed bycorpus part, we understand x
= (ğ‘¥ğ‘–)ğ‘›ğ‘–=1as the correspond-ing vector with sumÃx =Ãğ‘›ğ‘–=1ğ‘¥ğ‘–, mean ğœ‡x=Ãx/ğ‘›, andstandard deviation ğœx=pÃğ‘›ğ‘–=1(ğ‘¥ğ‘–âˆ’ ğœ‡x)2/ğ‘›.We examine the following dispersion measures:Range ğ‘… =Ãğ‘›ğ‘–=1[ğ‘£ğ‘–> 0]ğ‘›(1)Gini index ğº = 1 âˆ’Ãğ‘›ğ‘–=1Ãğ‘›ğ‘—=1î˜Œî˜Œğ‘ğ‘–âˆ’ ğ‘ğ‘—î˜Œî˜Œ2ğ‘›(2)Juillandâ€™s [20] ğ· = 1 âˆ’ğœpğœ‡pâˆšğ‘› âˆ’ 1(3)Lyneâ€™s
[21] ğ·3= 1 âˆ’Ãğ‘›ğ‘–=1(ğ‘Ÿğ‘–âˆ’ ğ‘ ğ‘–)24(4)Griesâ€™s [4] DP = 1 âˆ’Ãğ‘›ğ‘–=1|ğ‘Ÿğ‘–âˆ’ ğ‘ ğ‘–|2(5)Rosengrenâ€™s
[22] ğ‘† =î˜€Ãğ‘›ğ‘–=1âˆšğ‘ğ‘–î˜2ğ‘›(6)Carrollâ€™s
[23] ğ·2=âˆ’Ãğ‘›ğ‘–=1ğ‘ğ‘–log ğ‘ğ‘–log ğ‘›(7)Regardless of the formulas above, we deï¬ne each mea-sure as 0 for words missing from the corpus.2ï¼‰Gini index is the discrete variant of the well-know in-dex of inequality.
It was proposed as a dispersion mea-sure independently by Murayama et al.
(2018)[24](asWord GINI = âˆ’log ğº) and Burch et al.
(2017)[25](asğ·ğ´, later adjusted to unequally sized parts, with a slightlydiï¬€erent normalization from our ğº
[26]).
We investigateğº and log ğº using the formula given by Glasser (1962)[27], which reduces computation time to ğ‘‚ (ğ‘› log ğ‘›).3ï¼‰As far as we can tell, â€œdistributional consistencyâ€ pro-posed by Zhang et al.
(2004)[28], is simply equal to Rosen-grenâ€™s ğ‘† (6).4ï¼‰Finally, we observe that inverse document frequency(idf) and variation coeï¬ƒcient (vc) can be expressed as lin-ear functions of log-range (1) and Juillandâ€™s D (3), respec-tively, and therefore do not need to be examined separatelyin terms of linear correlation:idf = log1ğ‘…ğ‘  = âˆ’log ğ‘… (8)vc =ğœpğœ‡p=âˆšğ‘› âˆ’ 1 (1 âˆ’ ğ·)(9)

4 Evaluation

We evaluate the measures on TUBELEX [1], a largeYouTube subtitle corpus for English, Chinese, Spanish, In-donesian, and Japanese.
Word frequency in TUBELEXwas already demonstrated to achieve correlation with psy-cholinguistic variables on par with or superior to ï¬lm sub-title corpora
[1].
TUBELEX also provides three levelsof linguistically valid corpus parts: videos, channels, andcategories (tens of thousands, thousands, and 15 parts re-spectively).
We use TUBELEX in its default tokenization.
For evaluation, we use the same datasets for LDT (3 lan-guages), word familiarity (5 languages), and lexical com-plexity (3 languages) as were used for extrinsic evaluationof TUBELEX log-frequency by Nohejl et al.
(2024)[1].Word familiarity and lexical complexity diï¬€er from thecommonly employed LDT or word naming tasks by being2ï¼‰
This is in line with the formulas for range, ğ‘†, and ğ·2, whichwould give 0 for a zero frequency word.
ğ·, ğ·3, and DP would beundeï¬ned, and Gini index would be 1.3ï¼‰
We also use sparse arrays to represent q, resulting in ğ‘‚(ğ‘š log ğ‘š)time, where ğ‘š is the number of non-zero elements of q.
The sparse-ness of frequency vectors q grows with the number of corpus parts,keeping computation time reasonable.4ï¼‰ This seems to have escaped the attention of Zhang et al.
[28] andGries.
Gries only noted that it gives the same numerical result in anexample scenario [4] and appears similar in cluster analysis [17].Table 1 Mean improvement in ğ‘…2aof the log-transformed mea-sure over the non-log-transformed (number of datasets of total 11with positive improvement, if any, in parentheses).
Cases wherelogarithm improves ğ‘…2aby at least 0.001 are printed in bold.(a)
Dispersion measures as single predictors.
Dispersion Î”ğ‘…2aof log ğ‘‘ vs. ğ‘‘ (#Datasets: Î”ğ‘…2a> 0)Measure ğ‘‘ Videos Channels CategoriesRange 0.356 (11) 0.329 (11) âˆ’0.060 (1)Gini Index 0.361
(11) 0.340 (11) âˆ’0.000 (5)Juillandâ€™s ğ· âˆ’0.230 âˆ’0.213 âˆ’0.202Griesâ€™s DP âˆ’0.095 âˆ’0.102 âˆ’0.169Rosengrenâ€™sğ‘†0.362(11)0.341(11)âˆ’0.273Carrollâ€™s ğ·2âˆ’0.218 âˆ’0.176 (1) âˆ’0.241Lyneâ€™s
ğ·3âˆ’0.112 âˆ’0.089 (1) âˆ’0.043Frequency (for comparison): 0.389 (11)(b) Two predictors: dispersion measure and log-frequency.
Dispersion Î”ğ‘…2aof log ğ‘‘ vs. ğ‘‘ (#Datasets: Î”ğ‘…2a> 0)Measure ğ‘‘ Videos Channels CategoriesRange 0.016 (8) 0.023 (10) âˆ’0.010 (3)Gini Index 0.003 (4) 0.002 (5) 0.002 (6)Juillandâ€™s ğ· âˆ’0.006 (3) âˆ’0.006 (3) âˆ’0.010 (1)Griesâ€™s DP âˆ’0.002 (3) 0.000 (7) âˆ’0.005 (3)Rosengrenâ€™s ğ‘† 0.009 (5) 0.009 (6) âˆ’0.018 (1)Carrollâ€™s ğ·2âˆ’0.010
(5) âˆ’0.006 (4) âˆ’0.014 (1)Lyneâ€™s ğ·3âˆ’0.002 (3) âˆ’0.004 (2) âˆ’0.001 (4)based on subjective ratings as opposed to reaction time,while the lexical complexity used in this case diï¬€ers fromthe other data by being rated by non-native speakers or amix of natives and non-natives.
We evaluate the dispersions in two scenarios: as singlepredictors, and as one of two predictors, the other one beinglog-frequency.
In both cases, we measure adjusted ğ‘…2[29]:ğ‘…2a= 1 âˆ’ (1 âˆ’ ğ‘…2)ğ‘› âˆ’ 1ğ‘› âˆ’ ğ‘ âˆ’ 1(10)where ğ‘› is the number of examples (dataset size) and ğ‘ isthe number of variables (1 or 2).
We compute ğ‘…2(coef-ï¬cient of determination) for linear least squares (multiple)regression ï¬tted to the whole dataset, which allows us tointerpret it as measure of (multiple) correlation strength.5ï¼‰We predict mean LDT from three studies: the En-glish Lexicon Project
[30], restricted to lower-case wordsfollowing the approach of Brysbaert and New
[8]; theMELD-SCH database
[31] of simpliï¬ed Chinese words;and SPALEX [32] for Spanish.
For English and Chinese,5ï¼‰
Using ğ‘…2instead of Pearsonâ€™s (multiple) correlation coeï¬ƒcient ğ‘Ÿ(ğ‘…) allows us to ignore the diï¬€erent polarity of the tasks (rare wordshave low familiarity but high complexity).
The adjustment is appro-priate for comparing diï¬€erent numbers of independent variables.we use the published mean LDT.
SPALEX only providesraw participant data, which we process by removing timesout of the range
[200 ms, 2000 ms][32], and comput-ing the means.
We predict mean word familiarity fromï¬ve databases: Chinese familiarity ratings [33], EnglishMRC lexical database
[34, 35], Indonesian lexical norms[36], Japanese word familiarity ratings for reception [37],and Spanish lexical norms
[38].
Lastly, we predict lexicalcomplexity for English, Spanish and Japanese using theevaluation sets of the MultiLS dataset [39].
In total, we areevaluating on 11 datasets (task-language combinations).


4.1 To Log or Not to Log

In Table 1, we compare each dispersion measure with itslog-transformed version.
Perhaps surprisingly, which oneis a better predictor does not depend solely on the mea-sure, but also on corpus part granularity, and whether themeasure is used as a single predictor or with log-frequency.
When used as single predictors (Table 1a), the logarith-mic transformation beneï¬ts range, Gini index, and Rosen-grenâ€™s ğ‘†, resulting in stronger correlations on all 11 datasetsâ€“ but only if videos and channels are used as parts.
For allthree measures, the diï¬€erence between using and not usinglog-transformation is critical (0.340 to 0.362), comparableto that between log-frequency and frequency (0.389).When dispersion measures are employed along with log-frequency (Table 1b), applying logarithm is moderatelybeneï¬cial for the same measures as above and for Giniindex for categories, but the improvements are not robustacross datasets.
We will report and discuss each measure with logarithmapplied or not applied according to these results.


4.2 Results

As shown in Figure 1 (solid bars), the only dispersionmeasure robustly stronger than log-frequency as predictorsof LDT, word familiarity, and lexical complexity is log-range for channels and videos.
Although it does not comenear in correlation strength, Griesâ€™s DP is worth noting asthe only measure performing the best with categories (thecoarsest part granularity).When used along with log-frequency, the followingmeasures result in particularly robust improvements (indecreasing order): log-range for channels, log-range forvideos, range for categories, and Rosengrenâ€™s ğ‘† for cate-

Dispersion Measures as Predictors of Lexical Decision Time,Word Familiarity, and Lexical Complexity

Adam Nohejlâ€ƒTaro Watanabe



Nara Institute of Science and Technology



{nohejl.adam.mt3, taro}@is.naist.jp



Abstract

Various measures of dispersion have been proposed topaint a fuller picture of a wordâ€™s distribution in a corpus,but only little has been done to validate them externally.
Weevaluate a wide range of dispersion measures as predictorsof lexical decision time, word familiarity, and lexical com-plexity in ï¬ve diverse languages.
We ï¬nd that the logarithmof range is not only a better predictor than log-frequencyacross all tasks and languages, but that it is also the mostpowerful additional variable to log-frequency, consistentlyoutperforming the more complex dispersion measures.
Wediscuss the eï¬€ects of corpus part granularity and logarith-mic transformation, shedding light on contradictory resultsof previous studies.


1 Introduction

Measures of dispersion have been proposed in corpuslinguistics to complement frequency, a measure of centraltendency.
While a wordâ€™s frequency tells us how commonthe word is in the whole corpus, its dispersion tells us howevenly it is spread.
For instance, the words very and yeah,or came and data may have similar overall frequencies, butyeah and data would likely have lower dispersions, as theyare speciï¬c to a certain register or domain.
The conceptually simplest dispersion measure is therange: the number of corpus parts in which a word occurs.
The parts may be of diï¬€erent granularity and function,e.g. individual texts, authors, domains, or registers.
In theTUBELEX corpus [1], which is based on YouTube videos,our example words have the following frequencies (in thou-sands of occurrences) and ranges (in thousands of YouTubechannels): very: 332 and 35; yeah:
333 and 19; came: 64and 17; data: 64 and 7, conï¬rming our expectations.
The number of texts in which a word appears was usedto organize pedagogical word lists as early as in 1920by Keniston
[2] and mentioned as â€œrangeâ€ by Thorndike(2021)[3].
Gries (2008)[4] lists thirteen more advanceddispersion measures that have been proposed over thedecades, often theoretically motivated or considering in-tuitive interpretability.
What is critically missing, as Gries[4] also argues, is external validation.
We aim to bridge this gap between theoretical corpus dis-persion research, psycholinguistics, and NLP applications,with a comprehensive evaluation of dispersion measureson ï¬ve languages, three tasks, and three levels of corpuspart granularity.
Two of the tasks predict psycholinguisticdata, lexical decision time (LDT) and word familiarity, andone is an NLP task, lexical complexity prediction.


2 Related Research

Adelman et al.
(2006)[5] evaluated log-range on Englishword naming and LDT, concluding that log-range1ï¼‰is abetter predictor than log-frequency.
Brysbaert and New (2009)[8] replicated the results ofAdelman et al.
on the SUBTLEX-US subtitle corpus.
Mostlater studies on ï¬lm subtitles reached similar conclusions[9, 10, 11, 12, 13], but a few of them did not ï¬nd a statisticalsigniï¬cant diï¬€erence on individual datasets [14, 15, 16].All of these studies investigated only log-range across sub-title ï¬les (typically thousands of ï¬les corresponding toï¬lms or show episodes) as a dispersion measure, and eval-uated it on LDT or word naming times, essentially repli-cating Brysbaert and Newâ€™s study [8] on other languages.
Gries (2010)[17] evaluated multiple dispersion mea-sures on English word naming and LDT data.
The studydid not reach a conclusive result on the two datasets.
Gries (2021)[18] experimented with log-frequency,1ï¼‰
Adelman et al. call range â€œcontextual diversityâ€, avoiding theterms â€œdispersionâ€ and â€œrangeâ€, which is arguably confusing [6, 7],but does not detract anything from the practical value of their study.word length, and multiple dispersion measures as featuresfor a random forest model of English auditory LDT.


3 Examined Measures

As we have noted, both frequency and range are com-monly log-transformed to achieve better correlation withpsycholinguistic variables.
Since our evaluation will in-clude words not present in the corpus, which would resultin undeï¬ned values (log 0), we take the following steps toexamine the log-transformation of all examined measuresand frequency: Using ğ‘› = number of corpus parts, we applysmoothing in the form (ğ‘‘ğ‘› + 1)/(ğ‘› + 1) to each dispersionmeasure ğ‘‘ if we log-transform it.
Therefore, with a slightabuse of notation, we always use â€œlog ğ‘‘â€ in the followingtext to refer to log((ğ‘‘ğ‘› +1)/(ğ‘›
+ 1)).
For (log-)frequency,we always use Laplace-smoothed frequency [19].We use all measures in forms appropriate for unequallysized corpus parts and normalized to the range
[0, 1],adapting Gini index and Griesâ€™s DP to ğ‘‘ = 1 âˆ’ ğ‘‘âˆ—fromtheir original formulas ğ‘‘âˆ—, so that high values indicate highdispersion.
We use the following variables, given a wordğ‘¤: ğ‘› is the number of corpus parts; ğ‘£ğ‘–is the number ofoccurrences of ğ‘¤ in part ğ‘–; ğ‘˜ğ‘–is the number of tokens inpart ğ‘–; ğ‘ ğ‘–= ğ‘˜ğ‘–/Ãk is the proportion of part ğ‘–; ğ‘Ÿğ‘–= ğ‘£ğ‘–/Ãvis the proportion of occurrences of ğ‘¤ in part ğ‘–; ğ‘ğ‘–= ğ‘£ğ‘–/ğ‘˜ğ‘–is the relative frequency of ğ‘¤ in part ğ‘–, i.e. frequency nor-malized per part; ğ‘ğ‘–= ğ‘ğ‘–/Ãp is the frequency normalizedper part and per word.
For each variable ğ‘¥ğ‘–indexed bycorpus part, we understand x
= (ğ‘¥ğ‘–)ğ‘›ğ‘–=1as the correspond-ing vector with sumÃx =Ãğ‘›ğ‘–=1ğ‘¥ğ‘–, mean ğœ‡x=Ãx/ğ‘›, andstandard deviation ğœx=pÃğ‘›ğ‘–=1(ğ‘¥ğ‘–âˆ’ ğœ‡x)2/ğ‘›.We examine the following dispersion measures:Range ğ‘… =Ãğ‘›ğ‘–=1[ğ‘£ğ‘–> 0]ğ‘›(1)Gini index ğº = 1 âˆ’Ãğ‘›ğ‘–=1Ãğ‘›ğ‘—=1î˜Œî˜Œğ‘ğ‘–âˆ’ ğ‘ğ‘—î˜Œî˜Œ2ğ‘›(2)Juillandâ€™s [20] ğ· = 1 âˆ’ğœpğœ‡pâˆšğ‘› âˆ’ 1(3)Lyneâ€™s
[21] ğ·3= 1 âˆ’Ãğ‘›ğ‘–=1(ğ‘Ÿğ‘–âˆ’ ğ‘ ğ‘–)24(4)Griesâ€™s [4] DP = 1 âˆ’Ãğ‘›ğ‘–=1|ğ‘Ÿğ‘–âˆ’ ğ‘ ğ‘–|2(5)Rosengrenâ€™s
[22] ğ‘† =î˜€Ãğ‘›ğ‘–=1âˆšğ‘ğ‘–î˜2ğ‘›(6)Carrollâ€™s
[23] ğ·2=âˆ’Ãğ‘›ğ‘–=1ğ‘ğ‘–log ğ‘ğ‘–log ğ‘›(7)Regardless of the formulas above, we deï¬ne each mea-sure as 0 for words missing from the corpus.2ï¼‰Gini index is the discrete variant of the well-know in-dex of inequality.
It was proposed as a dispersion mea-sure independently by Murayama et al.
(2018)[24](asWord GINI = âˆ’log ğº) and Burch et al.
(2017)[25](asğ·ğ´, later adjusted to unequally sized parts, with a slightlydiï¬€erent normalization from our ğº
[26]).
We investigateğº and log ğº using the formula given by Glasser (1962)[27], which reduces computation time to ğ‘‚ (ğ‘› log ğ‘›).3ï¼‰As far as we can tell, â€œdistributional consistencyâ€ pro-posed by Zhang et al.
(2004)[28], is simply equal to Rosen-grenâ€™s ğ‘† (6).4ï¼‰Finally, we observe that inverse document frequency(idf) and variation coeï¬ƒcient (vc) can be expressed as lin-ear functions of log-range (1) and Juillandâ€™s D (3), respec-tively, and therefore do not need to be examined separatelyin terms of linear correlation:idf = log1ğ‘…ğ‘  = âˆ’log ğ‘… (8)vc =ğœpğœ‡p=âˆšğ‘› âˆ’ 1 (1 âˆ’ ğ·)(9)

4 Evaluation

We evaluate the measures on TUBELEX [1], a largeYouTube subtitle corpus for English, Chinese, Spanish, In-donesian, and Japanese.
Word frequency in TUBELEXwas already demonstrated to achieve correlation with psy-cholinguistic variables on par with or superior to ï¬lm sub-title corpora
[1].
TUBELEX also provides three levelsof linguistically valid corpus parts: videos, channels, andcategories (tens of thousands, thousands, and 15 parts re-spectively).
We use TUBELEX in its default tokenization.
For evaluation, we use the same datasets for LDT (3 lan-guages), word familiarity (5 languages), and lexical com-plexity (3 languages) as were used for extrinsic evaluationof TUBELEX log-frequency by Nohejl et al.
(2024)[1].Word familiarity and lexical complexity diï¬€er from thecommonly employed LDT or word naming tasks by being2ï¼‰
This is in line with the formulas for range, ğ‘†, and ğ·2, whichwould give 0 for a zero frequency word.
ğ·, ğ·3, and DP would beundeï¬ned, and Gini index would be 1.3ï¼‰
We also use sparse arrays to represent q, resulting in ğ‘‚(ğ‘š log ğ‘š)time, where ğ‘š is the number of non-zero elements of q.
The sparse-ness of frequency vectors q grows with the number of corpus parts,keeping computation time reasonable.4ï¼‰ This seems to have escaped the attention of Zhang et al.
[28] andGries.
Gries only noted that it gives the same numerical result in anexample scenario [4] and appears similar in cluster analysis [17].Table 1 Mean improvement in ğ‘…2aof the log-transformed mea-sure over the non-log-transformed (number of datasets of total 11with positive improvement, if any, in parentheses).
Cases wherelogarithm improves ğ‘…2aby at least 0.001 are printed in bold.(a)
Dispersion measures as single predictors.
Dispersion Î”ğ‘…2aof log ğ‘‘ vs. ğ‘‘ (#Datasets: Î”ğ‘…2a> 0)Measure ğ‘‘ Videos Channels CategoriesRange 0.356 (11) 0.329 (11) âˆ’0.060 (1)Gini Index 0.361
(11) 0.340 (11) âˆ’0.000 (5)Juillandâ€™s ğ· âˆ’0.230 âˆ’0.213 âˆ’0.202Griesâ€™s DP âˆ’0.095 âˆ’0.102 âˆ’0.169Rosengrenâ€™sğ‘†0.362(11)0.341(11)âˆ’0.273Carrollâ€™s ğ·2âˆ’0.218 âˆ’0.176 (1) âˆ’0.241Lyneâ€™s
ğ·3âˆ’0.112 âˆ’0.089 (1) âˆ’0.043Frequency (for comparison): 0.389 (11)(b) Two predictors: dispersion measure and log-frequency.
Dispersion Î”ğ‘…2aof log ğ‘‘ vs. ğ‘‘ (#Datasets: Î”ğ‘…2a> 0)Measure ğ‘‘ Videos Channels CategoriesRange 0.016 (8) 0.023 (10) âˆ’0.010 (3)Gini Index 0.003 (4) 0.002 (5) 0.002 (6)Juillandâ€™s ğ· âˆ’0.006 (3) âˆ’0.006 (3) âˆ’0.010 (1)Griesâ€™s DP âˆ’0.002 (3) 0.000 (7) âˆ’0.005 (3)Rosengrenâ€™s ğ‘† 0.009 (5) 0.009 (6) âˆ’0.018 (1)Carrollâ€™s ğ·2âˆ’0.010
(5) âˆ’0.006 (4) âˆ’0.014 (1)Lyneâ€™s ğ·3âˆ’0.002 (3) âˆ’0.004 (2) âˆ’0.001 (4)based on subjective ratings as opposed to reaction time,while the lexical complexity used in this case diï¬€ers fromthe other data by being rated by non-native speakers or amix of natives and non-natives.
We evaluate the dispersions in two scenarios: as singlepredictors, and as one of two predictors, the other one beinglog-frequency.
In both cases, we measure adjusted ğ‘…2[29]:ğ‘…2a= 1 âˆ’ (1 âˆ’ ğ‘…2)ğ‘› âˆ’ 1ğ‘› âˆ’ ğ‘ âˆ’ 1(10)where ğ‘› is the number of examples (dataset size) and ğ‘ isthe number of variables (1 or 2).
We compute ğ‘…2(coef-ï¬cient of determination) for linear least squares (multiple)regression ï¬tted to the whole dataset, which allows us tointerpret it as measure of (multiple) correlation strength.5ï¼‰We predict mean LDT from three studies: the En-glish Lexicon Project
[30], restricted to lower-case wordsfollowing the approach of Brysbaert and New
[8]; theMELD-SCH database
[31] of simpliï¬ed Chinese words;and SPALEX [32] for Spanish.
For English and Chinese,5ï¼‰
Using ğ‘…2instead of Pearsonâ€™s (multiple) correlation coeï¬ƒcient ğ‘Ÿ(ğ‘…) allows us to ignore the diï¬€erent polarity of the tasks (rare wordshave low familiarity but high complexity).
The adjustment is appro-priate for comparing diï¬€erent numbers of independent variables.we use the published mean LDT.
SPALEX only providesraw participant data, which we process by removing timesout of the range
[200 ms, 2000 ms][32], and comput-ing the means.
We predict mean word familiarity fromï¬ve databases: Chinese familiarity ratings [33], EnglishMRC lexical database
[34, 35], Indonesian lexical norms[36], Japanese word familiarity ratings for reception [37],and Spanish lexical norms
[38].
Lastly, we predict lexicalcomplexity for English, Spanish and Japanese using theevaluation sets of the MultiLS dataset [39].
In total, we areevaluating on 11 datasets (task-language combinations).


4.1 To Log or Not to Log

In Table 1, we compare each dispersion measure with itslog-transformed version.
Perhaps surprisingly, which oneis a better predictor does not depend solely on the mea-sure, but also on corpus part granularity, and whether themeasure is used as a single predictor or with log-frequency.
When used as single predictors (Table 1a), the logarith-mic transformation beneï¬ts range, Gini index, and Rosen-grenâ€™s ğ‘†, resulting in stronger correlations on all 11 datasetsâ€“ but only if videos and channels are used as parts.
For allthree measures, the diï¬€erence between using and not usinglog-transformation is critical (0.340 to 0.362), comparableto that between log-frequency and frequency (0.389).When dispersion measures are employed along with log-frequency (Table 1b), applying logarithm is moderatelybeneï¬cial for the same measures as above and for Giniindex for categories, but the improvements are not robustacross datasets.
We will report and discuss each measure with logarithmapplied or not applied according to these results.


4.2 Results

As shown in Figure 1 (solid bars), the only dispersionmeasure robustly stronger than log-frequency as predictorsof LDT, word familiarity, and lexical complexity is log-range for channels and videos.
Although it does not comenear in correlation strength, Griesâ€™s DP is worth noting asthe only measure performing the best with categories (thecoarsest part granularity).When used along with log-frequency, the followingmeasures result in particularly robust improvements (indecreasing order): log-range for channels, log-range forvideos, range for categories, and Rosengrenâ€™s ğ‘† for cate-Lyne's D3Gries's DPJuilland's DGini IndexCarroll's D2Rosengren's SRangeDispersion Measure (DM)0.00.10.20.30.40.5R2a (Mean Over 11 Datasets)log-frequency: 0.4270.4310.1140.4310.1220.4300.1140.4430.2910.4340.2720.4360.1520.4390.2850.4370.3810.4360.3670.4410.337 (log)0.438 (log)0.421 (log)0.440 (log)0.432 (log)0.4450.3310.4380.4220.4410.4330.451 0.3880.445 (log)0.436 (log)0.446 (log)0.441 (log)0.452 0.4010.459 (log) 0.450 (log) 0.453 (log) 0.446 (log)
Single Variable (DM)CategoriesChannelsVideosTwo Variables (DM, log-freq.)CategoriesChannelsVideosFigure
1 Mean ğ‘…2acomputed over 11 datasets for each dispersion measure, part granularity, and prediction with/without log-frequencyas a second variable, where â€œ(log)â€ indicates log-transformed measures.
Stars indicate robust predictors, namely:â‹†single predictorsthat were not signiï¬cantly (ğ‘ < 0.001) worse than log-frequency for any dataset, andâ‹†predictors that, when used with log-frequency,improved the prediction by Î”ğ‘…2aâ‰¥ 0.01 for at least 8 of 11 datasets.gories, as shown in Figure 1 (hatched bars).


5 Discussion

We extended the previous results of Adelman et al.(2006)[5] and ï¬lm subtitle studies (e.g. [8]), which showedthat log-range predicts LDT better than log-frequency, toword familiarity and lexical complexity prediction.
Moreimportantly, we found that the viability of range as a sin-gle predictor depends on (1) a ï¬ne corpus part granularity,i.e. channels or videos in the case of TUBELEX, and (2)the log-transformation.
This explains the low correlationachieved using only non-log-transformed range in somestudies, e.g. Baayen (2010)[40].
When dispersion is usedalong with log-frequency, log-range for videos and chan-nels (ï¬ne parts) are still the best choices, followed by rangeand Rosenbergâ€™s ğ‘†, both non-log-transformed and basedon categories (coarse parts).These ï¬nding oï¬€er a guideline for choosing dispersionmeasures as model variables, based on the corpus parts areavailable.
The previous studies that we know of have notcompared multiple part granularities of a single corpus.
Besides three levels of granularity, our evaluation en-compassed 11 datasets (task-language combinations).
Asthe results generally agreed across datasets, we have notreported them individually.
For instance, the robust singlepredictors (markedâ‹†in Figure 1) were signiï¬cantly betterthan log-frequency on most datasets and not signiï¬cantlydiï¬€erent on two to three of them.
We focused on the gen-eral, not the insigniï¬cant exceptions.
This highlights theimportance of evaluation on multiple datasets and puts intoperspective the insigniï¬cant diï¬€erences between log-rangeand log-frequencies on individual datasets reported in afew previous studies [14, 15, 16].We believe that linear regression, which we used foranalysis, gives more widely applicable and interpretableresults than rank correlation (Gries, 2010
[17])6ï¼‰or ran-dom forests (Gries, 2021
[18])7ï¼‰.
We hope that futureinvestigations of what we have called â€œexceptionsâ€ or lessâ€œwidely applicableâ€ bring deeper insights into speciï¬c usecases and interactions with diï¬€erent data and granularities.
Our results are immediately applicable to NLP tasks thathave relied on frequencies for modeling words perceived ascommon or simple, such as language learning applicationsor lexical simpliï¬cation.
Range data for TUBELEX, whichwe have used, is readily available as channels and videosin its word lists, and for most SUBTLEX language muta-tions as â€œcontextual diversityâ€ (CD), all based on corpusparts of comparable granularity.6ï¼‰
Rank correlation is an appropriate evaluation method for applica-tions that require only ranking, but it obscures the diï¬€erent â€œshapesâ€of dispersion metrics.7ï¼‰
With enough training data a random forest may be more ï¬ttingfor a practical application, but caution is needed when using it as anevaluation tool.
The experiment in [18] used the full data for bothtraining and testing.
Moreover, the features optimal for a randomforest and large training data may not perform well in other scenarios.



Acknowledgments

We are grateful to an anonymous reviewer of [1] forencouraging us to explore other dispersion metrics, giventhe curious result achieved by Gini index in our evaluation.

References


[1] Adam Nohejl, Frederikus Hudi, Eunike Andriani Kardinata, Shin-taro Ozaki, Maria Angelica Riera Machin, Hongyu Sun, Justin Vas-selli, and Taro Watanabe. Beyond Film Subtitles: Is YouTube theBest Approximation of Spoken Vocabulary? ArXiv preprint, Vol.arXiv:2410.03240v1 [cs], , October 2024.
[2] Hayward Keniston. Common Words in Spanish. Hispania, Vol. 3,No. 2, pp. 85â€“96, 1920.
[3] Edward Lee Thorndike. The Teacherâ€™s Word Book. Teachers College,Columbia University, 1921.
[4] Stefan Th Gries. Dispersions and adjusted frequencies in corpora.International Journal of Corpus Linguistics, Vol. 13, No. 4, pp. 403â€“437, January 2008.
[5] James S. Adelman, Gordon D.A. Brown, and JosÂ´e F. Quesada. Con-textual Diversity, Not Word Frequency, Determines Word-Naming andLexical Decision Times. Psychological Science, Vol. 17, No. 9, pp.814â€“823, September 2006.
[6] Geoï¬€ Hollis. Delineating linguistic contexts, and the validity of contextdiversity as a measure of a wordâ€™s contextual variability. Journal ofMemory and Language, Vol. 114, p. 104146, October 2020.
[7] Stefan Th. Gries. Analyzing Dispersion. In Magali Paquot and Ste-fan Th. Gries, editors, A Practical Handbook of Corpus Linguistics, pp.99â€“118. Springer International Publishing, Cham, 2020.
[8] Marc Brysbaert and Boris New. Moving beyond Kucera and Francis: Acritical evaluation of current word frequency norms and the introductionof a new and improved word frequency measure for American English.Behavior Research Methods, Vol. 41, No. 4, pp. 977â€“990, November2009.
[9] Qing Cai and Marc Brysbaert. SUBTLEX-CH: Chinese Word andCharacter Frequencies Based on Film Subtitles. PLoS ONE, Vol. 5,No. 6, p. e10729, June 2010.
[10] Maria Dimitropoulou, Jon Andoni DuËœnabeitia, Alberto AvilÂ´es, JosÂ´eCorral, and Manuel Carreiras. Subtitle-based word frequencies as thebest estimate of reading behavior: The case of Greek. Frontiers inpsychology, Vol. 1, p. 218, 2010.
[11] Emmanuel Keuleers, Marc Brysbaert, and Boris New. SUBTLEX-NL: A new measure for Dutch word frequency based on ï¬lm subtitles.Behavior Research Methods, Vol. 42, No. 3, pp. 643â€“650, August 2010.
[12] Roger Boada, Marc Guasch, Juan Haro, Josep Demestre, and Pilar FerrÂ´e.SUBTLEX-CAT: Subtitle word frequencies and contextual diversity forCatalan. Behavior Research Methods, Vol. 52, No. 1, pp. 360â€“375,February 2020.
[13] Hien Pham, Benjamin V. Tucker, and R. Harald Baayen. Constructingtwo Vietnamese corpora and building a lexical database. LanguageResources and Evaluation, Vol. 53, No. 3, pp. 465â€“498, September2019.
[14] Walter J. B. van Heuven, Pawel Mandera, Emmanuel Keuleers, andMarc Brysbaert. SUBTLEX-UK: A new and improved word frequencydatabase for British English. The Quarterly Journal of ExperimentalPsychology, Vol. 67, No. 6, pp. 1176â€“1190, 2014.
[15] Pawe l Mandera, Emmanuel Keuleers, Zoï¬a Wodniecka, and MarcBrysbaert. Subtlex-pl: Subtitle-based word frequency estimates forPolish. Behavior Research Methods, Vol. 47, No. 2, pp. 471â€“483, June2015.
[16] Walter JB van Heuven, Joshua S Payne, and Manon W Jones.SUBTLEX-CY: A new word frequency database for Welsh. QuarterlyJournal of Experimental Psychology, pp. 1052â€“1067, August 2023.
[17] Stefan Th Gries. Dispersions and adjusted frequencies in corpora:Further explorations. In Corpus-Linguistic Applications, pp. 197â€“212.Brill, January 2010.
[18] Stefan Th Gries. What do (most of) our dispersion measures measure(most)? Dispersion? Journal of Second Language Studies, Vol. 5,No. 2, pp. 171â€“205, November 2021.
[19] Marc Brysbaert and Kevin Diependaele. Dealing with zero word fre-quencies: A review of the existing rules of thumb and a suggestion foran evidence-based choice. Behavior Research Methods, Vol. 45, No. 2,pp. 422â€“430, June 2013.
[20] A.G. Juilland, D.R. Brodin, and C. Davidovitch. Frequency Dictionaryof French Words. Romance Languages and Their Structures. Mouton,1971.
[21] Anthony A. Lyne. The Vocabulary of French Business Correspon-dence: Word Frequencies, Collocations, and Problems of LexicometricMethod. Travaux de Linguistique Quantitative. Slatkine, 1985.
[22] Inger Rosengren. The quantitative concept of language and its rela-tion to the structure of frequency dictionaries.Â´Etudes de linguistiqueappliquÂ´ee, Vol. 1, p. 103, 1971.
[23] John B. Carroll. An Alternative to Juillandâ€™s Usage Coeï¬ƒcient forLexical Frequencies. ETS Research Bulletin Series, Vol. 1970, No. 2,pp. iâ€“15, 1970.
[24] Taichi Murayama, Shoko Wakamiya, and Eiji Aramaki. WORD GINI: Aproposal and application of an index to capture word usage bias [WORDGINI: Go no shiyÂ¯o no katayori wo tsukamaeru shihyÂ¯o no teian to sonoÂ¯oyÂ¯o] (in Japanese). The 24th Annual Conference of the Associationfor Natural Language Processing [Gengoshori gakkai dai 24 kai nenjitaikai], pp. 698â€“701, 2018.
[25] Brent Burch, Jesse Egbert, and Douglas Biber. Measuring and interpret-ing lexical dispersion in corpus linguistics. Journal of Research Designand Statistics in Linguistics and Communication Science, Vol. 3, No. 2,pp. 189â€“216, October 2017.
[26] Jesse Egbert, Brent Burch, and Douglas Biber. Lexical dispersion andcorpus design. International Journal of Corpus Linguistics, Vol. 25,No. 1, pp. 89â€“115, April 2020.
[27] Gerald J. Glasser. Variance Formulas for the Mean Diï¬€erence andCoeï¬ƒcient of Concentration. Journal of the American Statistical Asso-ciation, Vol. 57, No. 299, pp. 648â€“654, September 1962.
[28] Huarui Zhang, Churen Huang, and Shiwen Yu. Distributional Con-sistency: As a General Method for Deï¬ning a Core Lexicon. InMaria Teresa Lino, Maria Francisca Xavier, FÂ´atima Ferreira, RuteCosta, and Raquel Silva, editors, Proceedings of the Fourth Interna-tional Conference on Language Resources and Evaluation (LRECâ€™04),Lisbon, Portugal, May 2004. European Language Resources Associa-tion (ELRA).
[29] Mordecai Ezekiel. Methods of Correlation Analysis. Wiley, Oxford,England, 1930.
[30] David A. Balota, Melvin J. Yap, Michael J. Cortese, Keith A. Hutchison,Brett Kessler, Bjorn Loftis, James H. Neely, Douglas L. Nelson, Greg B.Simpson, and Rebecca Treiman. The English Lexicon Project. BehaviorResearch Methods, Vol. 39, No. 3, pp. 445â€“459, August 2007.
[31] Yiu-Kei Tsang, Jian Huang, Ming Lui, Mingfeng Xue, Yin-Wah FionaChan, Suiping Wang, and Hsuan-Chih Chen. MELD-SCH: A megas-tudy of lexical decision in simpliï¬ed Chinese. Behavior Research Meth-ods, Vol. 50, No. 5, pp. 1763â€“1777, October 2018.
[32] Jose Armando Aguasvivas, Manuel Carreiras, Marc Brysbaert, Pawe lMandera, Emmanuel Keuleers, and Jon Andoni DuËœnabeitia. SPALEX:A Spanish Lexical Decision Database From a Massive Online DataCollection. Frontiers in Psychology, Vol. 9, , November 2018.
[33] Yongqiang Su, Yixun Li, and Hong Li. Familiarity ratings for 24,325simpliï¬ed Chinese words. Behavior Research Methods, Vol. 55, No. 3,pp. 1496â€“1509, April 2023.
[34] Max Coltheart. The MRC psycholinguistic database. The Quar terlyJournal of Experimental Psychology A: Human Experimental Psychol-ogy, Vol. 33A, No. 4, pp. 497â€“505, 1981.
[35] M. (Max) Coltheart and Michael John Wilson. MRC PsycholinguisticDatabase Machine Usable Dictionary : Expanded Shorter Oxford En-glish Dictionary entries / Max Coltheart and Michael Wilson. OxfordText Archive, March 1987.
[36] Agnes Sianipar, Pieter van Groenestijn, and Ton Dijkstra. Aï¬€ectiveMeaning, Concreteness, and Subjective Frequency Norms for Indone-sian Words. Frontiers in Psychology, Vol. 7, , December 2016.
[37] Masayuki Asahara. Word Familiarity Rate Estimation Using a BayesianLinear Mixed Model. In Silviu Paun and Dirk Hovy, editors, Proceed-ings of the First Workshop on Aggregating and Analysing CrowdsourcedAnnotations for NLP, pp. 6â€“14, Hong Kong, November 2019. Associa-tion for Computational Linguistics.
[38] Marc Guasch, Pilar FerrÂ´e, and Isabel Fraga. Spanish norms for aï¬€ectiveand lexico-semantic variables for 1,400 words. Behavior ResearchMethods, Vol. 48, No. 4, pp. 1358â€“1369, December 2016.
[39] Matthew Shardlow, Fernando Alva-Manchego, Riza Batista-Navarro,Stefan Bott, Saul Calderon Ramirez, RÂ´emi Cardon, Thomas FrancÂ¸ois,Akio Hayakawa, Andrea Horbach, Anna HÂ¨ulsing, Yusuke Ide,Joseph Marvin Imperial, Adam Nohejl, Kai North, Laura Occhipinti,Nelson PerÂ´ez Rojas, Nishat Raihan, Tharindu Ranasinghe, Martin SolisSalazar, SanjaË‡Stajner, Marcos Zampieri, and Horacio Saggion. TheBEA 2024 Shared Task on the Multilingual Lexical Simpliï¬cationPipeline. In Ekaterina Kochmar, Marie Bexte, Jill Burstein, AndreaHorbach, Ronja Laarmann-Quante, AnaÂ¨Ä±s Tack, Victoria Yaneva, andZheng Yuan, editors, Proceedings of the 19th Workshop on InnovativeUse of NLP for Building Educational Applications (BEA 2024), pp.571â€“589, Mexico City, Mexico, June 2024. Association for Computa-tional Linguistics.
[40] R. H. Baayen. Demythologizing the word frequency eï¬€ect: A discrim-inative learning perspective. The Mental Lexicon, Vol. 5, No. 3, pp.436â€“461, January 2010.