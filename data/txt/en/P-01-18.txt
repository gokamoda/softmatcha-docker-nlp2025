mBART for Supervised Gloss-Free Sign Language Translation:Integrating RGB and Facial Keypoint Images

Jiannan Mao

1âˆ—

â€ƒ Chenchen Ding

2

â€ƒHour Kaing

2

â€ƒHideki Tanaka

2

â€ƒMasao Utiyama

2

â€ƒTadahiro Matsumoto

11

Gifu University, Gifu, Japan â€ƒ

2

ASTREC, UCRI, NICT, Japan



â€ƒ{mao, tad}@mat.info.gifu-u.ac.jp



â€ƒ{chenchen.ding, hour kaing, hideki.tanaka, mutiyama}@nict.go.jp



Abstract

Sign language translation (SLT) has traditionally de-pended on gloss annotations, which are costly and time-consuming to produce.
This work presents a gloss-free SLTframework that integrates raw RGB video input with facialkeypoint features, enabling richer visual representations.
We leverage a two-stage approach: ï¬rst aligning visualand textual features with a frozen multilingual mBARTencoder, then reï¬ning translation through the mBART de-coder.
Evaluations on the PHOENIX-2014T dataset showperformance gains over baselines, yielding a +0.64 BLEUimprovement.
These results conï¬rm that incorporating fa-cial keypoints strategy can signiï¬cantly improve gloss-freesign language translation.


1 Introduction

Sign languages are visual signals used for communi-cation among the Deaf or Hard of Hearing (DHH).
Theselanguages are primarily expressed through manual articula-tions, but they are also greatly enr iched by the movement ofthe body, mouth, eyes, and eyebrows.
This visual complex-ity not only enhances the expressiveness of sign languagesbut also helps convey thoughts more clearly
[1, 2].For those of us with intact hearing and speaking abili-ties, there is often a misconception that â€œDHH individualsprefer reading spoken language; therefore, when commu-nicating with DHH, it is suï¬ƒcient to rely solely on spokenlanguage, using written text, either on paper or via smartdevices [3].â€
This perspective fails to account for a criti-cal fact: for many DHH individuals, sign language is notâˆ—This work was done during the ï¬rst authorâ€™s internship at NationalInstitute of Information and Communications Technology, Kyoto,Japan.merely a communication tool but their primary and mostnatural language, deeply intertwined with their identity andculture
[4, 5].
Unlike written spoken languages, which mayfeel secondary or foreign to many DHH individuals, signlanguage provides a more direct and expressive connectionto their thoughts and emotions [5].In this context, Sign Language Translation (SLT) sys-tems are essential to bridge the communication gap be-tween DHH individuals and the hearing population, en-abling more meaningful interactions and fostering a societythat tr uly values diversity, equity, and
inclusion
[6].To address this challenge, researchers have explored var-ious approaches inspired by the Neural Machine Transla-tion (NMT) framework, adapted to handle visual inputs.
Typically, a sequence of video frames is processed by avisual network to either predict glosses, or extract imagefeatures, which are then mapped to spoken language usingNMT
[7, 8, 9, 10], as shown in Figure 1.
While glossesenhance translation accuracy, their production demandscostly, time-intensive manual annotation.
As a result,gloss-free SLT has emerged as a trend, aiming to directlytranslate raw video into text [9, 11, 12].This work focuses on gloss-free sign language transla-tion, employing the mBART model as a teacher modelto supervise the outputs of a visual network through theencoder outputs of mBART.
For the visual network, weutilize a combination of RGB images and facial keypoints,enable the model to capture more detailed facial featuresduring the learning process.
Our experimental results showa signiï¬cant improvement, with an increase of 0.64 BLEUpoints over the baseline.
This approach not only conï¬rmsthe eï¬€ectiveness of combining RGB images with facialkeypoints for visual feature extraction but also underscores

11August_2010_Wednesday_tagesschau-2Spoken Languages: tiefer luftdruck bestimmt in den nÃ¤chsten tagen unser wetterï¼ˆ low air pressure will determine our weather in the next few days ï¼‰Gloss Annotations (gloss-based):  DRUCK TIEF KOMMENImage Features (gloss-free):Visual Network......encoderdecoderFigure 1
The diï¬€erence between gloss-free and gloss-based approaches in sign language translation.
In the example, we demonstratedthe diï¬€erence in word order between sign and spoken languages : DRUCK corresponds to luftdruck, and TIEF corresponds to tiefer.the eï¬ƒcacy of employing mBART as a supervisory model.


2 Related Works

Gloss-Free Glosses are written labels used to repre-sent gestures in sign language, providing a stable represen-tation units by segmenting continuous gestures into discretelexical elements.
For instance, the gesture for â€˜Put the bookon the tableâ€™ might be glossed as â€˜PUT BOOK TABLEâ€™.
While glosses act as a bridge between sign and spokenlanguage, they are not equivalent to spoken words.
Theyfollow the syntax of sign language, which can diï¬€er fromspoken language order, as shown in Figure 1.Currently, the majority of gloss-free sign language trans-lation studies rely on datasets without gloss annotations[13, 14, 15].
Our work follows this gloss-free researchdirection.
For comparability with the baseline [9], we usea dataset containing gloss annotations [16] but completelydisregard the gloss information.
Facial Keypoints Visual feature extraction is cru-cial for sign language translation.
Combining keypointswith RGB images has been shown to improve recog-nition accuracy by oï¬€ering richer visual representations[8, 17].
In particular, incorporating facial keypoints en-hances these representations, as they provide ï¬ne-grainedsemantic cues, such as expressions, which help distinguishambiguous gestures.
In this work, we extract facial keypoints to enhance themodelâ€™s ability to learn detailed facial features, which areoften overlooked in existing studies.mBART in SLT mBART is a pre-trained multilin-gual model that using denoising autoencoding to captureuniversal linguistic features
[18].
It has demonstratedstrong performance in low-resource tasks, including signlanguage translation [7, 8, 9, 10].We adopt mBART as a teacher model to supervise thevisual networks, aligning its outputs with the encoder rep-resentations of mBART.
This strategy bridges the gap be-tween visual features and linguistic understanding, result-ing in improved translation performance.


3 Method

Our method employs a two-stage framework for gloss-free sign language translation, as shown in Figure 2.
For thevisual feature extraction network, we adopt the approachproposed by Zhou et al.
[9], which combines ResNet [19],temporal blocks
[20], and parts of the mBART encoder
[18]to form the visual network.
Two separate visual networkare utilized: one processes the raw RGB image sequences,while the other processes facial keypoints.
These visualfeatures are aligned with the frozen mBART encoder inthe ï¬rst stage, and further integrated and trained with themBART decoder in the second stage, enabling the transla-tion from sign language to spoken language.
Data Preparation
The original dataset ğ· containRGB sequences ğ‘‘ğ‘Ÿğ‘”ğ‘and corresponding text ğ‘‘ğ‘¡ğ‘’ğ‘¥ğ‘¡: ğ· =(ğ‘‘ğ‘Ÿğ‘”ğ‘, ğ‘‘ğ‘¡ğ‘’ğ‘¥ğ‘¡).
We extract the facial keypoints sequenceğ‘‘ğ‘˜ğ‘’ğ‘¦for each ğ‘‘ğ‘Ÿğ‘”ğ‘using HRNet
[21], forming ğ·â€²=(ğ‘‘ğ‘Ÿğ‘”ğ‘, ğ‘‘ğ‘˜ğ‘’ğ‘¦, ğ‘‘ğ‘¡ğ‘’ğ‘¥ğ‘¡).
Here, ğ‘‘ğ‘Ÿğ‘”ğ‘, ğ‘‘ğ‘˜ğ‘’ğ‘¦are sequences inâ„ğ‘ Ã—ğ¿ğ‘Ÿğ‘”ğ‘, and ğ‘‘ğ‘¡ğ‘’ğ‘¥ğ‘¡âˆˆ â„ğ‘ Ã—ğ¿ğ‘¡ğ‘’ ğ‘¥ğ‘¡, where ğ‘ is the datasetsize.
These augmented data provide richer visual cuesthat support more nuanced interpretation of signs imagesequences.

tiefer luftdruckbestimmt  in den nÃ¤chsten tagen unser wetterHRNetVisual NetworkVisual Networktiefer luftdruckbestimmt  in den nÃ¤chsten tagen unser wetterencoderdecoderencoderdecoder(R,T)(K,T)First Stage TrainingSecond Stage TrainingData Preparation / Second Stage First StageDiscardFigure 2 Overview of our two-stage framework.
In the ï¬rst stage, we align visual (RGB + facial keypoints) and textual features usinga frozen mBART encoder.
In the second stage, we fuse the learned visual representations and train the mBART decoder to produce theï¬nal translations.
Colored elements in background correspond to the main focus at each stage.
First Stage: Feature Alignment Sign and spokenlanguage sequences diï¬€er in length (ğ¿ğ‘Ÿğ‘”ğ‘â‰  ğ¿ğ‘¡ğ‘’ğ‘¥ğ‘¡), mak-ing direct alignment challenging.
To address this, we in-troduce special markersâŸ¨ğ‘Ÿğ‘”ğ‘âŸ©,âŸ¨ğ‘˜ğ‘’ğ‘¦âŸ©, andâŸ¨ğ¸ğ‘‚ğ‘†âŸ©at theend of each respective sequence.
These markers produce aglobal summar y vector for each modality.
We deï¬ne two visual networks: ğ‘“ğ‘Ÿğ‘”ğ‘for RGB inputsand ğ‘“ğ‘˜ğ‘’ğ‘¦for facial keypoints.
Both employ ResNet [ 19],temporal blocks
[20], and parts of the mBART encoderarchitecture [18] as per [9].
Extracted features are:ğ‘… = ğ‘“ğ‘Ÿğ‘”ğ‘(ğ‘‘ğ‘Ÿğ‘”ğ‘,âŸ¨ğ‘Ÿğ‘”ğ‘âŸ©), ğ¾ = ğ‘“ğ‘˜ğ‘’ğ‘¦(ğ‘‘ğ‘˜ğ‘’ğ‘¦,âŸ¨ğ‘˜ğ‘’ğ‘¦âŸ©)Here, ğ‘… and ğ¾ represent the feature vectors extracted fromthe respective positions of the markersâŸ¨ğ‘Ÿğ‘”ğ‘âŸ©andâŸ¨ğ‘˜ğ‘’ğ‘¦âŸ©,which aggregate the information from the entire sequence.
For the text sequence, we employ the mBART encoder,ğ‘šğµğ´ğ‘…ğ‘‡ğ‘’ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ, as a feature extractor.
During this phase,ğ‘šğµğ´ğ‘…ğ‘‡ğ‘’ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿremains frozen to serve as a teacher model,to ensure robust and stable supervision through text em-beddings.
The text sequence is processed as follows:ğ‘‡ = ğ‘šğµğ´ğ‘…ğ‘‡ğ‘’ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ(ğ‘‘ğ‘¡ğ‘’ğ‘¥ğ‘¡,âŸ¨ğ¸ğ‘‚ğ‘†âŸ©)Here, ğ‘‡ is the feature vector extracted at the position ofâŸ¨ğ¸ğ‘‚ğ‘†âŸ©, capturing the overall semantics of the text.
To ensure eï¬€ective alignment between visual and textualmodalities, the loss function maximizes their dot-productsimilarity in the shared latent space, as follows:L= âˆ’12î˜î›•log sim(ğ‘…, ğ‘‡) +î›•log sim(ğ¾, ğ‘‡)î˜‘Here, ğ‘ ğ‘–ğ‘š(ğ‘…, ğ‘‡) measures the similarity between featurevectors.
By maximizing these alignments, the model learnsto capture the semantic correspondences between the visualand text modalities in the ï¬rst stage of training.
Second Stage: Translation At this stage, we utilizethe RGB image visual network ğ‘“ğ‘Ÿğ‘”ğ‘and the facial key-point visual network ğ‘“ğ‘˜ğ‘’ğ‘¦, obtained from the ï¬rst stage oftraining, to extract features from the input data ğ‘‘ğ‘Ÿğ‘”ğ‘andğ‘‘ğ‘˜ğ‘’ğ‘¦. Subsequently, we fuse these two features using theVisual-Language Mapper
[7], a fully-connected MLP withtwo hidden layers, as follows:ğ¹ğ‘Ÿğ‘”ğ‘= ğ‘“ğ‘Ÿğ‘”ğ‘(ğ‘‘ğ‘Ÿğ‘”ğ‘), ğ¹ğ‘˜ğ‘’ğ‘¦= ğ‘“ğ‘˜ğ‘’ğ‘¦(ğ‘‘ğ‘˜ğ‘’ğ‘¦)ğ¹ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›= ğ‘‰ ğ¿ ğ‘€ ([ğ¹ğ‘Ÿğ‘”ğ‘, ğ¹ğ‘˜ğ‘’ğ‘¦])The fused features ğ¹ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›are fed into the mBART de-coder for training, and the loss for this process is deï¬nedas follows:
L= âˆ’î›ğ‘™ğ‘œğ‘”ğ‘ƒ(ğ‘‘ğ‘–ğ‘¡ğ‘’ğ‘¥ğ‘¡|ğ¹ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›, ğ‘‘<ğ‘–ğ‘¡ğ‘’ğ‘¥ğ‘¡)The second stage bridges visual and textual modalities bydirectly optimizing the translation task.
Leveraging thefused features ğ¹ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘›as input to the mBART decoder,it ensures ï¬‚uent, semantically accurate spoken languagetranslations.
While the ï¬rst stage focuses on feature align-ment through similarity maximization, this stage reï¬nesend-to-end translation quality by ï¬ne-tuning the decoderand visual network within a shared representation space.
Together, these two stages enable progressive learning of

semantic alignment, resulting in robust, accurate gloss-freesign language translation.


4 Experiments



4.1 Settings

The experiments were conducted on the PHOENIX-2014T dataset [16], which contains 8,257 German SignLanguage (DGS) videos paired with corresponding Ger-man translations drawn from weather forecast broadcasts.
The dataset is split into training (7,096), development(519), and test (642) sets.
We strictly follow a gloss-freescenario by not using any provided gloss annotations.
In the ï¬rst training stage, we utilize only the mBART-cc25 [18] encoder (frozen as a teacher model) to providetextual supervision.1ï¼‰In the second stage, we employ theï¬rst three layers of the mBART-cc25 decoder to gener-ate ï¬nal translations.
All other training hyperparametersclosely follow Zhou et al.
[9] to ensure consistency.
We evaluate our model using standard automatic met-rics: BLEU-4
[22] and ROUGE
[23], allowing direct com-parison to previous work.

4.2 Results and Discussions

Table 1 presents a comparison of our gloss-free signlanguage translation method against both gloss-based andgloss-free baselines on the PHOENIX-2014T dataset.
Forgloss-based approaches, methods such as MMTLB
[7],TS-SLT [8], and CV-SLT [10] achieve relatively high per-formance, with CV-SLT scoring the highest BLEU-4 andROUGE values (29.27 and 54.33, respectively).
Thesemethods beneï¬t from annotated gloss intermediates, whichprovide an explicit linguistic bridge between sign and spo-ken language, thus improving translation quality.
In contrast, our work, along with GFSLT
[9], focuses ona gloss-free scenario, which is more challenging due to theabsence of explicitly annotated sign glosses.
Within thissetting, GFSLT-rgb relies solely on raw RGB video frames,while GFSLT-key substitutes RGB input with whole hu-man body keypoints extracted via HRNet.
Interestingly,the GFSLT-key variant, which encodes the entire bodyskeletal motion, achieves lower scores (16.08 BLEU-4,35.21 ROUGE) than GFSLT-rgb (21.44 BLEU-4, 42.49ROUGE).
This suggests that while body keypoints provide1ï¼‰ huggingface.co/facebook/mbart-large-cc25Table 1 Results on the PHOENIX-2014T dataset.
Improverepresents the gains of our method compared to GFSLT-rgb[9].Gloss Method BLEU-4 ROUGEbasedMMTLB
[7] 28.39 52.65TS-SLT [8] 28.95 53.48CV-SLT [10] 29.27 54.33freeGFSLT-rgb
[9] 21.44 42.49GFSLT-key 16.08 35.21rgb+key facial(our) 22.08 44.12Improve - +0.64 +1.63skeletal motion cues, they may lose important visual de-tails (e.g., subtle body movements, hand shape nuances)that contr ibute to accurate sign interpretation.
Our proposed method, denoted as rgb+key facial(our),combines the strengths of raw RGB input with facial key-points.
This fusion outper forms both GFSLT-rgb andGFSLT-key, improving BLEU-4 by +0.64 and ROUGE by+1.63 over the RGB-only baseline.
The improvement in-dicates that integrating detailed facial cues with the globalscene information from RGB frames leads to more se-mantically aligned and contextually richer representations,ultimately enhancing translation performance.
Despite these gains, gloss-free methods, including ours,still exhibit a performance gap compared to gloss-based ap-proaches.
Nevertheless, our results demonstrate that care-fully selecting and fusing multiple visual cues can mitigatethe challenges posed by the lack of gloss annotations.


5 Conclusion and Future Work

This study extends existing eï¬€orts in gloss-free signlanguage translation, an area in sign language processingwhere no intermediate gloss annotations are used.
We pro-pose an approach that integ rates RGB frames with facialkeypoint data.
By exploiting complementary informationfrom these modalities, our method better captures complexspatial and temporal patterns of sign language.
This mul-timodal strategy improves translation accuracy comparedto relying solely on RGB inputs and may facilitate moreaccurate, context-aware translations.
In future work, we plan to explore additional visual cues,reï¬ne fusion strategies, and incorporate more powerful lan-guage models to further enhance gloss-free sign languagetranslation.



Acknowledgement

This work was ï¬nancially supported by JST SPRING,Grant Number JPMJSP2125. The author (Initial) wouldlike to take this opportunity to thank the î¢ THERS MakeNew Standards Prog ram for the Next Generation Re-searchers.î¢¡

References


[1] Annika Herrmann and Markus Steinbach. Nonmanualsin sign language, Vol. 53. John Benjamins Publishing,2013.
[2] Kathleen, Paul, and Dot Sign Language. Britishsign language. https://bsl.surrey.ac.uk/principles/f-non-manual-features.
[3] Razieh Rastgoo, Kourosh Kiani, Sergio Escalera, VassilisAthitsos, and Mohammad Sabokrou. A survey on recentadvances in sign language production. Expert Systemswith Applications, Vol. 243, p. 122846, 2024.
[4] H.D.L. Bauman. Open Your Eyes: Deaf Studies Talk-ing. University of Minnesota Press, 2008.
[5] A. Mindess. Reading Between the Signs: Inter-cultural Communication for Sign Language Inter-preters. Intercultural Press, 2011.
[6] United Nations. Convention on the Rights of Persons withDisabilities (CRPD). https://www.ohchr.org/en/instruments-mechanisms/instruments/convention-rights-persons-disabilities.
[7] Yutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, andStephen Lin. A simple multi-modality transfer learningbaseline for sign language translation. In Proceedings ofthe IEEE/CVF conference on computer vision andpattern recognition, pp. 5120â€“5130, 2022.
[8] Yutong Chen, Ronglai Zuo, Fangyun Wei, Yu Wu, ShujieLiu, and Brian Mak. Two-stream network for sign languagerecognition and translation. NeurIPS, 2022.
[9] Benjia Zhou, Zhigang Chen, Albert ClapÂ´es, Jun Wan,Yanyan Liang, Sergio Escalera, Zhen Lei, and Du Zhang.Gloss-free sign language translation: Improving fromvisual-language pretraining. In Proceedings of theIEEE/CVF International Conference on ComputerVision (ICCV), pp. 20871â€“20881, October 2023.
[10] Biao Fu Cong Hu Jinsong Su Yidong Chen Rui Zhao,Liang Zhang. Conditional variational autoencoder for signlanguage translation with cross-modal alignment. In Pro-ceedings of the AAAI Conference on Artiï¬cial In-telligence, 2024.
[11] Jinhui Ye, Xing Wang, Wenxiang Jiao, Junwei Liang, andHui Xiong. Improving gloss-free sign language translationby reducing representation density. 2024.
[12] Zhigang Chen, Benjia Zhou, Yiqing Huang, Jun Wan, YiboHu, Hailin Shi, Yanyan Liang, Zhen Lei, and Du Zhang.C2rl: Content and context representation learning forgloss-free sign language translation and retrieval, 2024.
[13] Samuel Albanie, GÂ¨ul Varol, Liliane Momeni, Hannah Bull,Triantafyllos Afouras, Himel Chowdhury, Neil Fox, Ben-cie Woll, Rob Cooper, Andrew McParland, et al. Bbc-oxford british sign language dataset. arXiv preprintarXiv:2111.03635, 2021.
[14] Amanda Duarte, Shruti Palaskar, Lucas Ventura, DeeptiGhadiyaram, Kenneth DeHaan, Florian Metze, Jordi Tor-res, and Xavier Giro-i Nieto. How2Sign: A Large-scaleMultimodal Dataset for Continuous American Sign Lan-guage. In Conference on Computer Vision and Pat-tern Recognition (CVPR), 2021.
[15] Bowen Shi, Diane Brentari, Greg Shakhnarovich, andKaren Livescu. Open-domain sign language translationlearned from online video. In EMNLP, 2022.
[16] Necati Cihan Camgoz, Simon Hadï¬eld, Oscar Koller, Her-mann Ney, and Richard Bowden. Neural sign languagetranslation. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pp. 7784â€“7793, 2018.
[17] Hao Zhou, Wengang Zhou, Yun Zhou, and Houqiang Li.Spatial-temporal multi-cue network for continuous signlanguage recognition. In Proceedings of the AAAI con-ference on artiï¬cial intelligence, Vol. 34, pp. 13009â€“13016, 2020.
[18] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, SergeyEdunov, Marjan Ghazvininejad, Mike Lewis, and LukeZettlemoyer. Multilingual denoising pre-training for neuralmachine translation. Transactions of the Associationfor Computational Linguistics, Vol. 8, pp. 726â€“742,2020.
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun. Deep residual learning for image recognition. InProceedings of the IEEE conference on computervision and pattern recognition, pp. 770â€“778, 2016.
[20] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An em-pirical evaluation of generic convolutional and recurrentnetworks for sequence modeling. arxiv. arXiv preprintarXiv:1803.01271, Vol. 10, , 2018.
[21] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu,Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao.Deep high-resolution representation learning for visualrecognition. IEEE Transactions on Pattern Analysisand Machine Intelligence, Vol. 43, No. 10, pp. 3349â€“3364, 2021.
[22] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation ofmachine translation. In Proceedings of the 40th an-nual meeting of the Association for ComputationalLinguistics, pp. 311â€“318, 2002.
[23] Chin-Yew Lin. Rouge: A package for automatic evaluationof summaries. In Text summarization branches out,pp. 74â€“81, 2004.