Loss as a Data Introspection Method: Looking into JapaneseAdvertising Text Generation

Joseph Foran Arseny Tolmachev



Hakuhodo Technologies



{joseph.foran,arseny.teramachi}@hakuhodo-technologies.co.jp



Abstract

This paper presents a case study that examines the dis-tribution of training loss values in a Japanese advertisingtext generation model.
Using a LongT5 architecture, weanalyze the characteristics of training examples that exhibitboth high and low loss values.
Our ﬁndings reveal severalkey patterns: low-loss examples often contain repetitivephrases and standardized advertising terminology, whilehigh-loss examples tend to feature more complex gram-matical structures and natural language patterns.
We alsoidentify potential issues in training data quality and dis-cuss their implications for model performance.
We ﬁndthat measuring training loss per example in the trainingdata is a useful diagnostic tool, for better understandingmodel characteristics.


1 Introduction

Recent dramatic improvements in language models haveresulted in signiﬁcant improvements in many Natural Lan-guage Processing tasks.
One such domain is the automaticgeneration of advertising text.
As the share of online adver-tising, in particular search advertising, continues to growwithin the overall advertising market, further automatingthis task is of increasing economic importance.
In thiswork, we introduce a case study in which we examine thedistribution of loss values across a dataset used to traina model that generates advertising text and present someﬁndings that may be of interest to the community.


1.1 Advertising Text Generation

In this work, we focus on the generation of advertisingtext for search advertisements.
These are advertisementsthat are displayed in connection to a user’s search query,and the aim is to display advertisements that will be of rel-     Figure 1
A plot of a kernel density estimate of the loss valuesper training example for our model.
We have annotated it withsome examples from the training set and their position in the lossspectrum.evance to the user based on keywords used in or related tothe search query.
Advertisers submit assets, consisting ofheadlines and description text, to an advertising platform.
They associate keywords with such assets, and when suchkeywords are triggered an auction is conducted in order todecide whose advertisements will be displayed.
The plat-form will display from the winning assets a combination ofheadlines and description texts, the composition of whichare decided by a black-box algorithm.
Because the combi-nation that will be displayed is not known beforehand, theadvertiser should ensure that any combination of headlinesand descriptions is coherent.
The two categories of text that are to be generated bythis model, headlines (sometimes referred to as titles), anddescriptions, have diﬀering characteristics.
In particular,the length of the text and font size used when displayingon a search results page diﬀer for these two types of text.
Also, as the name suggests, headlines tend to be snappier,whereas the descriptions will be longer and have additionaldetail.

In general, Advertising Text Generation (ATG) modelsgenerate advertising text based on the contents of a LandingPage (LP) that contains details of the product / servicebeing advertised.
This task can be considered similar tosummarization, though the strict length requirements andneed to entice user’s interest cause it to diﬀer in signiﬁcantways from typical summarization tasks.
Also, rather thana single summary, it is desirable to generate a large varietyof texts based on the LP, each emphasizing diﬀerent appealpoints of the product or service being promoted.
To be eﬀective, advertisements need to attract the user’sinterest, leading them to the LP with the aim of convertingthat interest into a sale.
Eﬀective advertisements conveyinformation about the product/service that is of relevanceto the user.
Usually products and services have multiplefeatures that appeal to diﬀerent users.
Generally the modelshould be able to generate many diﬀerent texts that cover allthese selling points.
But while diversity in text generationis desirable, this is balanced by another important require-ment in that they are truthful regardng what is on oﬀer, asfalse claims can cause ﬁnancial or reputational damage tothe advertiser.
For this reason, ATG models should alsoaim to be faithful to the contents of the LP which they useas input.


1.2 Data Rejuvenation

Our motivation for examining the training loss distribu-tion was as a step in applying the Data Rejuvenation
[1]method.
This method identiﬁes training examples that con-tribute less to the performance of the model, and replacesthe target side of the training data with a synthetically gen-erated example.
Speciﬁcally, there are ﬁve general steps1.
Using all the original training data to train a model.2.
Identify each example’s contribution to the model anddivide the original training data into ”eﬀective” and”ineﬀective” sets.3. Train a second model only on the ”eﬀective” set.4.
Use this model to generate pseudo-data using thesource side of the ”ineﬀective” dataset, thus ”reju-venating” these examples.5.
Use a combination of the ”eﬀective” dataset and thepsuedo-data of the rejuvenated ”ineﬀective” datasetto train a ﬁnal model.
Jiao et.
al
[1] show that this methodology can improvethe quality of Neural Machine Translation(NMT) models.
Although this method was originally proposed for that par-ticular task, in principle, it should be applicable in generalto any sequence to sequence generation task.
In this work,we use the idea of examining the training data loss spectrumto the aforementioned ATG task.
That is we perform steps1 and 2 above, and examine in more detail the distributionof training losses across all examples.


1.3 LongT5

The architecture of our generative model is the LongT5model [2].
This is a Transformer [3] based model with anencoder-decoder structure.
During training, the LP con-tents are fed into the encoder side, while the target adver-tising text is shown to the decoder, preceded by a preﬁxprompt token that indicates the target type i.e. headline ordescription.
During inference the desired type of outputcan be controlled by setting this preﬁx prompt token.


2 Training Details



2.1 Training methodology

We pre-train a LongT5 model using Japanese portionsof publicly available text corpora.
We then ﬁne-tune thismodel using an advertising text dataset.
In this dataset,the source side of our examples contains text about theadvertised good or service, while on the target side we useassociated advertising texts.


3 Analysis of training loss distribu-



tion

After completion of training of our model, we measuredthe cross-entropy loss per example for all items in ourtraining set, using the ﬁnal model’s parameters.
In Figure1 we show a density plot of this distribution, calculatedusing the Gaussian KDE algorithm
[4] as implemented inSciPy
[5].It can be noticed that the distribution of cross-entropyloss is close to being unimodal.
We examined examplesat various points along the distribution of losses.
In Table1 (Headlines) and Table A2 (Descriptions) we provide ex-amples from both the lowest, middle and highest deciles ofcross-entropy loss.
When we examined examples from diﬀerent parts of thespectrum, we found some characteristics that were of inter-est.
Firstly, many low-loss targets were repeated frequently

Low Medium Highビルトイン食洗機が最大 70 ％OFF 初めてのお買い物で送料無料脱毛が毎回違う店舗や人だけど…浴室乾燥機交換／最大 67 ％OFF 秋葉原で評判の歯医者あれあったっけが分かる冷蔵庫株主優待券の中古/未使用品折り畳み椅子ロータイプ仮面様顔貌も脳の回復で改善電気柵の中古/未使用品電気殺虫器室内でより確かな位置検出に格安新幹線＋宿泊パック比較洗濯機で洗える羽毛布団富栄養化を改善お役立ち資料無料配布中破格の 100 枚 150 円から家の隙間から侵入する煙対策フィルター専門店だから種類豊富発送ビニール袋 1 枚 14 円〜 出掛けすに家でスマホ副業宅配買取で全国から高く買取ますライフコーチングを受ける台東嘉明湖３日團免背公糧睡袋無料サンプル最短当日で出荷防犯カメラ屋外電池式知って楽しい役立つ調理セミナー工事不要ですぐに使える車載ソーラーパネル海外で大人気のハンドメイド上海行きの格安フライト東京都の産業医をご紹介ブラウザ内で簡単に文字起こしお手頃中古車や未使用車が多数離れていても一体感を感じられる漢方茶とあなたの体質相性は？和光市ホテルペチコートワンピースカップ付き頚椎を寝ながらストレッチする枕花器花瓶の人気アイテムタフトシートカバー当日前日予約枠ご用意中10 万円の少額から始められる団地間 4.5 畳福祉ネイリストの需要が増えてる【法人向け】SMS 配信 1 通 5 円〜 長岡市で葬儀／小さなお葬式食洗機取り付け業者大阪専門家バイク保険【安く抑えたい】方へスコップ収納ケースなら可愛い覆い袋にお納めしてご返骨名古屋激安ホテル 1 枚から格安で印刷、後払い OK 高校で本格イラスト。体験授業も3L,4L,5L サイズの商品が豊富ホース径変換風呂の床だけリフォームが 1 日で【格安】月額￥2,900 から探せるバイク売却 10 社一括査定神奈川第 1 位の実績アリ年間販売台数 13 万台の販売実績 20 業種, 累計 2,500 万人以上が利用土に落ちた成分は自然物に分解新作が試せる！ 10/12 まで返品送料無料屋外用ロールカーテン出しずらい高声も初回で上達自転車八王子の中古/未使用品気になる使用感事業承継で安定成長と雇用維持を銀行金利よりも高い利回り 7.0% ネットワークの安定性向上高精度工作機械メーカの歯研機Table 1 Headlines examples.
These examples were sampled from the lower, middle and high deciles of the training loss spectrum.in the data, with diﬀering source examples.
This indicatesthat there is possibly needless repetition of these exampleswhich may harm the ability of the model to generate mul-tiple diverse samples when given similar sources, as theseexamples become memorized.
One characteristic of theATG task that diﬀers from many other NLP tasks is thatnovelty is especially required
and so the introduction oftoo strong a training signal by repetitive targets, even if thesources for each target diﬀers, should be avoided.
As was perhaps to be expected, among the high-levelloss examples were found some examples that got throughthe training data cleaning / ﬁltering process.
In particularthere were a few examples where the language was notJapanese and others where it was Japanese but there weremisspellings or bad grammatical mistakes.
These point outissues with the data collection process.
Another characteristic found was that many low lossexamples followed well established patterns.
For example,many low loss examples started with the phrase “【公式】”and were followed by a brand name1）which is commonlyused in Japanese advertising text.
It was also noted that for both headlines and descrip-tions, the low loss examples tended to be more fragmentedand to use phrase-like structures more.
They tend to bedeclarative and snappy.
On the other hand, high-loss ex-amples tended to use more complete sentence structuresand natural Japanese expressions.
They also tend to usemore varied grammatical structures, while there is a ten-dency for similar phrases to be repeated in the low-lossexamples e.g 無料, 豊富なの品揃え。
Examples in themiddle of the range were closer to everyday Japanese, hav-ing less commerical / retail vocabulary than the low lossexamples, but at the same time tending towards havingsimpler vocabulary and kanji than the high-loss examples.1）
Examples containing trademarks or company names are excludedfrom this paper, so we do not provide speciﬁc examples.

The products referred to in these examples seem to be moreeveryday than the more specialised / niche products that thehigh-loss examples tend towards.
Focusing on Descriptions, low-loss examples tend to belist-like and categorical.
This is likely also inﬂuenced bythe nature of LPs which often contain such lists of featuresand products.
The model in these cases has likely learned toassign high probability to phrases that it encounters in thesecontexts.
High-loss examples tend to be more narrative andexplanatory, while the mid-range examples fall somewherein between, with more formal and direct language than thehigh-loss examples but with less incidents of repetitive,list-type structures.


4 Conclusions and Future work

Via this case study, we have shown that analysing thedistribution of training data can help give good insightsinto the model, highlighting potential improvements to bemade to the training data.
In particular, examining theexamples at the extremes of the training loss distributionprovided insights, such as the presence of highly frequenttarget advertisements which may harm the model’s abilityto generate diverse outputs.
We found that the linguisticcharacteristics of examples that the model found hard tolearn diﬀered from those it found relatively easy to learn.
For future work, we plan to apply the rest of the Data Re-juvenation stages and investigate whether it leads to theimprovement of the ability of our models to generate di-verse and ﬂuent advertising text.


References

[1] Wenxiang Jiao, Xing Wang, Shilin He, Irwin King, MichaelLyu, and Zhaopeng Tu.
Data rejuvenation: Exploiting in-active training examples for neural machine translation.
InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pp.
2255–2266.
Association for Computational Linguistics,2020.[2]
Mandy Guo, Joshua Ainslie, David Uthus, Santiago On-tanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.
LongT5:
Eﬃcient text-to-text transformer for long se-quences.
In Findings of the Association for Computa-tional Linguistics: NAACL 2022, pp. 724–736, Seattle,United States, 2022.
Association for Computational Lin-guistics.[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and IlliaPolosukhin.
Attention is all you need.
In Proceedings ofthe 31st International Conference on Neural Infor-mation Processing Systems, NIPS’17, p. 6000–6010,Red
Hook, NY, USA, 2017.
Curran Associates Inc.[4] David W. Scott.
Multivariate Density Estimation: The-ory, Practice, and Visualization.
John Wiley & Sons,New York, Chicester, 1992.[5]
SciPy Developers.
Gaussian KDE in SciPy, 2023.



A Description examples

Loss level DescriptionLow犬用品・猫用品・小動物用品・鳥用品・観賞魚用品など豊富な品揃え。注目アイテムもチェック。接着・補修用品の豊富な品揃え：接着剤、粘着/養生テープ、潤滑油、補修材など。エレクトロニクス部品の豊富な品揃え：制御、半導体部品から、センサ、コネクタまで。スニーカー、サンダル、パンプスなど豊富な品揃え。靴のサイズ・幅・ヒールの高さから検索できる電設用品の豊富な品揃え：スイッチ、コンセントプラグ、結束バンド、電設用資材など。エレクトロニクス部品の豊富な品揃え：制御、半導体部品から、センサ、コネクタまで安全・保護用品の豊富な品揃え：ヘルメット、防じんマスク、保護メガネ、軍手など。メモリ 16GB、SSD 最大 2TB まで増加しても業界最安値の価格設定！7 月 29 日金曜日 9 時から 7 月 31 日日曜日 23 時 59 分まで。大人気アイテム続々入荷中。【低価格で高品質】デザイン、形、サイズ、素材、色、機能、価格帯など超充実！大型通販専門店サプリから食品・コスメ・日用品まで。品質・原材料にこだわった商品が盛りだくさん！毎月使ったデータ分だけ支払うお得なワンプラン。例えば 3GB までなら 980 円/月(税込 1,078 円)。お米・麺類・レトルト・調味料・スイーツなど豊富な品揃え。レビュー高評価商品をチェック。Medium150A 20k フランジの通販。配管部材やポンプ・ホースなど豊富に品揃え！ 入会金・年会費無料。利用数 64 万人以上の経費精算システム。領収書のスキャンも簡単。電子帳簿保存法にも対応一人ひとりのペースに合わせた指導だから早く成績が伸びる！ 小学生向け学習塾。未公開の不動産情報も多数／無料会員登録で限定物件をご紹介！ お客様の声掲載中。非公開講師も多数ご紹介。気になる講師の料金やスケジュール等ご相談ください！ 気になる講演料は収入印紙や送付用封筒、謄本もまとめて注文できる。福島のホテル＆航空券。人気の宿泊施設や温泉宿を多数ご紹介。お得なパッケージツアー。自社ローンだから審査が通り易い ― 他店で審査に通らなかった方もご相談ください！商品開発をはじめ、様々な取り組みも行っています。豊富なラインナップ・ CM 情報・天然 100 ％のシルクで子供でも使えるほどの安心成分。特別会員価格。野蚕のシルクパウダー。Highラクラク自動集荷サービス。返送時は配送業者が返送用の宅配伝票を持参し引き取りにに伺います。手の震え（振戦）やすくみ足は症状であり原因ではない。原因対策をお手伝いします。IT ・ビジネス・産業の専門メディアに訪れる会員から、業種、役職クラス、企業規模などの属性指定最大 32 名まで OK の掘りごたつ座敷。8 名× 4 卓あって、間仕切りで仕切ると個…大自然のキャンパスで心身共に安定させ、勉強に部活に仲間と励み、最後は大学合格を目指します。定額制で Web フォームが作り放題！いまこそあらゆる申し込み・問い合わせをクラウド化。キレイめにまとまる Dress シリーズ。エレガントなヒールタイプの Womens シリーズなど種類豊富。イラストアニメが分かりやすい！酵素サプリがきく理由徹底解説と人気商品との比較。ページの挿入、削除、抽出、クロップ、順番変更、回転が簡単に実現できます。水道管を使ったバー・ドアハンドルや、再利用素材を使った引出取手などのオリジナル金物が多数。お土産は通販が一番！ WEB 注文でサッと手ぶら旅行へ。定番の和菓子はモチロン、梨ゼリーなど多数シダー、フランキンセンス、ベチバーなどの特有のノートが楽しめるフレグランス。ワタリガニ、カボス、豊後牛どれをとっても逸品揃いの大人気自治体。Table A2 Example Descriptions with indicated loss level. These examples include descriptions from both the lowest and highestdecile of the loss spectrum.