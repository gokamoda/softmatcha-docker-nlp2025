Bidirectional Transformer Rerankerfor Grammatical Error Correction

Ying Zhang

1

â€ƒ Hidetaka Kamigaito

2

â€ƒ Manabu Okumura

1,31

RIKEN Center for Advanced Intelligence Project



â€ƒ

2

NARA Institute of Science and Technology (NAIST)

3

Institute of Science Tokyo



ying.zhang@riken.jp â€ƒkamigaito.h@is.naist.jp â€ƒoku@lr.pi.titech.ac.jp



æ²è¼‰å·ã®æƒ…å ±

31 å·» 1 å· pp. 3-46.doi: https://doi.org/10.5715/jnlp.31.3

Abstract

Pre-trained sequence-to-sequence (seq2seq) modelshave achieved state-of-the-art results in the grammaticalerror correction tasks.
However, these models are plaguedby prediction bias owing to their unidirectional decoding(Figure 1a).
Thus, this study proposed a bidirectionaltransformer reranker (BTR) that re-estimates the proba-bility of each candidate sentence generated by the pre-trained seq2seq model (Figure 2).
The BTR preserves theseq2seq-style transformer architecture but utilizes a BERT-style self-attention mechanism in the decoder to computethe probability of each target token using masked languagemodeling to capture bidirectional representations from thetarget context (Figure 1b).
To guide the reranking process,the BTR adopted negative sampling in the objective func-tion to minimize the unlikelihood.
During inference, theBTR yielded the ï¬nal results after comparing the rerankedtop-1 results with the original ones using an acceptancethreshold ğœ†.
Experimental results showed that, whenreranking candidates from a pre-trained seq2seq model,the T5-base, the BTR on top of T5-base yielded scoresof 65.47 and 71.27 ğ¹0.5on the CoNLL-14 and BEA testsets, respectively, and yielded 59.52 GLEU score on theJFLEG corpus, with improvements of 0.36, 0.76, and 0.48points compared with the original T5-base.
Furthermore,when reranking candidates from T5-large, the BTR on topof T5-base improved the original T5-large by 0.26 on theBEA test set.ğ‘¥!ğ‘¥"ğ‘¥#ğ‘¥!ğ‘¥"ğ‘¥#(a) Unidirectionalğ‘¥!ğ‘¥"ğ‘¥#ğ‘¥!ğ‘¥"ğ‘¥#(b)
BidirectionalFigure 1: Decoding patterns for generating a sequenceğ’™ = (ğ‘¥1, ğ‘¥2, ğ‘¥3).Input #Base ModelBidirectional Transfo rmer!!"#$select !
%&'Top-"Corrected Sentences!%&'$(!%&'|#)âˆ’ $ !!
"#$# > *FalseTrueselect !!
"
#$InputBase ModelBidirectional Transfo rmerTop-1 Sentence !()*
+Accept !
%&'Top-"Candidate SentencesReranked Top-1Sentence !
%&'$(!%&'|#)âˆ’ $ !!
"#$# > *FalseTrueReject !
%&'and Accept !!
"#$ReranksGeneratesFigure 2: Overview of the reranking procedure by usingthe bidirectional transformer reranker (BTR).CandidateAccept Reject EqualProportion(%) 12.50 21.11 66.39ğ’šğ‘ğ‘ğ‘ ğ‘’61.67 61.66â€  68.78ğ’šğµğ‘‡ ğ‘…63.97â€  57.28 68.78Table 1: Results on the CoNLL-14 dataset.
The ï¬rst blockshows the proportions (%) of ï¬nal decisions regarding thesuggestions from ğ’šğµğ‘‡ ğ‘…: Accept, Reject, or Equal (indi-cating the suggestion ğ’šğµğ‘‡
ğ‘…matches the original selectionğ’šğ‘ğ‘ğ‘ ğ‘’).
The second block reports the ğ¹0.5scores for thesentences selected by the base model (ğ’šğ‘ğ‘ğ‘ ğ‘’) and the BTR(ğ’šğµğ‘‡ ğ‘…) for each decision type.