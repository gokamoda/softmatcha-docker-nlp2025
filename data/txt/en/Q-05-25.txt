Bidirectional Transformer Rerankerfor Grammatical Error Correction

Ying Zhang

1

  Hidetaka Kamigaito

2

  Manabu Okumura

1,31

RIKEN Center for Advanced Intelligence Project



 

2

NARA Institute of Science and Technology (NAIST)

3

Institute of Science Tokyo



ying.zhang@riken.jp  kamigaito.h@is.naist.jp  oku@lr.pi.titech.ac.jp



掲載号の情報

31 巻 1 号 pp. 3-46.doi: https://doi.org/10.5715/jnlp.31.3

Abstract

Pre-trained sequence-to-sequence (seq2seq) modelshave achieved state-of-the-art results in the grammaticalerror correction tasks.
However, these models are plaguedby prediction bias owing to their unidirectional decoding(Figure 1a).
Thus, this study proposed a bidirectionaltransformer reranker (BTR) that re-estimates the proba-bility of each candidate sentence generated by the pre-trained seq2seq model (Figure 2).
The BTR preserves theseq2seq-style transformer architecture but utilizes a BERT-style self-attention mechanism in the decoder to computethe probability of each target token using masked languagemodeling to capture bidirectional representations from thetarget context (Figure 1b).
To guide the reranking process,the BTR adopted negative sampling in the objective func-tion to minimize the unlikelihood.
During inference, theBTR yielded the ﬁnal results after comparing the rerankedtop-1 results with the original ones using an acceptancethreshold 𝜆.
Experimental results showed that, whenreranking candidates from a pre-trained seq2seq model,the T5-base, the BTR on top of T5-base yielded scoresof 65.47 and 71.27 𝐹0.5on the CoNLL-14 and BEA testsets, respectively, and yielded 59.52 GLEU score on theJFLEG corpus, with improvements of 0.36, 0.76, and 0.48points compared with the original T5-base.
Furthermore,when reranking candidates from T5-large, the BTR on topof T5-base improved the original T5-large by 0.26 on theBEA test set.𝑥!𝑥"𝑥#𝑥!𝑥"𝑥#(a) Unidirectional𝑥!𝑥"𝑥#𝑥!𝑥"𝑥#(b)
BidirectionalFigure 1: Decoding patterns for generating a sequence𝒙 = (𝑥1, 𝑥2, 𝑥3).Input #Base ModelBidirectional Transfo rmer!!"#$select !
%&'Top-"Corrected Sentences!%&'$(!%&'|#)− $ !!
"#$# > *FalseTrueselect !!
"
#$InputBase ModelBidirectional Transfo rmerTop-1 Sentence !()*
+Accept !
%&'Top-"Candidate SentencesReranked Top-1Sentence !
%&'$(!%&'|#)− $ !!
"#$# > *FalseTrueReject !
%&'and Accept !!
"#$ReranksGeneratesFigure 2: Overview of the reranking procedure by usingthe bidirectional transformer reranker (BTR).CandidateAccept Reject EqualProportion(%) 12.50 21.11 66.39𝒚𝑏𝑎𝑠𝑒61.67 61.66† 68.78𝒚𝐵𝑇 𝑅63.97† 57.28 68.78Table 1: Results on the CoNLL-14 dataset.
The ﬁrst blockshows the proportions (%) of ﬁnal decisions regarding thesuggestions from 𝒚𝐵𝑇 𝑅: Accept, Reject, or Equal (indi-cating the suggestion 𝒚𝐵𝑇
𝑅matches the original selection𝒚𝑏𝑎𝑠𝑒).
The second block reports the 𝐹0.5scores for thesentences selected by the base model (𝒚𝑏𝑎𝑠𝑒) and the BTR(𝒚𝐵𝑇 𝑅) for each decision type.