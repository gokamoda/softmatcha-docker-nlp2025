Skip-bigrams reconstruct trigrams in 2-word languages

Shohei Hidaka

11

Japan Advanced Institute of Science and Technology



shhidaka@jaist.ac.jp



Abstract

In natural language processing, it has been empiricallyknown that skip-grams, co-occurrence statistics of twowords with some number of words in between them, isan eÔ¨Äective source of data to learn semantic nature of thewords.
In this study, we propose a new theoretical accountfor why a set of skip-grams is eÔ¨Äective at least for two-wordlanguages, by giving a theorem that a set of trigram prob-abilities is representable with a set of skip bigrams.
Thisrepresentation theorem justiÔ¨Åes the use of skip bigrams orso-called shiftgrams as a computationally eÔ¨Écient sourceto access higher order n-gram.


1 EÔ¨Äectiveness of skip-gram statis-



tics

In natural language processing, it has been empiricallyknown that semantic structure of words are represented bythe word vector by learning the skip-grams [1, 2], which isco-occurrence statistics ( ùëãùë°, ùëãùë°+ùë†) of a pair of word at ùë° andword ùë°+ùë† with a skip length ùë† = 1, 2, . .
..
There are previousstudies that have tried to explain this empirical Ô¨Ånding[3, 4, 5].
Most of such previous studies have hypothesizedthat the word vector gives a eÔ¨Äective representation due totheir special settings of the learning scheme of the wordvector models (i.e., negative sampling)[3].In this study, we take an apprroach distinct from theseprevious studies, and mathematically analyze the funda-mental nature of language systems, represented by n-gram statistics.
Our primary focus is how trigram statis-tics, the conditional probability ùëÉ(ùëãùë°+2|ùëãùë°, ùëãùë°+1), can berepresented by a set of ùë†-skip bigrams, ùëÉ(ùëãùë°+ùë†|ùëãùë°)
forùë† = 1, 2, . .
..
There is a trivial relationship that ùë†-skip bi-grams ùëÉ(ùëãùë°+ùë†|ùëãùë°) for each ùë† = 1, 2, . . .
is constructed by agiven trigram ùëÉ(ùëãùë°+2|ùëãùë°, ùëãùë°+1) of a Markov process.
Ourquestion is the converse ‚Äì can we construct the trigram onlyfrom a set of ùë†-skip bigrams?
This is a focus special case,that may be generalized to the relationship between ùë†-skipbigrams and a general ùëõ-grams.
If such fundamental rela-tionship between (n-1)-g rams and n-grams is established,it would explain why skip-gram statistics is a good sourceof data to learn semantic nature of words or language ingeneral ‚Äì skip-gram gives a suÔ¨Écient statistics of n-gramsand is computable eÔ¨Éciently.

2 Skip bigram

In this study, we assume a language ùêø has a setof ùëò words ùïéùëò:= {0, 1, 2, . .
.
, ùëò ‚àí 1}, and we call aMarkov process over a series of ùëã0, ùëã1, . . .
‚àà ùïéùëòlan-guage system.
In particular, a language system iscalled ùëõ-grams of ùêø, if ùëÉ(ùëãùë°|ùëãùë° ‚àí1, ùëãùë° ‚àí2, . .
.
, ùëãùë° ‚àíùëõ‚àíùë†) =ùëÉ(ùëãùë°|ùëãùë° ‚àí1, ùëãùë° ‚àí2, . . .
, ùëãùë° ‚àíùëõ) for any ùë° and ùë† = 0, 1, 2, . .
..
So any ùëõ-gram language system with ùëò words has(ùëò ‚àí 1)ùëòùëõ‚àí1parameters, those are the conditionalprobabilities ùëÉ(ùëãùë°|ùëãùë° ‚àíùëõ+1, . . .
, ùëãùë° ‚àí2, ùëãùë° ‚àí1) ‚â• 0
with√çùëãùë°‚ààùïéùëòùëÉ(ùëãùë°|ùëãùë° ‚àíùëõ+1, . . .
, ùëãùë° ‚àí2, ùëãùë° ‚àí1) = 1.
In this study,we assume any language system under analysis is ergodic,or equivalently it has a unique set of stationary probabili-ties.
To encode the joint random variables of ùëö-series, with-out loss of generality, we Ô¨Åx the encoder map ‚Ñéùëò,ùëö:ùïéùëöùëò‚Üí ùê∂ùëò,ùëö:= {1, 2, . . .
, ùëòùëö} by‚Ñéùëò,ùëö(ùëãùë° ‚àíùëö+1, ùëãùë° ‚àíùëö+2, . . .
, ùëãùë°) := 1 +ùëö√ïùëó=1ÓòÄùëãùë° ‚àíùëö+ ùëó‚àí 1ÓòÅùëòùëó‚àí1.(1)In this encoding of the joint random variables, the transitionmatrix ùëÑ2‚àà ‚Ñùùëò√óùëòof any bigram language system is of theformùëÑ2:=¬©Óö≠Óö≠Óö≠Óö≠Óö≠Óö≠¬´ùëû0|0ùëû0|1. . .
ùëû0| ùëò‚àí1ùëû1|0ùëû1|1. . .
ùëû1| ùëò‚àí1............
ùëûùëò‚àí1|0ùëûùëò‚àí1|1. . .
ùëûùëò‚àí1| ùëò‚àí1¬™¬Æ¬Æ¬Æ¬Æ¬Æ¬Æ¬¨,(2)where
ùëûùëñ| ùëó:= ùëÉ(ùëãùë°= ùëñ|ùëãùë° ‚àí1= ùëó) and√çùëñ‚ààùïéùëòùëûùëñ| ùëó=
1 forany ùëó ‚àà ùïéùëò.
Moreover, the transition matrix ùëÑ3‚àà ‚Ñùùëò2√óùëò2

Skip-bigrams reconstruct trigrams in 2-word languages

Shohei Hidaka

11

Japan Advanced Institute of Science and Technology



shhidaka@jaist.ac.jp



Abstract

In natural language processing, it has been empiricallyknown that skip-grams, co-occurrence statistics of twowords with some number of words in between them, isan eÔ¨Äective source of data to learn semantic nature of thewords.
In this study, we propose a new theoretical accountfor why a set of skip-grams is eÔ¨Äective at least for two-wordlanguages, by giving a theorem that a set of trigram prob-abilities is representable with a set of skip bigrams.
Thisrepresentation theorem justiÔ¨Åes the use of skip bigrams orso-called shiftgrams as a computationally eÔ¨Écient sourceto access higher order n-gram.


1 EÔ¨Äectiveness of skip-gram statis-



tics

In natural language processing, it has been empiricallyknown that semantic structure of words are represented bythe word vector by learning the skip-grams [1, 2], which isco-occurrence statistics ( ùëãùë°, ùëãùë°+ùë†) of a pair of word at ùë° andword ùë°+ùë† with a skip length ùë† = 1, 2, . .
..
There are previousstudies that have tried to explain this empirical Ô¨Ånding[3, 4, 5].
Most of such previous studies have hypothesizedthat the word vector gives a eÔ¨Äective representation due totheir special settings of the learning scheme of the wordvector models (i.e., negative sampling)[3].In this study, we take an apprroach distinct from theseprevious studies, and mathematically analyze the funda-mental nature of language systems, represented by n-gram statistics.
Our primary focus is how trigram statis-tics, the conditional probability ùëÉ(ùëãùë°+2|ùëãùë°, ùëãùë°+1), can berepresented by a set of ùë†-skip bigrams, ùëÉ(ùëãùë°+ùë†|ùëãùë°)
forùë† = 1, 2, . .
..
There is a trivial relationship that ùë†-skip bi-grams ùëÉ(ùëãùë°+ùë†|ùëãùë°) for each ùë† = 1, 2, . . .
is constructed by agiven trigram ùëÉ(ùëãùë°+2|ùëãùë°, ùëãùë°+1) of a Markov process.
Ourquestion is the converse ‚Äì can we construct the trigram onlyfrom a set of ùë†-skip bigrams?
This is a focus special case,that may be generalized to the relationship between ùë†-skipbigrams and a general ùëõ-grams.
If such fundamental rela-tionship between (n-1)-g rams and n-grams is established,it would explain why skip-gram statistics is a good sourceof data to learn semantic nature of words or language ingeneral ‚Äì skip-gram gives a suÔ¨Écient statistics of n-gramsand is computable eÔ¨Éciently.

2 Skip bigram

In this study, we assume a language ùêø has a setof ùëò words ùïéùëò:= {0, 1, 2, . .
.
, ùëò ‚àí 1}, and we call aMarkov process over a series of ùëã0, ùëã1, . . .
‚àà ùïéùëòlan-guage system.
In particular, a language system iscalled ùëõ-grams of ùêø, if ùëÉ(ùëãùë°|ùëãùë° ‚àí1, ùëãùë° ‚àí2, . .
.
, ùëãùë° ‚àíùëõ‚àíùë†) =ùëÉ(ùëãùë°|ùëãùë° ‚àí1, ùëãùë° ‚àí2, . . .
, ùëãùë° ‚àíùëõ) for any ùë° and ùë† = 0, 1, 2, . .
..
So any ùëõ-gram language system with ùëò words has(ùëò ‚àí 1)ùëòùëõ‚àí1parameters, those are the conditionalprobabilities ùëÉ(ùëãùë°|ùëãùë° ‚àíùëõ+1, . . .
, ùëãùë° ‚àí2, ùëãùë° ‚àí1) ‚â• 0
with√çùëãùë°‚ààùïéùëòùëÉ(ùëãùë°|ùëãùë° ‚àíùëõ+1, . . .
, ùëãùë° ‚àí2, ùëãùë° ‚àí1) = 1.
In this study,we assume any language system under analysis is ergodic,or equivalently it has a unique set of stationary probabili-ties.
To encode the joint random variables of ùëö-series, with-out loss of generality, we Ô¨Åx the encoder map ‚Ñéùëò,ùëö:ùïéùëöùëò‚Üí ùê∂ùëò,ùëö:= {1, 2, . . .
, ùëòùëö} by‚Ñéùëò,ùëö(ùëãùë° ‚àíùëö+1, ùëãùë° ‚àíùëö+2, . . .
, ùëãùë°) := 1 +ùëö√ïùëó=1ÓòÄùëãùë° ‚àíùëö+ ùëó‚àí 1ÓòÅùëòùëó‚àí1.(1)In this encoding of the joint random variables, the transitionmatrix ùëÑ2‚àà ‚Ñùùëò√óùëòof any bigram language system is of theformùëÑ2:=¬©Óö≠Óö≠Óö≠Óö≠Óö≠Óö≠¬´ùëû0|0ùëû0|1. . .
ùëû0| ùëò‚àí1ùëû1|0ùëû1|1. . .
ùëû1| ùëò‚àí1............
ùëûùëò‚àí1|0ùëûùëò‚àí1|1. . .
ùëûùëò‚àí1| ùëò‚àí1¬™¬Æ¬Æ¬Æ¬Æ¬Æ¬Æ¬¨,(2)where
ùëûùëñ| ùëó:= ùëÉ(ùëãùë°= ùëñ|ùëãùë° ‚àí1= ùëó) and√çùëñ‚ààùïéùëòùëûùëñ| ùëó=
1 forany ùëó ‚àà ùïéùëò.
Moreover, the transition matrix ùëÑ3‚àà ‚Ñùùëò2√óùëò2of any trigram language system is of the formùëÑ3:=√ïùëñ, ùëó ‚ààùïéùëòùëíùëò,ùëñ‚äó ùëíùëò, ùëóùëí‚ä§ùëò, ùëó‚äó ùëüùëñ, ùëó, (3)where ùëüùëñ, ùëó= (ùëûùëñ| (0, ùëó ), . . .
, ùëûùëñ| (ùëò‚àí1, ùëó )).Let
ùúÉ2= (ùúÉ0, ùúÉ1, . . .
, ùúÉùëò‚àí1)‚ä§‚àà ‚Ñùùëòbe the stationaryprobability vector of the bigram system such that ùúÉ2=ùëÑ2ùúÉ2, and ùúÉ3= (ùúÉ(0,0), ùúÉ(1,0), . . . , ùúÉ(ùëò‚àí1,ùëò‚àí1))‚ä§‚àà ‚Ñùùëò2bethe stationary probability vector of the trigram system suchthat ùúÉ3= ùëÑ3ùúÉ3.


2.1 Tensor form and tensor product


The set of ùëõ-gram conditional probabilitiesùëÉ(ùëãùë°|ùëãùë° ‚àí1, . .
.
, ùëãùë° ‚àíùëõ+1)is naturally represented bya ùëõthorder tensor (ùëõ-tensor in short).
Real-valued ùëõ-tensor‚Ñùùëò1√óùëò2√ó...√óùëòùëõis a vector space of real-valued maps{0, . . .
, ùëò1‚àí 1} √ó . . .
√ó {0, . .
. , ùëòùëõ‚àí 1} ‚Üí ‚Ñù. Let usdenote ‚Ñùùëòùëõ:= ‚Ñùùëò1√óùëò2√ó...
√óùëòùëõfor ùëò = ùëò1= ùëò2= . . .
= ùëòùëõ.We call a tensor product ‚òÖ : ‚Ñùùëòùëõ√ó ‚Ñùùëòùëõ‚Üí ‚Ñùùëòùëõconvo-lution deÔ¨Åned byùëÉ ‚òÖ
ùëÑ := (4)√ïùëãùë° ‚àí1‚ààùêæùëÉ(ùëãùë°, ùëãùë° ‚àí1, . .
.
, ùëãùë° ‚àíùëõ+1)ùëÑ(ùëãùë° ‚àí1, ùëãùë° ‚àí2, . . .
, ùëãùë° ‚àíùëõ),for ùëÉ, ùëÑ ‚àà ‚Ñùùëòùëõ.
In particular, denote for ùëö ‚â• 0ùëÑùëö:=Ô£±Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£≥ùê∏ùëõ, ùëòif ùëö = 0,ùëÑùëö‚àí1‚òÖ ùëÑ otherwise, (5)where ùê∏ùëõ, ùëò‚àà ‚Ñùùëòùëõis the left unit tensor satisfying ùê∏ùëõ, ùëò‚òÖùëÑ = ùëÑ for any tensor ùëÑ ‚àà ‚Ñùùëòùëõ. SpeciÔ¨Åcally,ùê∏ùëõ, ùëò(ùëñ1, ùëñ2, . .
.
, ùëñùëõ) =Ô£±Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£≥1 if ùëñ1= ùëñ20 otherwise.
(6)This convolution is useful to represent a time-shift oper-ation in the following sense: If ùëÑ ‚àà ‚Ñùùëòùëõis time-invariantùëõ-gram conditional probability ùëÑ(ùëãùë°, ùëãùë° ‚àí1, . . .
, ùëãùë° ‚àíùëõ+1) =ùëÉ(ùëãùë°|ùëãùë° ‚àí1, . . .
, ùëãùë° ‚àíùëõ+1), the convolution of ùëòthpower rep-resents shift in the time step of random variables:ùëÑùëò(ùëãùë°, ùëãùë° ‚àíùëò, . . .
, ùëãùë° ‚àíùëõ‚àíùëò+2)(7)= ùëÉ(ùëãùë°|ùëãùë° ‚àíùëò, . . .
, ùëãùë° ‚àíùëõ‚àíùëò+2).
(8)DeÔ¨Åne reduction operator ùëüùúÉ,ùêº: ‚Ñùùëòùëõ‚Üí
‚Ñùùëò|ùêº |for anyùêº ‚äÜ ùëÅ = { 1, 2, . .
. , ùëõ} for ùëÑ ‚àà ‚Ñùùëòùëõbyùëüùêº(ùëÑ) =√ï(ùëñùëó)‚àà{1,...,ùëõ}\ùêº‚ààùêæùëõ‚àí|ùêº|ùëÑ(ùëñ1, ùëñ2, . . .
, ùëñùëõ), (9)ùëö-shifgram ùëÜùëö: ‚Ñùùëòùëõ‚Üí ‚Ñùùëò2is deÔ¨Åned byùëÜùëö(ùëÑ) :
= ùëü{1,2}ÓòÄùëÑùëöŒòùëÑÓòÅ, (10)where ŒòùëÑis the stationar y tensor associated to ùëÑ.The ùëö-shiftgram of ùëõ-gram tensor ùëÑ ‚àà ‚Ñùùëòùëõhas thefollowing properties.
For ùëö‚Ä≤= 2, 3, . . .
, ùëõ,ùëÜùëö(ùëÑ) = ùëü{1,ùëö‚Ä≤}ÓòêùëÑùëö‚àíùëö‚Ä≤+2ŒòùëÑÓòë. (11)

3 Inverse problem



3.1 eÔ¨Äective isomorphism between tri-



grams and shiftgrams

Suppose we have the series of all ùëö-shiftgrams ùëÜ(ùëÑ) :
={ùëÜùëö(ùëÑ)}ùëö=0,1,...of some unknown ùëõ-gram probabilitytensor ùëÑ ‚àà ‚Ñùùëòùëõ.
Then can we uniquely identify theoriginal probability tensor ùëÑ that generates ùëÜ(ùëÑ)?
Letus focus on ùëõ = 3 in this paper.
For each Ô¨Åxedùëö‚Ä≤= 0, 1, . .
., we have ùëõ ‚àí 1 diÔ¨Äerent ùëö-shiftgramsùëÜùëö(ùëÑ)
= ùëü{1, (ùëö+2‚àíùëö‚Ä≤) }ÓòÄùëÑùëö‚Ä≤ŒòùëÑÓòÅfor ùëö‚Ä≤‚â§ ùëö ‚â§ ùëö‚Ä≤+ ùëõ ‚àí 2due to the identity (11).
For each ùëö‚Ä≤, ùëÑ is constrainedby (ùëõ ‚àí 1) matrices of ùëö-shiftgrams ùëÜùëö(ùëÑ) and the sum√çùëñ‚ààùêæùëÑùëö‚Ä≤(ùëñ, ùëó, ùëò) = 1 is also constrained.
So ùëÑùëö‚Ä≤mayhave at most (ùëò ‚àí 1)ùëõpolynomial equations, but only(ùëò ‚àí 1)ùëõ‚àí1equations are new constraints not expressedby (11) for ùëö‚Ä≤‚â• ùëõ ‚àí 1.
Thus, there are at mostùëò2+ ùëò (ùëò ‚àí1)
+ ùëö‚Ä≤(ùëò ‚àí1)2polynomial equations for a ser iesof ùëÜ0(ùëÑ), ùëÜ1(ùëÑ), . . . , ùëÜùëö‚Ä≤(ùëÑ), and thus at least ùëö‚Ä≤‚â• ùëò tohave the suÔ¨Écient number ùëò3of polynomial equations toidentify ùëÑ ‚àà ‚Ñùùëòùëõ.

3.2 Case with ùëò = 2 and ùëõ = 3

To be speciÔ¨Åc, let us study ùëò = 2 and ùëõ = 3as a minimal example.
In this case ùëÜ1(ùëÑ), ùëÜ2(ùëÑ) isneeded to have a suÔ¨Écient number of equations.
LetÓòêùëÑ1ùëÑ2. . .
ùëÑùëöÓòëdenote the third order tensor byseries of matrices ùëÑ (ùëñ, ùëó, ùëò) = ùëÑùëò(ùëñ, ùëó) for ùëñ, ùëó, ùëò ‚àà ùêæ. Forùëò = 2 and ùëõ = 3, the trigram probability tensor isùëÑ =Óòêùëû00ùëû10ùëû01ùëû11Óòë(12)= ùëû0|00ùëû0|10ùëû0|01ùëû0|11ùëû1|00ùëû1|10ùëû1|01ùëû1|11!, (13)where ùëûùëñ ùëó= (ùëû0|ùëñ ùëó, ùëû1|ùëñ ùëó)‚ä§‚àà ‚Ñù2, andùëÑ2=Óòê(ùëû00, ùëû10)ùëû00(ùëû01, ùëû11)ùëû10(ùëû00, ùëû10)ùëû01(ùëû01, ùëû11)ùëû11Óòë.(14)

Skip-bigrams reconstruct trigrams in 2-word languages

Shohei Hidaka

11

Japan Advanced Institute of Science and Technology



shhidaka@jaist.ac.jp



Abstract

In natural language processing, it has been empiricallyknown that skip-grams, co-occurrence statistics of twowords with some number of words in between them, isan eÔ¨Äective source of data to learn semantic nature of thewords.
In this study, we propose a new theoretical accountfor why a set of skip-grams is eÔ¨Äective at least for two-wordlanguages, by giving a theorem that a set of trigram prob-abilities is representable with a set of skip bigrams.
Thisrepresentation theorem justiÔ¨Åes the use of skip bigrams orso-called shiftgrams as a computationally eÔ¨Écient sourceto access higher order n-gram.


1 EÔ¨Äectiveness of skip-gram statis-



tics

In natural language processing, it has been empiricallyknown that semantic structure of words are represented bythe word vector by learning the skip-grams [1, 2], which isco-occurrence statistics ( ùëãùë°, ùëãùë°+ùë†) of a pair of word at ùë° andword ùë°+ùë† with a skip length ùë† = 1, 2, . .
..
There are previousstudies that have tried to explain this empirical Ô¨Ånding[3, 4, 5].
Most of such previous studies have hypothesizedthat the word vector gives a eÔ¨Äective representation due totheir special settings of the learning scheme of the wordvector models (i.e., negative sampling)[3].In this study, we take an apprroach distinct from theseprevious studies, and mathematically analyze the funda-mental nature of language systems, represented by n-gram statistics.
Our primary focus is how trigram statis-tics, the conditional probability ùëÉ(ùëãùë°+2|ùëãùë°, ùëãùë°+1), can berepresented by a set of ùë†-skip bigrams, ùëÉ(ùëãùë°+ùë†|ùëãùë°)
forùë† = 1, 2, . .
..
There is a trivial relationship that ùë†-skip bi-grams ùëÉ(ùëãùë°+ùë†|ùëãùë°) for each ùë† = 1, 2, . . .
is constructed by agiven trigram ùëÉ(ùëãùë°+2|ùëãùë°, ùëãùë°+1) of a Markov process.
Ourquestion is the converse ‚Äì can we construct the trigram onlyfrom a set of ùë†-skip bigrams?
This is a focus special case,that may be generalized to the relationship between ùë†-skipbigrams and a general ùëõ-grams.
If such fundamental rela-tionship between (n-1)-g rams and n-grams is established,it would explain why skip-gram statistics is a good sourceof data to learn semantic nature of words or language ingeneral ‚Äì skip-gram gives a suÔ¨Écient statistics of n-gramsand is computable eÔ¨Éciently.

2 Skip bigram

In this study, we assume a language ùêø has a setof ùëò words ùïéùëò:= {0, 1, 2, . .
.
, ùëò ‚àí 1}, and we call aMarkov process over a series of ùëã0, ùëã1, . . .
‚àà ùïéùëòlan-guage system.
In particular, a language system iscalled ùëõ-grams of ùêø, if ùëÉ(ùëãùë°|ùëãùë° ‚àí1, ùëãùë° ‚àí2, . .
.
, ùëãùë° ‚àíùëõ‚àíùë†) =ùëÉ(ùëãùë°|ùëãùë° ‚àí1, ùëãùë° ‚àí2, . . .
, ùëãùë° ‚àíùëõ) for any ùë° and ùë† = 0, 1, 2, . .
..
So any ùëõ-gram language system with ùëò words has(ùëò ‚àí 1)ùëòùëõ‚àí1parameters, those are the conditionalprobabilities ùëÉ(ùëãùë°|ùëãùë° ‚àíùëõ+1, . . .
, ùëãùë° ‚àí2, ùëãùë° ‚àí1) ‚â• 0
with√çùëãùë°‚ààùïéùëòùëÉ(ùëãùë°|ùëãùë° ‚àíùëõ+1, . . .
, ùëãùë° ‚àí2, ùëãùë° ‚àí1) = 1.
In this study,we assume any language system under analysis is ergodic,or equivalently it has a unique set of stationary probabili-ties.
To encode the joint random variables of ùëö-series, with-out loss of generality, we Ô¨Åx the encoder map ‚Ñéùëò,ùëö:ùïéùëöùëò‚Üí ùê∂ùëò,ùëö:= {1, 2, . . .
, ùëòùëö} by‚Ñéùëò,ùëö(ùëãùë° ‚àíùëö+1, ùëãùë° ‚àíùëö+2, . . .
, ùëãùë°) := 1 +ùëö√ïùëó=1ÓòÄùëãùë° ‚àíùëö+ ùëó‚àí 1ÓòÅùëòùëó‚àí1.(1)In this encoding of the joint random variables, the transitionmatrix ùëÑ2‚àà ‚Ñùùëò√óùëòof any bigram language system is of theformùëÑ2:=¬©Óö≠Óö≠Óö≠Óö≠Óö≠Óö≠¬´ùëû0|0ùëû0|1. . .
ùëû0| ùëò‚àí1ùëû1|0ùëû1|1. . .
ùëû1| ùëò‚àí1............
ùëûùëò‚àí1|0ùëûùëò‚àí1|1. . .
ùëûùëò‚àí1| ùëò‚àí1¬™¬Æ¬Æ¬Æ¬Æ¬Æ¬Æ¬¨,(2)where
ùëûùëñ| ùëó:= ùëÉ(ùëãùë°= ùëñ|ùëãùë° ‚àí1= ùëó) and√çùëñ‚ààùïéùëòùëûùëñ| ùëó=
1 forany ùëó ‚àà ùïéùëò.
Moreover, the transition matrix ùëÑ3‚àà ‚Ñùùëò2√óùëò2of any trigram language system is of the formùëÑ3:=√ïùëñ, ùëó ‚ààùïéùëòùëíùëò,ùëñ‚äó ùëíùëò, ùëóùëí‚ä§ùëò, ùëó‚äó ùëüùëñ, ùëó, (3)where ùëüùëñ, ùëó= (ùëûùëñ| (0, ùëó ), . . .
, ùëûùëñ| (ùëò‚àí1, ùëó )).Let
ùúÉ2= (ùúÉ0, ùúÉ1, . . .
, ùúÉùëò‚àí1)‚ä§‚àà ‚Ñùùëòbe the stationaryprobability vector of the bigram system such that ùúÉ2=ùëÑ2ùúÉ2, and ùúÉ3= (ùúÉ(0,0), ùúÉ(1,0), . . . , ùúÉ(ùëò‚àí1,ùëò‚àí1))‚ä§‚àà ‚Ñùùëò2bethe stationary probability vector of the trigram system suchthat ùúÉ3= ùëÑ3ùúÉ3.


2.1 Tensor form and tensor product


The set of ùëõ-gram conditional probabilitiesùëÉ(ùëãùë°|ùëãùë° ‚àí1, . .
.
, ùëãùë° ‚àíùëõ+1)is naturally represented bya ùëõthorder tensor (ùëõ-tensor in short).
Real-valued ùëõ-tensor‚Ñùùëò1√óùëò2√ó...√óùëòùëõis a vector space of real-valued maps{0, . . .
, ùëò1‚àí 1} √ó . . .
√ó {0, . .
. , ùëòùëõ‚àí 1} ‚Üí ‚Ñù. Let usdenote ‚Ñùùëòùëõ:= ‚Ñùùëò1√óùëò2√ó...
√óùëòùëõfor ùëò = ùëò1= ùëò2= . . .
= ùëòùëõ.We call a tensor product ‚òÖ : ‚Ñùùëòùëõ√ó ‚Ñùùëòùëõ‚Üí ‚Ñùùëòùëõconvo-lution deÔ¨Åned byùëÉ ‚òÖ
ùëÑ := (4)√ïùëãùë° ‚àí1‚ààùêæùëÉ(ùëãùë°, ùëãùë° ‚àí1, . .
.
, ùëãùë° ‚àíùëõ+1)ùëÑ(ùëãùë° ‚àí1, ùëãùë° ‚àí2, . . .
, ùëãùë° ‚àíùëõ),for ùëÉ, ùëÑ ‚àà ‚Ñùùëòùëõ.
In particular, denote for ùëö ‚â• 0ùëÑùëö:=Ô£±Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£≥ùê∏ùëõ, ùëòif ùëö = 0,ùëÑùëö‚àí1‚òÖ ùëÑ otherwise, (5)where ùê∏ùëõ, ùëò‚àà ‚Ñùùëòùëõis the left unit tensor satisfying ùê∏ùëõ, ùëò‚òÖùëÑ = ùëÑ for any tensor ùëÑ ‚àà ‚Ñùùëòùëõ. SpeciÔ¨Åcally,ùê∏ùëõ, ùëò(ùëñ1, ùëñ2, . .
.
, ùëñùëõ) =Ô£±Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£≥1 if ùëñ1= ùëñ20 otherwise.
(6)This convolution is useful to represent a time-shift oper-ation in the following sense: If ùëÑ ‚àà ‚Ñùùëòùëõis time-invariantùëõ-gram conditional probability ùëÑ(ùëãùë°, ùëãùë° ‚àí1, . . .
, ùëãùë° ‚àíùëõ+1) =ùëÉ(ùëãùë°|ùëãùë° ‚àí1, . . .
, ùëãùë° ‚àíùëõ+1), the convolution of ùëòthpower rep-resents shift in the time step of random variables:ùëÑùëò(ùëãùë°, ùëãùë° ‚àíùëò, . . .
, ùëãùë° ‚àíùëõ‚àíùëò+2)(7)= ùëÉ(ùëãùë°|ùëãùë° ‚àíùëò, . . .
, ùëãùë° ‚àíùëõ‚àíùëò+2).
(8)DeÔ¨Åne reduction operator ùëüùúÉ,ùêº: ‚Ñùùëòùëõ‚Üí
‚Ñùùëò|ùêº |for anyùêº ‚äÜ ùëÅ = { 1, 2, . .
. , ùëõ} for ùëÑ ‚àà ‚Ñùùëòùëõbyùëüùêº(ùëÑ) =√ï(ùëñùëó)‚àà{1,...,ùëõ}\ùêº‚ààùêæùëõ‚àí|ùêº|ùëÑ(ùëñ1, ùëñ2, . . .
, ùëñùëõ), (9)ùëö-shifgram ùëÜùëö: ‚Ñùùëòùëõ‚Üí ‚Ñùùëò2is deÔ¨Åned byùëÜùëö(ùëÑ) :
= ùëü{1,2}ÓòÄùëÑùëöŒòùëÑÓòÅ, (10)where ŒòùëÑis the stationar y tensor associated to ùëÑ.The ùëö-shiftgram of ùëõ-gram tensor ùëÑ ‚àà ‚Ñùùëòùëõhas thefollowing properties.
For ùëö‚Ä≤= 2, 3, . . .
, ùëõ,ùëÜùëö(ùëÑ) = ùëü{1,ùëö‚Ä≤}ÓòêùëÑùëö‚àíùëö‚Ä≤+2ŒòùëÑÓòë. (11)

3 Inverse problem



3.1 eÔ¨Äective isomorphism between tri-



grams and shiftgrams

Suppose we have the series of all ùëö-shiftgrams ùëÜ(ùëÑ) :
={ùëÜùëö(ùëÑ)}ùëö=0,1,...of some unknown ùëõ-gram probabilitytensor ùëÑ ‚àà ‚Ñùùëòùëõ.
Then can we uniquely identify theoriginal probability tensor ùëÑ that generates ùëÜ(ùëÑ)?
Letus focus on ùëõ = 3 in this paper.
For each Ô¨Åxedùëö‚Ä≤= 0, 1, . .
., we have ùëõ ‚àí 1 diÔ¨Äerent ùëö-shiftgramsùëÜùëö(ùëÑ)
= ùëü{1, (ùëö+2‚àíùëö‚Ä≤) }ÓòÄùëÑùëö‚Ä≤ŒòùëÑÓòÅfor ùëö‚Ä≤‚â§ ùëö ‚â§ ùëö‚Ä≤+ ùëõ ‚àí 2due to the identity (11).
For each ùëö‚Ä≤, ùëÑ is constrainedby (ùëõ ‚àí 1) matrices of ùëö-shiftgrams ùëÜùëö(ùëÑ) and the sum√çùëñ‚ààùêæùëÑùëö‚Ä≤(ùëñ, ùëó, ùëò) = 1 is also constrained.
So ùëÑùëö‚Ä≤mayhave at most (ùëò ‚àí 1)ùëõpolynomial equations, but only(ùëò ‚àí 1)ùëõ‚àí1equations are new constraints not expressedby (11) for ùëö‚Ä≤‚â• ùëõ ‚àí 1.
Thus, there are at mostùëò2+ ùëò (ùëò ‚àí1)
+ ùëö‚Ä≤(ùëò ‚àí1)2polynomial equations for a ser iesof ùëÜ0(ùëÑ), ùëÜ1(ùëÑ), . . . , ùëÜùëö‚Ä≤(ùëÑ), and thus at least ùëö‚Ä≤‚â• ùëò tohave the suÔ¨Écient number ùëò3of polynomial equations toidentify ùëÑ ‚àà ‚Ñùùëòùëõ.

3.2 Case with ùëò = 2 and ùëõ = 3

To be speciÔ¨Åc, let us study ùëò = 2 and ùëõ = 3as a minimal example.
In this case ùëÜ1(ùëÑ), ùëÜ2(ùëÑ) isneeded to have a suÔ¨Écient number of equations.
LetÓòêùëÑ1ùëÑ2. . .
ùëÑùëöÓòëdenote the third order tensor byseries of matrices ùëÑ (ùëñ, ùëó, ùëò) = ùëÑùëò(ùëñ, ùëó) for ùëñ, ùëó, ùëò ‚àà ùêæ. Forùëò = 2 and ùëõ = 3, the trigram probability tensor isùëÑ =Óòêùëû00ùëû10ùëû01ùëû11Óòë(12)= ùëû0|00ùëû0|10ùëû0|01ùëû0|11ùëû1|00ùëû1|10ùëû1|01ùëû1|11!, (13)where ùëûùëñ ùëó= (ùëû0|ùëñ ùëó, ùëû1|ùëñ ùëó)‚ä§‚àà ‚Ñù2, andùëÑ2=Óòê(ùëû00, ùëû10)ùëû00(ùëû01, ùëû11)ùëû10(ùëû00, ùëû10)ùëû01(ùëû01, ùëû11)ùëû11Óòë.(14)Thus, with the stationary tensor ŒòùëÑ(ùëñ, ùëó, ùëò) := ùúÉùëó ùëò, theùëö-shiftgrams for ùëö = 1, 2, 3 areùëÜ1(ùëÑ)
=√ïùëó ‚àà {0,1}ùëûùëñ ùëóùúÉùëñ ùëóùëí‚ä§ùëõ,ùëñ. (15)ùëÜ2(ùëÑ) =√ïùëñ‚àà {0,1}ùëûùëñ ùëóùúÉùëñ ùëóùëí‚ä§ùëõ, ùëó. (16)ùëÜ3(ùëÑ) =√ïùëñ, ùëó ‚àà {0,1}(ùëû0ùëñ, ùëû1ùëñ)ùëûùëñ ùëóùúÉùëñ ùëóùëí‚ä§ùëõ, ùëó. (17)With (15), (16), and the sum-to-one constraint for ùëñ, ùëó ‚àà ùêæ1‚ä§ùëòùëûùëñ, ùëó= 1, (18)7 independent linear equations are for ùëÑ ‚àà ‚Ñù23by Ô¨Åxing1, 2-shiftgrams.
Lemma 1
For ùëò = 2 and ùëõ = 3, there are at most twotrigram probability tensors ùëÑ satisfy (18), (15), (16), and(16) for a given ùëÜ1(ùëÑ), ùëÜ2(ùëÑ), and ùëÜ3(ùëÑ), ifùúÉùëñ ùëó= e‚ä§ùëò,ùëñùëÜ1(ùëÑ)eùëò, ùëó(19)1‚ä§ùëòùëÜ1(ùëÑ)
= 1‚ä§ùëòùëÜ2(ùëÑ)(20)ùëÜ1(ùëÑ)1ùëò= ùëÜ2(ùëÑ)1ùëò. (21)Otherwise, there is no ùëÑ satisfying the equations (18), (15),(16), and (17).Proof Here we explicitly solve the equations (18), (15),(16), and (16) by letting the tensor ùëÑ as its variables.
SpeciÔ¨Åcally, the vectorized variables vec(ùëÑ)‚àà ‚Ñùùëòùëõisrequired to be in the kernel of the matrix ùê∂vec(ùëÑ)= ùë† ‚àà‚Ñù3ùëò2such that:ùê∂ := e3,1‚äó ùêºùëò2‚äó 1‚ä§ùëò+ e3,2‚äó 1‚ä§ùëò‚äó ùêºùëò2+ e3,3‚äó ùêºùëò‚äó 1‚ä§ùëò‚äó ùêºùëò(22)ùë† := e3,1‚äó vecÓòÄŒòùëÑÓòÅ+ e3,2‚äó vec(ùëÜ1(ùëÑ))+ e3,3‚äó vec(ùëÜ2(ùëÑ)).(23)This equation ùê∂vec(ùëÑ)= ùë† gives a set of 7 independentlinear equations, only if (19), (20), and (21) holds.
SpeciÔ¨Å-cally, the solution is ùëûùëñ| ùëó ùëò=
ùëéùëñ ùëó ùëòùë• +ùëèùëñ ùëó ùëòfor each ùëñ, ùëó, ùëò ‚àà
ùêæwith any ùë• ‚àà ‚Ñù, whereùëéùëñ ùëó= ùúÉ‚àí1ùëñ ùëó(‚àí1)ùëñ+ ùëó(1, ‚àí1)‚ä§(24)ùëèùëñ ùëó= ùúÉùëñ ùëóeùëò,2+Óòêùõø1ùëñùõø0 ùëóùëÜ(2)00+ ùõø0ùëñùõø1 ùëó ùëÜ(1)00+ ùõø1ùëñùõø1 ùëóÓòêùëÜ(1)01‚àí ùëÜ(2)00ÓòëÓòë(1, ‚àí1)‚ä§(25)Inserting ùëûùëñ| ùëó ùëò= ùëéùëñ ùëó ùëòùë• + ùëèùëñ ùëó ùëòto (17), it gives a quadraticequationùõºùë•2+ ùõΩùë• + ùõæ = 0, (26)whereùõº =√ïùëñ, ùëó ‚ààùêæùúÉ‚àí1ùëñ ùëó(27)ùõΩ = ùúÉ00(2ùëé000ùëè000+ ùëé010ùëè100+ ùëè010ùëé100)(28)+ ùúÉ01(ùëé000ùëè001+ ùëè000ùëé001+ ùëé010ùëè101+ ùëè010ùëé101)ùõæ = (ùëè0|00, ùëè0|10)Óòêùëè00ùëè01Óòë(ùúÉ00, ùúÉ01)‚ä§. (29)This quadratic equation has the leading coeÔ¨Écient ùõº ‚â† 0.Thus, it has at most two probability tensors ùëÑ satisfyingthe equations, unless the quadratic equation has a factor(ùëéùëñ ùëó ùëòùë• + ùëèùëñ ùëó
ùëò‚àí ùëûùëñ| ùëó ùëò) for some ùëñ, ùëó, ùëò ‚àà ùïÇ. ‚ñ°

4 Summary and Conjecture

Lemma 1 demonstrates a given set of ùëö-shiftgrams isgenerally suÔ¨Écient to reconstruct trigrams in two-wordlanguages up to Ô¨Ånite samples (there two possible trigramprobability tensors ùëÑ).
We expect that this special lemmacan be probably extended to any general ùëò > 2, and perhapsfor ùëõ > 3 as well.
This putative generalized theorem wouldfully explains why a set of ùëö-shiftgrams or skip-bigramsapproximates ùëõ-gram probabilities well.
Also this general-ized theorem would give mapping how higher ùëõ-grams areembedded into a series of ùëö-grams, and the number of suchmaps will be bounded by the number of words ùëò, which ismuch smaller than an exponential function of ùëõ.
Thus, itmay open up a theoretical explanation why n-grams, withan exponential number of combinations, can be learnedeÔ¨Éciently.
To tackle further general cases with more words ùëò > 2and higher ùëõ > 3-grams, we need to understand how con-volution ‚òÖ behaves over ùëõ-gram tensor and which algebrais suitable to understand such tensor operations.

Skip-bigrams reconstruct trigrams in 2-word languages

Shohei Hidaka

11

Japan Advanced Institute of Science and Technology



shhidaka@jaist.ac.jp



Abstract

In natural language processing, it has been empiricallyknown that skip-grams, co-occurrence statistics of twowords with some number of words in between them, isan eÔ¨Äective source of data to learn semantic nature of thewords.
In this study, we propose a new theoretical accountfor why a set of skip-grams is eÔ¨Äective at least for two-wordlanguages, by giving a theorem that a set of trigram prob-abilities is representable with a set of skip bigrams.
Thisrepresentation theorem justiÔ¨Åes the use of skip bigrams orso-called shiftgrams as a computationally eÔ¨Écient sourceto access higher order n-gram.


1 EÔ¨Äectiveness of skip-gram statis-



tics

In natural language processing, it has been empiricallyknown that semantic structure of words are represented bythe word vector by learning the skip-grams [1, 2], which isco-occurrence statistics ( ùëãùë°, ùëãùë°+ùë†) of a pair of word at ùë° andword ùë°+ùë† with a skip length ùë† = 1, 2, . .
..
There are previousstudies that have tried to explain this empirical Ô¨Ånding[3, 4, 5].
Most of such previous studies have hypothesizedthat the word vector gives a eÔ¨Äective representation due totheir special settings of the learning scheme of the wordvector models (i.e., negative sampling)[3].In this study, we take an apprroach distinct from theseprevious studies, and mathematically analyze the funda-mental nature of language systems, represented by n-gram statistics.
Our primary focus is how trigram statis-tics, the conditional probability ùëÉ(ùëãùë°+2|ùëãùë°, ùëãùë°+1), can berepresented by a set of ùë†-skip bigrams, ùëÉ(ùëãùë°+ùë†|ùëãùë°)
forùë† = 1, 2, . .
..
There is a trivial relationship that ùë†-skip bi-grams ùëÉ(ùëãùë°+ùë†|ùëãùë°) for each ùë† = 1, 2, . . .
is constructed by agiven trigram ùëÉ(ùëãùë°+2|ùëãùë°, ùëãùë°+1) of a Markov process.
Ourquestion is the converse ‚Äì can we construct the trigram onlyfrom a set of ùë†-skip bigrams?
This is a focus special case,that may be generalized to the relationship between ùë†-skipbigrams and a general ùëõ-grams.
If such fundamental rela-tionship between (n-1)-g rams and n-grams is established,it would explain why skip-gram statistics is a good sourceof data to learn semantic nature of words or language ingeneral ‚Äì skip-gram gives a suÔ¨Écient statistics of n-gramsand is computable eÔ¨Éciently.

2 Skip bigram

In this study, we assume a language ùêø has a setof ùëò words ùïéùëò:= {0, 1, 2, . .
.
, ùëò ‚àí 1}, and we call aMarkov process over a series of ùëã0, ùëã1, . . .
‚àà ùïéùëòlan-guage system.
In particular, a language system iscalled ùëõ-grams of ùêø, if ùëÉ(ùëãùë°|ùëãùë° ‚àí1, ùëãùë° ‚àí2, . .
.
, ùëãùë° ‚àíùëõ‚àíùë†) =ùëÉ(ùëãùë°|ùëãùë° ‚àí1, ùëãùë° ‚àí2, . . .
, ùëãùë° ‚àíùëõ) for any ùë° and ùë† = 0, 1, 2, . .
..
So any ùëõ-gram language system with ùëò words has(ùëò ‚àí 1)ùëòùëõ‚àí1parameters, those are the conditionalprobabilities ùëÉ(ùëãùë°|ùëãùë° ‚àíùëõ+1, . . .
, ùëãùë° ‚àí2, ùëãùë° ‚àí1) ‚â• 0
with√çùëãùë°‚ààùïéùëòùëÉ(ùëãùë°|ùëãùë° ‚àíùëõ+1, . . .
, ùëãùë° ‚àí2, ùëãùë° ‚àí1) = 1.
In this study,we assume any language system under analysis is ergodic,or equivalently it has a unique set of stationary probabili-ties.
To encode the joint random variables of ùëö-series, with-out loss of generality, we Ô¨Åx the encoder map ‚Ñéùëò,ùëö:ùïéùëöùëò‚Üí ùê∂ùëò,ùëö:= {1, 2, . . .
, ùëòùëö} by‚Ñéùëò,ùëö(ùëãùë° ‚àíùëö+1, ùëãùë° ‚àíùëö+2, . . .
, ùëãùë°) := 1 +ùëö√ïùëó=1ÓòÄùëãùë° ‚àíùëö+ ùëó‚àí 1ÓòÅùëòùëó‚àí1.(1)In this encoding of the joint random variables, the transitionmatrix ùëÑ2‚àà ‚Ñùùëò√óùëòof any bigram language system is of theformùëÑ2:=¬©Óö≠Óö≠Óö≠Óö≠Óö≠Óö≠¬´ùëû0|0ùëû0|1. . .
ùëû0| ùëò‚àí1ùëû1|0ùëû1|1. . .
ùëû1| ùëò‚àí1............
ùëûùëò‚àí1|0ùëûùëò‚àí1|1. . .
ùëûùëò‚àí1| ùëò‚àí1¬™¬Æ¬Æ¬Æ¬Æ¬Æ¬Æ¬¨,(2)where
ùëûùëñ| ùëó:= ùëÉ(ùëãùë°= ùëñ|ùëãùë° ‚àí1= ùëó) and√çùëñ‚ààùïéùëòùëûùëñ| ùëó=
1 forany ùëó ‚àà ùïéùëò.
Moreover, the transition matrix ùëÑ3‚àà ‚Ñùùëò2√óùëò2of any trigram language system is of the formùëÑ3:=√ïùëñ, ùëó ‚ààùïéùëòùëíùëò,ùëñ‚äó ùëíùëò, ùëóùëí‚ä§ùëò, ùëó‚äó ùëüùëñ, ùëó, (3)where ùëüùëñ, ùëó= (ùëûùëñ| (0, ùëó ), . . .
, ùëûùëñ| (ùëò‚àí1, ùëó )).Let
ùúÉ2= (ùúÉ0, ùúÉ1, . . .
, ùúÉùëò‚àí1)‚ä§‚àà ‚Ñùùëòbe the stationaryprobability vector of the bigram system such that ùúÉ2=ùëÑ2ùúÉ2, and ùúÉ3= (ùúÉ(0,0), ùúÉ(1,0), . . . , ùúÉ(ùëò‚àí1,ùëò‚àí1))‚ä§‚àà ‚Ñùùëò2bethe stationary probability vector of the trigram system suchthat ùúÉ3= ùëÑ3ùúÉ3.


2.1 Tensor form and tensor product


The set of ùëõ-gram conditional probabilitiesùëÉ(ùëãùë°|ùëãùë° ‚àí1, . .
.
, ùëãùë° ‚àíùëõ+1)is naturally represented bya ùëõthorder tensor (ùëõ-tensor in short).
Real-valued ùëõ-tensor‚Ñùùëò1√óùëò2√ó...√óùëòùëõis a vector space of real-valued maps{0, . . .
, ùëò1‚àí 1} √ó . . .
√ó {0, . .
. , ùëòùëõ‚àí 1} ‚Üí ‚Ñù. Let usdenote ‚Ñùùëòùëõ:= ‚Ñùùëò1√óùëò2√ó...
√óùëòùëõfor ùëò = ùëò1= ùëò2= . . .
= ùëòùëõ.We call a tensor product ‚òÖ : ‚Ñùùëòùëõ√ó ‚Ñùùëòùëõ‚Üí ‚Ñùùëòùëõconvo-lution deÔ¨Åned byùëÉ ‚òÖ
ùëÑ := (4)√ïùëãùë° ‚àí1‚ààùêæùëÉ(ùëãùë°, ùëãùë° ‚àí1, . .
.
, ùëãùë° ‚àíùëõ+1)ùëÑ(ùëãùë° ‚àí1, ùëãùë° ‚àí2, . . .
, ùëãùë° ‚àíùëõ),for ùëÉ, ùëÑ ‚àà ‚Ñùùëòùëõ.
In particular, denote for ùëö ‚â• 0ùëÑùëö:=Ô£±Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£≥ùê∏ùëõ, ùëòif ùëö = 0,ùëÑùëö‚àí1‚òÖ ùëÑ otherwise, (5)where ùê∏ùëõ, ùëò‚àà ‚Ñùùëòùëõis the left unit tensor satisfying ùê∏ùëõ, ùëò‚òÖùëÑ = ùëÑ for any tensor ùëÑ ‚àà ‚Ñùùëòùëõ. SpeciÔ¨Åcally,ùê∏ùëõ, ùëò(ùëñ1, ùëñ2, . .
.
, ùëñùëõ) =Ô£±Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£≥1 if ùëñ1= ùëñ20 otherwise.
(6)This convolution is useful to represent a time-shift oper-ation in the following sense: If ùëÑ ‚àà ‚Ñùùëòùëõis time-invariantùëõ-gram conditional probability ùëÑ(ùëãùë°, ùëãùë° ‚àí1, . . .
, ùëãùë° ‚àíùëõ+1) =ùëÉ(ùëãùë°|ùëãùë° ‚àí1, . . .
, ùëãùë° ‚àíùëõ+1), the convolution of ùëòthpower rep-resents shift in the time step of random variables:ùëÑùëò(ùëãùë°, ùëãùë° ‚àíùëò, . . .
, ùëãùë° ‚àíùëõ‚àíùëò+2)(7)= ùëÉ(ùëãùë°|ùëãùë° ‚àíùëò, . . .
, ùëãùë° ‚àíùëõ‚àíùëò+2).
(8)DeÔ¨Åne reduction operator ùëüùúÉ,ùêº: ‚Ñùùëòùëõ‚Üí
‚Ñùùëò|ùêº |for anyùêº ‚äÜ ùëÅ = { 1, 2, . .
. , ùëõ} for ùëÑ ‚àà ‚Ñùùëòùëõbyùëüùêº(ùëÑ) =√ï(ùëñùëó)‚àà{1,...,ùëõ}\ùêº‚ààùêæùëõ‚àí|ùêº|ùëÑ(ùëñ1, ùëñ2, . . .
, ùëñùëõ), (9)ùëö-shifgram ùëÜùëö: ‚Ñùùëòùëõ‚Üí ‚Ñùùëò2is deÔ¨Åned byùëÜùëö(ùëÑ) :
= ùëü{1,2}ÓòÄùëÑùëöŒòùëÑÓòÅ, (10)where ŒòùëÑis the stationar y tensor associated to ùëÑ.The ùëö-shiftgram of ùëõ-gram tensor ùëÑ ‚àà ‚Ñùùëòùëõhas thefollowing properties.
For ùëö‚Ä≤= 2, 3, . . .
, ùëõ,ùëÜùëö(ùëÑ) = ùëü{1,ùëö‚Ä≤}ÓòêùëÑùëö‚àíùëö‚Ä≤+2ŒòùëÑÓòë. (11)

3 Inverse problem



3.1 eÔ¨Äective isomorphism between tri-



grams and shiftgrams

Suppose we have the series of all ùëö-shiftgrams ùëÜ(ùëÑ) :
={ùëÜùëö(ùëÑ)}ùëö=0,1,...of some unknown ùëõ-gram probabilitytensor ùëÑ ‚àà ‚Ñùùëòùëõ.
Then can we uniquely identify theoriginal probability tensor ùëÑ that generates ùëÜ(ùëÑ)?
Letus focus on ùëõ = 3 in this paper.
For each Ô¨Åxedùëö‚Ä≤= 0, 1, . .
., we have ùëõ ‚àí 1 diÔ¨Äerent ùëö-shiftgramsùëÜùëö(ùëÑ)
= ùëü{1, (ùëö+2‚àíùëö‚Ä≤) }ÓòÄùëÑùëö‚Ä≤ŒòùëÑÓòÅfor ùëö‚Ä≤‚â§ ùëö ‚â§ ùëö‚Ä≤+ ùëõ ‚àí 2due to the identity (11).
For each ùëö‚Ä≤, ùëÑ is constrainedby (ùëõ ‚àí 1) matrices of ùëö-shiftgrams ùëÜùëö(ùëÑ) and the sum√çùëñ‚ààùêæùëÑùëö‚Ä≤(ùëñ, ùëó, ùëò) = 1 is also constrained.
So ùëÑùëö‚Ä≤mayhave at most (ùëò ‚àí 1)ùëõpolynomial equations, but only(ùëò ‚àí 1)ùëõ‚àí1equations are new constraints not expressedby (11) for ùëö‚Ä≤‚â• ùëõ ‚àí 1.
Thus, there are at mostùëò2+ ùëò (ùëò ‚àí1)
+ ùëö‚Ä≤(ùëò ‚àí1)2polynomial equations for a ser iesof ùëÜ0(ùëÑ), ùëÜ1(ùëÑ), . . . , ùëÜùëö‚Ä≤(ùëÑ), and thus at least ùëö‚Ä≤‚â• ùëò tohave the suÔ¨Écient number ùëò3of polynomial equations toidentify ùëÑ ‚àà ‚Ñùùëòùëõ.

3.2 Case with ùëò = 2 and ùëõ = 3

To be speciÔ¨Åc, let us study ùëò = 2 and ùëõ = 3as a minimal example.
In this case ùëÜ1(ùëÑ), ùëÜ2(ùëÑ) isneeded to have a suÔ¨Écient number of equations.
LetÓòêùëÑ1ùëÑ2. . .
ùëÑùëöÓòëdenote the third order tensor byseries of matrices ùëÑ (ùëñ, ùëó, ùëò) = ùëÑùëò(ùëñ, ùëó) for ùëñ, ùëó, ùëò ‚àà ùêæ. Forùëò = 2 and ùëõ = 3, the trigram probability tensor isùëÑ =Óòêùëû00ùëû10ùëû01ùëû11Óòë(12)= ùëû0|00ùëû0|10ùëû0|01ùëû0|11ùëû1|00ùëû1|10ùëû1|01ùëû1|11!, (13)where ùëûùëñ ùëó= (ùëû0|ùëñ ùëó, ùëû1|ùëñ ùëó)‚ä§‚àà ‚Ñù2, andùëÑ2=Óòê(ùëû00, ùëû10)ùëû00(ùëû01, ùëû11)ùëû10(ùëû00, ùëû10)ùëû01(ùëû01, ùëû11)ùëû11Óòë.(14)Thus, with the stationary tensor ŒòùëÑ(ùëñ, ùëó, ùëò) := ùúÉùëó ùëò, theùëö-shiftgrams for ùëö = 1, 2, 3 areùëÜ1(ùëÑ)
=√ïùëó ‚àà {0,1}ùëûùëñ ùëóùúÉùëñ ùëóùëí‚ä§ùëõ,ùëñ. (15)ùëÜ2(ùëÑ) =√ïùëñ‚àà {0,1}ùëûùëñ ùëóùúÉùëñ ùëóùëí‚ä§ùëõ, ùëó. (16)ùëÜ3(ùëÑ) =√ïùëñ, ùëó ‚àà {0,1}(ùëû0ùëñ, ùëû1ùëñ)ùëûùëñ ùëóùúÉùëñ ùëóùëí‚ä§ùëõ, ùëó. (17)With (15), (16), and the sum-to-one constraint for ùëñ, ùëó ‚àà ùêæ1‚ä§ùëòùëûùëñ, ùëó= 1, (18)7 independent linear equations are for ùëÑ ‚àà ‚Ñù23by Ô¨Åxing1, 2-shiftgrams.
Lemma 1
For ùëò = 2 and ùëõ = 3, there are at most twotrigram probability tensors ùëÑ satisfy (18), (15), (16), and(16) for a given ùëÜ1(ùëÑ), ùëÜ2(ùëÑ), and ùëÜ3(ùëÑ), ifùúÉùëñ ùëó= e‚ä§ùëò,ùëñùëÜ1(ùëÑ)eùëò, ùëó(19)1‚ä§ùëòùëÜ1(ùëÑ)
= 1‚ä§ùëòùëÜ2(ùëÑ)(20)ùëÜ1(ùëÑ)1ùëò= ùëÜ2(ùëÑ)1ùëò. (21)Otherwise, there is no ùëÑ satisfying the equations (18), (15),(16), and (17).Proof Here we explicitly solve the equations (18), (15),(16), and (16) by letting the tensor ùëÑ as its variables.
SpeciÔ¨Åcally, the vectorized variables vec(ùëÑ)‚àà ‚Ñùùëòùëõisrequired to be in the kernel of the matrix ùê∂vec(ùëÑ)= ùë† ‚àà‚Ñù3ùëò2such that:ùê∂ := e3,1‚äó ùêºùëò2‚äó 1‚ä§ùëò+ e3,2‚äó 1‚ä§ùëò‚äó ùêºùëò2+ e3,3‚äó ùêºùëò‚äó 1‚ä§ùëò‚äó ùêºùëò(22)ùë† := e3,1‚äó vecÓòÄŒòùëÑÓòÅ+ e3,2‚äó vec(ùëÜ1(ùëÑ))+ e3,3‚äó vec(ùëÜ2(ùëÑ)).(23)This equation ùê∂vec(ùëÑ)= ùë† gives a set of 7 independentlinear equations, only if (19), (20), and (21) holds.
SpeciÔ¨Å-cally, the solution is ùëûùëñ| ùëó ùëò=
ùëéùëñ ùëó ùëòùë• +ùëèùëñ ùëó ùëòfor each ùëñ, ùëó, ùëò ‚àà
ùêæwith any ùë• ‚àà ‚Ñù, whereùëéùëñ ùëó= ùúÉ‚àí1ùëñ ùëó(‚àí1)ùëñ+ ùëó(1, ‚àí1)‚ä§(24)ùëèùëñ ùëó= ùúÉùëñ ùëóeùëò,2+Óòêùõø1ùëñùõø0 ùëóùëÜ(2)00+ ùõø0ùëñùõø1 ùëó ùëÜ(1)00+ ùõø1ùëñùõø1 ùëóÓòêùëÜ(1)01‚àí ùëÜ(2)00ÓòëÓòë(1, ‚àí1)‚ä§(25)Inserting ùëûùëñ| ùëó ùëò= ùëéùëñ ùëó ùëòùë• + ùëèùëñ ùëó ùëòto (17), it gives a quadraticequationùõºùë•2+ ùõΩùë• + ùõæ = 0, (26)whereùõº =√ïùëñ, ùëó ‚ààùêæùúÉ‚àí1ùëñ ùëó(27)ùõΩ = ùúÉ00(2ùëé000ùëè000+ ùëé010ùëè100+ ùëè010ùëé100)(28)+ ùúÉ01(ùëé000ùëè001+ ùëè000ùëé001+ ùëé010ùëè101+ ùëè010ùëé101)ùõæ = (ùëè0|00, ùëè0|10)Óòêùëè00ùëè01Óòë(ùúÉ00, ùúÉ01)‚ä§. (29)This quadratic equation has the leading coeÔ¨Écient ùõº ‚â† 0.Thus, it has at most two probability tensors ùëÑ satisfyingthe equations, unless the quadratic equation has a factor(ùëéùëñ ùëó ùëòùë• + ùëèùëñ ùëó
ùëò‚àí ùëûùëñ| ùëó ùëò) for some ùëñ, ùëó, ùëò ‚àà ùïÇ. ‚ñ°

4 Summary and Conjecture

Lemma 1 demonstrates a given set of ùëö-shiftgrams isgenerally suÔ¨Écient to reconstruct trigrams in two-wordlanguages up to Ô¨Ånite samples (there two possible trigramprobability tensors ùëÑ).
We expect that this special lemmacan be probably extended to any general ùëò > 2, and perhapsfor ùëõ > 3 as well.
This putative generalized theorem wouldfully explains why a set of ùëö-shiftgrams or skip-bigramsapproximates ùëõ-gram probabilities well.
Also this general-ized theorem would give mapping how higher ùëõ-grams areembedded into a series of ùëö-grams, and the number of suchmaps will be bounded by the number of words ùëò, which ismuch smaller than an exponential function of ùëõ.
Thus, itmay open up a theoretical explanation why n-grams, withan exponential number of combinations, can be learnedeÔ¨Éciently.
To tackle further general cases with more words ùëò > 2and higher ùëõ > 3-grams, we need to understand how con-volution ‚òÖ behaves over ùëõ-gram tensor and which algebrais suitable to understand such tensor operations.


Acknowledgements

This work was supported by JSPS KAKENHIJP23H0369ÔºåJST PRESTO JPMJPR20C9.


References

[1] Tomas Mikolov, Wen-tau Yih, and GeoÔ¨Ärey Zweig.
Linguis-tic regularities in continuous space word representations.2013.[2] A Vaswani.
Attention is all you need.
Advances in NeuralInformation Processing Systems, 2017.[3]
Omer Levy and Yoav Goldberg.
Neural word embeddingas implicit matrix factorization.
In Advances in NeuralInformation Processing Systems, 2014.[4]
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,and Andrej Risteski.
Linear algebraic structure of wordsenses, with applications to polysemy.
Transactions ofthe Association for Computational Linguistics, Vol. 6,pp.
483‚Äì495, 2018.[5]
Takuma Torii, Akihiro Maeda, and Shohei Hidaka.
Dis-tributional hypothesis as isomorphism between word-wordco-occurrence and analogical parallelograms.
PloS one,Vol. 19, No. 10, p. e0312151, 2024.