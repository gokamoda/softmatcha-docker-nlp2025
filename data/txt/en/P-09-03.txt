Advancements in Sentiment Analysis: A MethodologicalExamination of News using multiple LLMs

Muhammad Ali Mahmood

1

, IÔ¨Äat Maab

2

, Muhammad Sibtain

1

, Asima Sar war

1

,



Muhammad Arsalan

3

, Masroor Hussain

1

FCSE, GIK Institute of Engineering Sciences and Technology, Topi

1

National Institute of Informatics, Tokyo

2

Technische Universit



¬®



at Braunschweig, Germany

3

{u2021346, u2021459, asima.sarwar, hussain}@giki.edu.pk

1

{maab}@nii.ac.jp

2

, {muhammad.arsalan}@tu-braunschweig.de

3

Abstract

As the digital news consumption continues to grow,sentiment analysis has become essential for understand-ing public opinion and the impact of media.
TraditionalNLP methods often fail to capture the in-depth emotions innews content, especially when taken from various mediasources.
This work identiÔ¨Åes gaps in the use of Ô¨Åne-tuneddeep learning models and large language models (LLMs)without Ô¨Åne-tuning in the sentiment analysis of news arti-cles, oÔ¨Äering enhanced insights across various model fam-ilies.
In our work, we collect news from BBC, and annotateBBC dataset using the proprietary OpenAI GPT-3.5-turbomodel, and Ô¨Åne-tune models such as DistilBERT, BERT,and RoBERTa-large.
We also compare Ô¨Åne-tuned modelswith LLM variants including Llama-3 and Qwen-2 modelswithout any model Ô¨Åne-tuning through crafted prompts.
Our results show that RoBERTa-large achieved the highestperformance, delivering an accuracy of 86%.


1 Introduction

Given the rise of misinformation, polarized reporting,and the complexities of contemporary global crises, it be-comes crucial to develop methods that can accurately in-terpret the language used in today‚Äôs digital media.
Politicalorientations, economic movements, and societal attitudesare all shaped by the media, which often carry subtle emo-tional undertones, whether intentional or not [1].
Tools thatcan unveil these hidden sentiments enhance transparencyand promote media literacy [2].Sentiment analysis using NLP has demonstrated sig-niÔ¨Åcant importance across diverse Ô¨Åelds such as marketanalysis, political opinion tracking, and social media an-alytics
[3, 4].
With the g rowth in online news and itsvarious formats, understanding how news articles expressemotional tone and sentiment becomes increasingly im-portant for stakeholders such as businesses, policymakers,and the general public [4].
Moreover, incorporating largelanguage models (LLMs) into this domain presents promis-ing avenues to improve the accuracy and interpretability ofsentiment classiÔ¨Åcation
[5].This paper investigates sentiment analysis on news ar-ticles collected from high-quality source i.e., BBC usingNewsAPI.
Traditional NLP models often struggle to cap-ture the nuanced emotions present in complex texts, espe-cially in news reporting, where the language used is oftensubtle and context-dependent [6].
While various senti-ment analysis tools exist, they frequently underperform inthe context of news articles due to limitations in their train-ing data and lack of domain-speciÔ¨Åc knowledge.
In ourstudy, we introduce a framework to enhance the accuracyof sentiment classiÔ¨Åcation while also oÔ¨Äer deeper insightsinto the narrative elements that shape public perception.
In our work, we utilize models such as BERT andRoBERTa [7, 8], which have been Ô¨Åne-tuned to determinesentiment classiÔ¨Åcation across news content.
We also uti-lize instruction-tuned open-source LLMs such as Llama-3(1B, 3B, 8B) and Qwen-2 (1.5B, 3B, 7B) without any Ô¨Åne-tuning to examine how they process sentiments comparedto Ô¨Åne-tuned approaches.
In addition, we identify the dom-inant words frequently used in news sources that inÔ¨Çuencesentiments, drawing on methodologies such as those de-scribed attention-based models by [9].
The signiÔ¨Åcance ofthis work lies in its response to the growing demand foradvanced analysis of contemporary and real-time medialandscapes.


2 Related Work

Sentiment analysis, particularly focusing on news con-tent, has signiÔ¨Åcantly advanced due to innovations in ma-chine learning (ML), deep learning (DL), and generativeAI (GenAI) approaches.
Each approach has contributeduniquely to the evolution of sentiment analysis techniques.
Popular classical techniques such as Naive Bayes, SVM,and the hybrid approaches have been adopted earlier for an-alyzing the sentiment from news articles.
[10] proposedNaive Bayes approach tailored for social and news sen-timent analysis to achieve higher performance on smalldatasets.
Similarly, [11] combined Naive Bayes and SVMfor text classiÔ¨Åcation, highlighting its eÔ¨Äectiveness in iden-tifying sentiment trends in news media.
[12] took sentimentanalysis a step further by incorporating eÔ¨Äective computingtechniques with SVM to facilitate more nuanced sentimentcategorization across a variety of news sources.
Build-ing on these approaches, [13] utilize a hybrid SVM modelto achieve greater accuracy, showing signiÔ¨Åcant improve-ments over traditional machine learning methods in senti-ment classiÔ¨Åcation.
Table 1: Summary of dataset distribution for three-classsentiment analysis.
Labels Number of Sentences PercentagePositive 3,500 35%Negative 3,500 35%Neutral 3,000 30%Total 10,000 100%Deep learning models like LSTM and BERT haveproven more eÔ¨Äective in sentiment analysis, par ticularlyexcelling with long-form news articles.
In this context,[14] showcased the strength of LSTM networks in identi-fying temporal patterns, making them particularly suitablefor analyzing sequenced news data.
Another work by Yanget.
al.
in [15] highlighted the exceptional capabilities ofBERT, a pre-trained transformer model, in delivering nu-anced sentiment classiÔ¨Åcation, achieving state-of-the-artperformance in news sentiment tasks.
Similarly, researchby [16] highlights the superior capability of LSTMs overCNNs in captur ing contextual sentiment in text analysis.[17] Ô¨Ånd strong eÔ¨Écacy of RoBERTa model for sentimentprediction, especially in the domain of politically sensitivenews content.
[18] use DistilBERT for real-time senti-ment classiÔ¨Åcation, achieving notable accuracy while sig-niÔ¨Åcantly reducing computational costs compared to largermodels.
Recent advancements in generative AI models, includ-ing GPT-3, have led to the development of robust frame-works for sentiment classiÔ¨Åcation through the training oflarge-scale language models.
As shown by [19], detectingsocial and news sentiments using GPT-3 also has trans-fer learning abilities for adapting to diverse datasets.
[20]investigated the integration of GPT-2 and RoBERTa formulti-source sentiment analysis, tackling issues of cross-domain adaptability and providing insights into eÔ¨Äectivesentiment classiÔ¨Åcation across varied news environments.
While sentiment analysis of news has advanced signiÔ¨Å-cantly, particularly with recent progress in machine lear n-ing, deep learning, and generative AI models, many ex-isting approaches still struggle to accurately analyze newscontent.
Traditional machine learning techniques, such asNa¬®ƒ±ve Bayes and SVM, failed to capture the complexity ofemotions in news articles because they lack contextual un-derstanding to interpret sentiments.
Although deep learn-ing models are more adept at handling sequential data, theyremain computationally expensive to train and may not ad-dress the contextual issues unless speciÔ¨Åcally Ô¨Åne-tuned onnews datasets.
While models like GPT-2 and GPT-3 haveshown promise for sentiment analysis, they are compu-tationally costly and require domain-speciÔ¨Åc Ô¨Åne-tuning,presenting challenges in practical applications.


3 Proposed Approach

Our methodology consists of a multiple-stage processfor analyzing the sentiment of news articles.
First, we uti-lize NewsAPI1Ôºâto collect data from BBC News corpus,selecting speciÔ¨Åc sources and keywords to Ô¨Ålter news arti-cles.
We Ô¨Åltered the news articles using speciÔ¨Åc keywordssuch as politics, technology, science, sports, etc. to remainrelevant to the targeted sentiment themes.
Second, we preprocess the data through text cleaningand tokenization.
The dataset was initially collected in theJSON for mat, after which it was cleansed and processedusing the Beautiful Soup library.
This initial step involvedremoving URLs, special characters, and stop words, ef-fectively eliminating extraneous information and allowing1Ôºâ
https://newsapi.org/PreprocessingText CleaningData LabellingOpenAI APISentimentAnalysisUsing MLFine Tune LargeLanguageModels(LLMs)TextBlobVaderFlairRobertaBERTDistilBertDominantWordExtractionFinalSentimentAI FrameworkData CollectionFigure 1: WorkÔ¨Çow pipeline for enhanced sentiment analysis of news articles, showing stages from data collection to Ô¨Ånalsentiment scoring.us to concentrate on the relevant text.
We then annotatethe collected samples, ensuring an equal distribution ofpositive, negative, and neutral sentiments, using widelyrecongnized GPT-3.5-turbo model to guide the labelingprocess.
In the Ô¨Ånal stages, we experiment with various traditionalsentiment analysis models, including TextBlob, Vader, andFlair.
Flair emerges as the top performer.
Flair‚Äôs per-formance served as as a baseline for experimenting withmore advanced models, i.e., we also Ô¨Åne-tune deep learningmodels such as DistilBERT, BERT, and RoBERTa-large onthe annotated dataset.
RoBERTa-large demonstrates su-perior performance.
Furthermore, zero-shot experimentsusing LLMs are performed with variants of Llama-3 andQwen-2 model families.
Table3 shows the template usedfor zero-shot experiments.
We also conÔ¨Ågure RoBERTamodel to extract dominant words associated with each sen-timent label, adding an interpretative layer to the resultsby highlighting inÔ¨Çuential words that drive the sentimentclassiÔ¨Åcation.
The integration of our Ô¨Åne-tuned models in sentimentanalysis pipeline show improved performance in discern-ing context-aware sentiment scores, which enhanced thepredictions compared to those from traditional models andLLMs used without any Ô¨Åne-tuning.
The workÔ¨Çow is illus-trated in Figure 1, detailing each phase from data collectionthrough to the Ô¨Ånal sentiment classiÔ¨Åcation step.
Table 1shows dataset distribution.
See Appendix A.1 for detailson dataset, A.2 for experimental setup and implementationdetails, and A.3 for evaluation metrics.
TextBlob Vader Flair010203040506070Figure 2: Comparison of ML models in terms of accuracy.


4 Results and Experiments

In our Ô¨Årst set of experiments, we use ML models for sen-timent classiÔ¨Åcation on our dataset, using TextBlob, Vader,and Flair.
The accuracy of each model was evaluated, withthe results summarized in Figure 2.
TextBlob achievedan accuracy of 50.36%, while VADER performed slightlybetter with an accuracy of 54.66%.
The best performanceamong the ML models was from Flair, with an accuracy of65.09%.We also compare Ô¨Åne-tuned DL models against LLMTable 2: Sample showing Sentiment Dominant (SD) scores across each sentiment type.
News Sample Sentiment Type SD ScoresFederal Reserve hints at potential rate hike amid inÔ¨Ça-tion concerns.
NEGATIVE hints: 5.99, hike: 4.13, inÔ¨Çation: 3.41, rate: 3.25,amid: 2.56Tech giant announces revolutionary smartphone withgroundbreaking features.
POSITIVE groundbreaking: 2.85, revolutionary: 1.54, smart-phone: 1.34, announces: 1.31, features: 0.98Global survey reveals shifting trends in consumer be-havior.
NEUTRAL shifting: 2.27, trends: 1.28, reveals: 0.99, survey:0.94, global:
0.85Prompt
LabelsYou are a sentiment analysis detector.
User: Classifythe following ‚Äúùë†ùëíùëõùë° ùëíùëõùëêùëí ‚Äù as ùë•ùëé, ùë•ùëè, or ùë•ùëêwithoutproviding additional details System: The sentence is:ùë•ùëé: Positiveùë•ùëè: Negativeùë•ùëê:
NeutralTable 3: Zero-shot prompt for the Llama-3 and Qwen-2models involves using the labels ùë•ùëé, ùë•ùëè, and ùë•ùëê, whichrepresent diÔ¨Äerent classes within the dataset.
Model Acc.
P R F1-scoreFine-tunedDistilBERT 80.40 76.35 76.25 76.29BERT 72.22 69.94 69.81 69.85RoBERTa-large 86.12 81.29 81.36 81.31No Ô¨Åne-tuningLlama-3.2-1B-Instruct 39.03 39.03 39.03 39.03Llama-3.2-3B-Instruct 51.97 54.51 51.97 51.97Llama-3-8B-Instruct 56.61 58.14 56.61 56.61Qwen-2-1.5B-Instruct 42.63 50.72 42.63 42.63Qwen-2.5-3B-Instruct 56.83 57.78 56.83 56.09Qwen-2-7B-Instruct 60.10 62.72 60.10 60.10Table 4: Summary of results with the Ô¨Åne-tuned modelsagainst LLMs with no Ô¨Åne-tuning in terms of accuracy(Acc.)
, P, R, and F1-scores for three-class sentiments.variants of Llama-3 and Qwen-2, noting that BERT, Dis-tilBERT, and RoBERTa-large demonstrated robust perfor-mance over ML models and LLMs.
Table 4 shows a sum-mary of our results.
Among Ô¨Åne-tuned models, it can beseen that RoBERTa-large outperformed with 86.12%, fol-lowed by DistilBERT with an accuracy of 80.40%, andBERT with 72.22%.
In case of LLMs used without anyÔ¨Åne-tuning, the Qwen-2-7B model demonstrated increasedperformance compared to a similarly sized Llama-3-8Bmodel, achieving an F1-score of 60.91 versus 56.61, re-spectively.
LLMs with smaller parameter counts, suchas Llama-3 (1B, 3B) and Qwen-2 (1.5B, 3B), do not ex-hibit signiÔ¨Åcant performance improvements.
These Ô¨Ånd-ings indicate that Ô¨Åne-tuned transformer-based models out-perform both traditional ML models and LLMs in handlingsentiment classiÔ¨Åcation tasks.
We found that while, Vaderand TextBlob provided some level of insight, their per-formance was limited, especially on sentences with subtleor compound sentiments, which RoBERTa handled moreeÔ¨Äectively.
In addition, we also integrate dominant word extraction,which identiÔ¨Åes key terms that inÔ¨Çuence sentiment catego-rization.
This approach adds interpretability to the model,an aspect often overlooked in previous studies.
Whilemost existing methods focus primarily on classiÔ¨Åcation ac-curacy, our technique oÔ¨Äers a deeper understanding of howspeciÔ¨Åc words impact sentiment labels.
For this purpose,we use our best performing model, RoBERTa-large.
Referto the results in Table2, which illustrates an example show-ing the dominant words and their respective SD scores.


5 Conclusion

The objective of our study is to better understand thesentiment analysis research by synthesizing results of ad-vanced proprietary LLMs, such as GPT-3.5-turbo, along-side various Ô¨Åne-tuned ML and DL models, against familyof LLMs used without any gradient updates.
As part ofthis study, we highlight open challenges associated withapplying these models to sentiment analysis.
Althoughthe Llama-3 and Qwen-2 models yield less substantial re-sults, they hold signiÔ¨Åcance in resource-constrained en-vironments where annotation can be costly.
Overall, ourÔ¨Åndings indicate that DL models, when Ô¨Åne-tuned, exhibitsubstantial capabilities in accurately detecting sentiments,outperforming traditional ML and generative LLMs.
No-tably, the RoBERTa model has proven eÔ¨Äective in senti-ment classiÔ¨Åcation within digital media contexts, althoughthe limited availability of labeled data may impede themodel‚Äôs generalizability.
Future work may involve senti-ment interpretation by integrating hybrid DL approacheswith LLMs, such as combining RoBERTa with XLNet orT5, to enhance the contextual analysis of sentiments.



Acknowledgements

The authors wish to express gratitude to the fundingorganisation as this study was carr ied out using the TSUB-AME4.0 supercomputer at Institute of Science Tokyo.

References


[1] Alexander Ligthart, Cagatay Catal, and Bedir Tekinerdo-gan. Systematic reviews in sentiment analysis: a tertiarystudy. ArtiÔ¨Åcial intelligence review, pp. 1‚Äì57, 2021.
[2] Kian Long Tan, Chin Poo Lee, and Kian Ming Lim. Asurvey of sentiment analysis: Approaches, datasets, andfuture research. Applied Sciences, Vol. 13, No. 7, p.4550, 2023.
[3] John Smith and Priya Patel. Explor ing sentiment analysistechniques in natural language processing. InternationalJournal of NLP Research, Vol. 12, No. 4, pp. 345‚Äì367,2020.
[4] Andrew Lee and Mei Lin Wong. A guide to sentimentanalysis using nlp. Journal of Data Science and Ap-plications, Vol. 10, No. 2, pp. 221‚Äì240, 2019.
[5] Ethan Brown and Olivia Taylor. Advancements in nlp:Deep learning models for sentiment analysis. DeepLearning and AI Applications, Vol. 15, No. 6, pp. 501‚Äì518, 2021.
[6] Bo Pang and Lillian Lee. Opinion mining and sentimentanalysis. Foundations and Trends in Information Re-trieval, Vol. 2, No. 1-2, pp. 1‚Äì135, 2008.
[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. Bert: Pre-training of deep bidirectional trans-formers for language understanding. In Proceedings ofthe 2019 Conference of the North American Chap-ter of the Association for Computational Linguis-tics: Human Language Technologies, pp. 4171‚Äì4186,2019.
[8] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, LukeZettlemoyer, and Veselin Stoyanov. Roberta: A robustlyoptimized bert pretraining approach. arXiv preprintarXiv:1907.11692, 2019.
[9] Zhenyu Li, Hua Zhang, and Yunfei Zhang. Attention-basedmodels for sentiment analysis. Computational Linguis-tics, Vol. 47, No. 1, pp. 123‚Äì145, 2021.
[10] H. Park, et al. Application of naive bayes in news senti-ment analysis. IEEE Transactions on ComputationalSocial Systems, 2020.
[11] B. Pang, et al. Sentiment analysis with naive bayes andsvm. Journal of Machine Learning Research, 2019.
[12] E. Cambria, et al. AÔ¨Äective computing and sentimentanalysis using ml. IEEE Transactions on AÔ¨ÄectiveComputing, 2021.
[13] X. Liu, et al. Hybr id svm for news sentiment analysis.International Journal of Data Mining, 2021.
[14] L. Zhang, et al. Sentiment analysis using lstm. Journalof Data Science, 2020.
[15] Q. Yang, et al. Bert for news sentiment analysis. IEEETransactions on Neural Networks and Learning Sys-tems, 2021.
[16] S. Tang, et al. Comparative study of cnn and lstm forsentiment analysis. Social Media Analysis Journal,2022.
[17] J. Sun, et al. Fine-tuned roberta for news sentiment. Jour-nal of Computational Linguistics, 2022.
[18] M. Liu, et al. Real-time sentiment analysis with distilbert.Journal of News Sentiment Analysis, 2023.
[19] T. Ahmed, et al. Gpt-3 for sentiment analysis of socialmedia. Social Media Research Journal, 2023.
[20] P. Brown, et al. Gpt-2 and roberta for advanced sentimentanalysis. Text Analytics Journal, 2023.
[21] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,et al. The llama 3 herd of models. arXiv preprintarXiv:2407.21783, 2024.
[22] AI Meta. Llama 3.2: Revolutionizing edge ai and visionwith open, customizable models. Meta AI, 2024.
[23] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, BowenYu, Chang Zhou, Chengpeng Li, Chengyuan Li, DayihengLiu, Fei Huang, et al. Qwen2 technical report. arXivpreprint arXiv:2407.10671, 2024.
[24] Hugging Face. The ai community building the future.URL: https://huggingface. co, 2021.



A Appendix



A.1 Dataset

In this study, we used a dataset custom-labeled froma collection of news articles obtained through NewsAPI.The initial dataset comprised roughly 18,000 unlabeledsamples from BBC News, chosen for its dependable andvaried coverage of multiple topics ranging from politics toscience, sports, and more.
We utilized the GPT-3.5-turbomodel to automatically assign sentiment categories: pos-itive, negative, or neutral‚Äïto each sample in the dataset.
Next, we downsample the data to achieve a balanced distri-bution of sentiments, resulting in a Ô¨Ånal dataset of 10,000samples, with approximately 3,500 instances of each sen-timent category.
Having a balanced dataset allowed usto train and evaluate our sentiment analysis models withincreased precision.


A.2 Experimental settings

We compare various Ô¨Åne-tuned models such as Distil-BERT, BERT, and RoBERTa-large for the task of sentimentclassiÔ¨Åcation.
In our experiments, a data split of 80/10/10for training, validation, and testing, respectively, is used,ensuring that the samples do not overlap between sets.
Ini-tially, we test with various batch size, learning rate, andthe number of training epochs to optimize the training ef-Ô¨Åciency of our models.
Weights decay and frequency oflogging was changed to prevent overÔ¨Åtting as well as tomonitor the training progress.
Moreover, for comparisonof our models with classical methods, we utilize modelsincluding TextBlob, Vader, and Flair to evaluate the im-provements oÔ¨Äered by the Ô¨Åne-tuned models.
The hyper-parameters used in the Ô¨Åne-tuning of our models are sum-marized in Table 5.
LLM variants include Llama-3
[21, 22]and Qwen-2
[23].
Moreover, we use PyTorch for modelimplementation, leveraging resources from HuggingFace[24] for DistilBERT, BERT, RoBERTa-large, Llama-3, andQwen-2, and the NLP-Toolkit for TextBlob, Vader, andFlair.


A.3 Evaluation Metrics

The metrics included accuracy (Acc.), precision (P), re-call (R), F1-score (F1), and Sentiment Dominance (SD),each tailored to the challenges inherent in analyzing senti-Table 5: Hyper-parameter settings used for Ô¨Åne-tuningmodels.
Hyper-parameter Settings for LLMsEpochs 5Learning rate 2 √ó
10‚àí5Batch size (train) 16Batch size (eval) 16Weight decay 0.01Logging steps 10Model saving strategy EpochTotal checkpoints saved 2Load best model at end Truement in news content.
SD is a unique metric for the model ability to Ô¨Ånd wordswhich are dominant, meaning words that carry majorityweight to the sentiment category it belongs to is calledSentiment Dominance.
In order to use a model to associatekey terms to the correct sentiment during Ô¨Åne tuning, weextract inÔ¨Çuential terms speciÔ¨Åc to each sentiment as ourmetric to evaluate model capability.