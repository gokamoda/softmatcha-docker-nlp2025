Constructing Open-source Large Language Model Evaluatorfor Japanese

Yikun Sun, Sakiko Yahata‚ÄÉ



Fei Cheng, Yugo Murawaki, Chenhui Chu, Sadao Kurohashi



Kyoto University



{sun,yahata}@nlp.ist.i.kyoto-u.ac.jp



‚ÄÉ{feicheng, murawaki, chu, kuro}@i.kyoto-u.ac.jp



Abstract

Evaluating the performance of large language models(LLMs) remains a crucial research topic, and conductinga comprehensive and accurate evaluation of LLM perfor-mance eÔ¨Éciently is challenging.
This challenge is par-ticularly acute for non-English languages.
GPT-4-basedautomated evaluation has proven eÔ¨Äective, demonstratinghigh consistency with human preference.
However, GPT-4-based evaluation still has several limitations, includingits closed-source nature and general preference.
This paperproposes an approach to construct an open-source JapaneseLLM evaluator, which has demonstrated robust consistencyon the Japanese Vicuna benchmark.
We present a methodfor rapidly generating score r ubrics that refer to speciÔ¨Åc in-structions, enabling more diverse evaluation criteria whenevaluating LLMs.
Our Japanese LLM evaluator trainingdata and models are available here.1Ôºâ2Ôºâ

1 Introduction

The quality assessment of text generated by large lan-guage models (LLMs) remains a signiÔ¨Åcant challenge inthe Ô¨Åeld of Natural Language Processing (NLP), as textquality directly reÔ¨Çects LLM performance [1, 2].
Cur-rently, utilizing high-performance LLMs (such as Chat-GPT) for automatic evaluation of LLM responses repre-sents a viable approach and demonstrates comparable eval-uation accuracy to human evaluation
[3].
However, thisapproach faces several inherent limitations, including itsclosed-source nature and limitations on general preference[4].1Ôºâ
https://huggingface.co/ku-nlp/jp
llm evaluator2Ôºâ
https://huggingface.co/datasets/ku-nlp/jp llmevaluator trainingPROMETHEUS
[5] developed an open-source LLMevaluator for English LLM evaluation.
This LLM canevaluate English LLM performance with diverse scorerubrics.
It demonstrates strong consistency with both hu-man evaluations and GPT-4-based evaluations.
However,PROMETHEUS exhibits limitations.
It is restricted toevaluating English-language LLMs.
Moreover, when itevaluates a speciÔ¨Åc instruction, it requires manually cre-ated score rubrics for that instruction.
Both of these re-quirements impose constraints on evaluation feasibility.
To address these limitations, we propose an open-sourceJapanese LLM Evaluator that provides diverse rubrics ofLLM performance evaluation.
Additionally, to generate di-verse score rubrics for diÔ¨Äerent instructions, we propose toenable the LLM evaluator to automatically generate scorerubrics for speciÔ¨Åc instruction evaluation.
To this end, we construct a dataset for training theJapanese LLM Evaluator.
This dataset includes diversescore rubrics and corresponding instructions, along withresponses ranging from quality scores 1 to 5 and respond-ing feedback, designed to guide the LLM evaluator inconducting evaluations.
Subsequently, we train an open-source Japanese LLM evaluator capable of eÔ¨Éciently as-sessing problems from multiple perspectives, meeting di-verse evaluation requirements.
Through experimentation,we demonstrated strong consistency with human evalua-tion on the Japanese Vicuna Benchmark [6].Furthermore, we train an open-source Japanese rubricsgenerator on our dataset.
It can automatically generateinstructive and diverse score rubrics based on speciÔ¨Åc in-structions.
Instead of generating rubrics manually, theseautomatic generation rubrics can guide LLM evaluationacross diÔ¨Äerent evaluation dimensions.50 English Seed Score RubricsRevised 50 Japanese Seed Score RubricsScore RubricsSelf-Instruction GenerationTranslation and post-editDiversity optimizationZero-shot GenerationInstruction and Reference AnswerReference-based GenerationResponse and FeedbackSeed TranslationSeed Rubrics AugmentInstruction and Reference Answer GenerationResponse and Feedback GenerationFigure 1 Japanese LLM evaluator training dataset constructionOur contributions are as follows:‚Ä¢ We generate Japanese LLM evaluator training datawith diverse score rubrics and instruction using GPT-4o.‚Ä¢
We train an open-source Japanese LLM evaluator onour dataset.‚Ä¢ Experiment results show the consistency withJapanese LLM evaluator and GPT-based evaluation.


2 Methodology

Our work consists of three main parts.
The Ô¨Årst part uti-lizes self-instruct
[7, 8, 9] to guide GPT-4o
[10] to automat-ically generate training data for Japanese LLM evaluators.
Through these high-quality training data, we can imple-ment various functionalities of Japanese LLM evaluatorsby performing supervised Ô¨Åne-tuning (SFT) on diÔ¨Äerentpre-trained LLMs.
The second part is training open-sourceJapanese LLM evaluators using high-quality training data.
The third part is to generate speciÔ¨Åc score rubrics for in-structions.
We will now elaborate on our work in detail.


2.1 Japanese LLM Evaluator Training



Dataset Construction

In Figure 1, this pipeline demonstrates how we utilize asmall number of seed tasks to guide GPT-4o in generatinghigh-quality training data with instructional eÔ¨Äectiveness.
This pipeline consists of four main steps:‚Ä¢ Seed Translation‚Ä¢ Seed Rubrics Augment‚Ä¢ Instruction and Reference Answer Generation‚Ä¢ Response and Feedback GenerationYou can Ô¨Ånd our detail process of Japanese LLM evaluatortraining dataset construction from Appendix A.

2.2 Japanese LLM Evaluator Training

In this section, we focus on the training process of theJapanese LLM evaluator.
For model training, we em-ploy low-rank adaptation (LoRA)[11] as our SFT trainingmethod.
This approach ensures model performance whilemaintaining computational eÔ¨Éciency.
The training data for the Japanese LLM evaluators com-prise the follow contents:‚Ä¢ Instruction: This is guidelines for directing modelresponses, covering various practically meaningful in-structional questions.‚Ä¢ Score Rubric:
This is evaluation criteria for spe-ciÔ¨Åc instructions, considering diÔ¨Äerent aspects of re-sponse quality evaluation.
For example, for speciÔ¨Åcinstruction, we may have multiple perspectives suchas ‚Äúcultural sensitivity,‚Äù ‚Äúgrammatical accuracy,‚Äù ‚Äúhu-mor sense‚Äù etc.
Under diÔ¨Äerent rubrics, the evaluationof responses will vary.
Score rubrics set score from1 to 5, enabling the LLM evaluator to understand andevaluate responses.‚Ä¢ Response: This is an answer obtained from diÔ¨ÄerentLLMs for speciÔ¨Åc instructions, serving as evaluationtargets for the LLM evaluator.‚Ä¢ Score: This is the output component of the LLMevaluator, summarizing the evaluation of responses.
The score is given directly with reference to scorerubrics, ranging from 1 to 5 as integers.‚Ä¢ Feedback: This is the output component of the LLMevaluator, providing detailed explanations for evalua-tion scores.
It encompasses understanding and inter-pretation of both response and score rubrics, explain-ing why speciÔ¨Åc scores were assigned under the givenrubrics.

2.3 Score Rubrics Generation during



Evaluation

When evaluating responses, the LLM evaluator needs tochoose diverse score rubrics for speciÔ¨Åc instructions.
Tra-ditionally, generating score rubrics has been done throughhandcraft.
To more eÔ¨Éciently generate diverse scorerubrics, we conducted SFT on pre-trained models to get anopen-source rubrics generator.
With our previously gen-erated training data, we paired ‚Äúinstructions‚Äù and ‚Äúscorerubrics‚Äù as training samples, guiding the rubrics genera-tor to automatically generate meaningful and diverse scorerubrics for various instructions.


3 Experimental Settings


In this section, we present the experimental settingsto evaluate our Japanese open-source LLM evaluator andscore rubrics generation.


3.1 LLMs as Evaluator

In this experiment, we chose three pre-trained modelsfor supervised Ô¨Åne-tuning.
The target pre-trained modelsinclude:‚Ä¢ Llama-3.1-8B‚Ä¢ llm-jp-3-13b‚Ä¢ Llama-3.1-Swallow-8B-v0.2LLama-3.1 model represents the current high-qualitycross-lingual pre-trained model in terms of scale and qual-ity.
It shows excellent potential across various tasks.
Llama-3.1-Swallow-8B-v0.2 and llm-jp-3-13b are base modelsthat took continued pre-training using Japanese datasets,exhibiting awesome performance in Japanese while main-taining English language capabilities.

3.2 LLM Evaluator Evaluation

We referenced the Japanese Vicuna Benchmark as ourevaluation benchmark.
This benchmark contains 80 di-verse questions designed to guide LLMs in generatingmeaningful responses.
Additionally, we referenced the score rubrics hand-crafted generated by PROMETHEUS for the Vicuna bench[3], which were translated from English to Japanesethrough human translation as evaluation criteria.
Based on the Japanese Vicuna Benchmark, we gener-ated 80 responses using the following LLMs as evaluationtargets for the LLM evaluator:‚Ä¢ GPT-4o‚Ä¢ llm-jp-3-13b
[12]‚Ä¢ Llama-3.1-Swallow-8B-v0.2 [13]‚Ä¢ Swallow-7b-hf
[14, 15]We evaluated the LLM evaluators‚Äô performance throughconsistency against the GPT‚Äì4-based evaluation.
GPT-based evaluation has demonstrated exceptional evaluationperformance and human consistency across many evalua-tion benchmarks.
It also shows higher eÔ¨Éciency comparedto manual evaluation.
We referenced GPT-4o scoring oftarget responses to calculate the consistency between theLLM evaluator and GPT-4-based evaluation.
To calculate consistency, we chose Pearson Correlationto compute the consistency:ùëü =‚àëùëõùëñ=1(ùë•ùëñ‚àí ¬Øùë•)(ùë¶ùëñ‚àí ¬Øùë¶)‚àö‚àëùëõùëñ=1(ùë•ùëñ‚àí ¬Øùë•)2‚àëùëõùëñ=1(ùë¶ùëñ‚àí ¬Øùë¶)2‚Ä¢ ùë•ùëñ: The score of the ùëñ-th question assessed by LLMevaluator.‚Ä¢ ùë¶ùëñ: The score of the ùëñ-th question assessed by GPT-4.‚Ä¢
¬Øùë• :
The mean score of all questions assessed by LLMevaluator (ùë•).‚Ä¢ ¬Øùë¶: The mean score of all questions assessed by GPT-4.‚Ä¢ ùëü: The Pearson Correlation CoeÔ¨Écient

3.3 Score Rubrics Generation Evaluation

We referenced instructions and handcrafted score rubricsfrom the Japanese Vicuna Benchmark.
We utilized theJapanese rubrics generator to generate score r ubrics forspeciÔ¨Åc instructions from this benchmark, a total of 80diÔ¨Äerent score rubrics.
Subsequently, we calculated theaverage F-measure between these generated score rubricsand handcrafted score rubrics using ROUGE-L to demon-strate the diversity and feasibility of our Japanese LLMevaluator‚Äôs generated score rubrics.
Figure 2 Pearson Correlation between our LLM evaluator and GPT-4-based evaluation.


4 Result Analysis



4.1 LLM Evaluator Evaluation

As shown in Figure 2, we can observe the Pearson Cor-relation between various base model LLM evaluators eval-uation results and GPT-4-based evaluation.
The JapaneseLLM evaluator demonstrates high Pearson Correlation re-sults for responses generated by Llama-3.1-Swallow-8B-v0.2, llm-jp-3-13b, and Swallow-7b-hf.
Regarding GPT-4o responses, only the LLAMA3 baseLLM evaluator shows a high Pearson Correlation.
How-ever, our additional analysis of GPT-4o average scoresacross diÔ¨Äerent LLM evaluators retails all of them is ap-proximately 4.3 score, demonstrating consistency in evalu-ation capabilities.
You can Ô¨Ånd the detail results of LLMsresponses average scores in Appendix B.

4.2 Score Rubrics Generation Evaluation

To evaluate the instruction with diverse criteria, wetrained the Japanese rubrics generator.
We evaluate thescore rubrics generation capability of the Japanese rubricsgenerator by calculating the ROUGE-L scores of scorerubrics generated by rubrics generator with speciÔ¨Åc instruc-tions.
All score rubrics newly generated for evaluation willbe compared with the score rubrics created handcrafted.
As results in Table 1, we can see the average ROUGE-Lscores of score rubrics generated by three Japanese LLMGenerator Llama-3.1-8b llm-jp-3-13b SwallowROUGE Score 0.243 0.211 0.270Table
1 Average ROUGE-L F-measure Scoreevaluators compared to handcrafted score rubrics.
Weobserve that all generated score rubrics have an averageF-measure higher than 0.2.
This shows that the JapaneseLLM evaluator provides instructive and diverse criteria forspeciÔ¨Åc instruction.


5 Conclusion

This paper presented a method for developing an open-source Japanese LLM evaluator.
With the human trans-lation of a small number of seed rubrics and utilizingGPT-4o, we constructed a comprehensive dataset for train-ing the Japanese LLM evaluator.
Subsequently, we per-formed SFT on the Japanese LLM evaluator using thistraining dataset, implementing the ability of LLM evalu-ation and score rubrics generation.
For LLM evaluation,the Japanese LLM evaluator demonstrates reliable con-sistency with GPT-4-based evaluation.
The score rubricsgeneration provides an automatic method for generating in-structive and diverse score rubrics by referencing speciÔ¨Åcinstructions, guiding the LLM evaluation.
For the futurework, we plan to explore the consistency between JapaneseLLM evaluation and human preference consistency.
More-over, we plan to make our Japanese LLM evaluator capablefor pairwise evaluation.



6 Acknowledgements

This work was supported by JSPS KAKENHI GrantNumber JP23K28144 and the ‚ÄúR&D Hub Aimed at Ensur-ing Transparency and Reliability of Generative AI Models‚Äùproject of the Ministry of Education, Culture, Sports, Sci-ence and Technology.

References


[1] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuo-han Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E.Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge withMT-Bench and Chatbot Arena, 2023.
[2] Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and XiaojunWan. LLM-based NLG Evaluation: Current Status andChallenges, 2024.
[3] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, andEric P. Xing. Vicuna: An Open-Source Chatbot Impress-ing GPT-4 with 90%* ChatGPT Quality, March 2023.
[4] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, ShayneLongpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sung-dong Kim, James Thorne, and Minjoon Seo. Prometheus:Inducing Fine-grained Evaluation Capability in LanguageModels, 2024.
[5] Seungone Kim, Juyoung Suk, Shayne Longpre,Bill Yuchen Lin, Jamin Shin, Sean Welleck, GrahamNeubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo.Prometheus 2: An Open Source Language Model Special-ized in Evaluating Other Language Models, 2024.
[6] Yikun Sun, Zhen Wan, Nobuhiro Ueda, Sakiko Yahata,Fei Cheng, Chenhui Chu, and Sadao Kurohashi. RapidlyDeveloping High-quality Instruction Data and EvaluationBenchmark for Large Language Models with Minimal Hu-man EÔ¨Äor t: A Case Study on Japanese. In NicolettaCalzolari, Min-Yen Kan, Veronique Hoste, AlessandroLenci, Sakriani Sakti, and Nianwen Xue, editors, Pro-ceedings of the 2024 Joint International Confer-ence on Computational Linguistics, Language Re-sources and Evaluation (LREC-COLING 2024), pp.13537‚Äì13547, Torino, Italia, May 2024. ELRA and ICCL.
[7] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, AlisaLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh Ha-jishirzi. Self-Instruct: Aligning Language Models withSelf-Generated Instructions, 2023.
[8] Or Honovich, Thomas Scialom, Omer Levy, and TimoSchick. Unnatural Instructions: Tuning Language Modelswith (Almost) No Human Labor, 2022.
[9] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.Baize: An Open-Source Chat Model with Parameter-EÔ¨Écient Tuning on Self-Chat Data, 2023.
[10] OpenAI. GPT-4 Technical Report. arXiv preprintarXiv:2303.08774, 2023.
[11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. LoRA: Low-Rank Adaptation of Large LanguageModels, 2021.
[12] LLM-jp: A Cross-organizational Project for the Re-search and Development of Fully Open Japanese LLMs,author=LLM-jp and : and Akiko Aizawa and Eiji Aramakiand Bowen Chen and Fei Cheng and Hiroyuki Deguchi andRintaro Enomoto and Kazuki Fujii and Kensuke Fukumotoand Takuya Fukushima and Namgi Han and Yuto Haradaand Chikara Hashimoto and Tatsuya Hiraoka and ShoheiHisada and Sosuke Hosokawa and Lu Jie and KeisukeKamata and Teruhito Kanazawa and Hiroki Kanezashiand Hiroshi Kataoka and Satoru Katsumata and DaisukeKawahara and Seiya Kawano and Atsushi Keyaki andKeisuke Kiryu and Hirokazu Kiyomaru and Takashi Ko-dama and Takahiro Kubo and Yohei Kuga and RyomaKumon and Shuhei Kurita and Sadao Kurohashi and Con-glong Li and Taiki Maekawa and Hiroshi Matsuda andYusuke Miyao and Kentaro Mizuki and Sakae Mizuki andYugo Murawaki and Akim Mousterou and Ryo Nakamuraand Taishi Nakamura and Kouta Nakayama and TomokaNakazato and Takuro Niitsuma and Jiro Nishitoba andYusuke Oda and Hayato Ogawa and Takumi Okamoto andNaoaki Okazaki and Yohei Oseki and Shintaro Ozaki andKoki Ryu and Rafal Rzepka and Keisuke Sakaguchi andShota Sasaki and Satoshi Sekine and Kohei Suda and SakuSugawara and Issa Sugiura and Hiroaki Sugiyama andHisami Suzuki and Jun Suzuki and Toyotaro Suzumura andKensuke Tachibana and Yu Takagi and Kyosuke Takamiand Koichi Takeda and Masashi Takeshita and MasahiroTanaka and Kenjiro Taura and Arseny Tolmachev andNobuhiro Ueda and Zhen Wan and Shuntaro Yada andSakiko Yahata and Yuya Yamamoto and Yusuke Yamauchiand Hitomi Yanaka and Rio Yokota and Koichiro Yoshino,2024.
[13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, AkhilMathur, Alan Schelten, Amy Yang, and Angela Fan et al.The Llama 3 Herd of Models, 2024.
[14] Kazuki Fujii, Taishi Nakamura, Mengsay Loem, HirokiIida, Masanari Ohi, Kakeru Hattori, Hirai Shota, SakaeMizuki, Rio Yokota, and Naoaki Okazaki. Continual Pre-Training for Cross-Lingual LLM Adaptation: EnhancingJapanese Language Capabilities. In Proceedings of theFirst Conference on Language Modeling, COLM, p.(to appear), University of Pennsylvania, USA, October2024.
[15] Naoaki Okazaki, Kakeru Hattori, Hirai Shota, Hiroki Iida,Masanari Ohi, Kazuki Fujii, Taishi Nakamura, MengsayLoem, Rio Yokota, and Sakae Mizuki. Building a LargeJapanese Web Corpus for Large Language Models. InProceedings of the First Conference on LanguageModeling, COLM, p. (to appear), University of Pennsyl-vania, USA, October 2024.
[16] Chin-Yew Lin. ROUGE: A Package for Automatic Evalu-ation of Summaries. In Text Summarization BranchesOut, pp. 74‚Äì81, Barcelona, Spain, July 2004. Associationfor Computational Linguistics.



A Japanese LLM Evaluator Train-



ing Dataset Construction Process



A.1 Seed Translation

The Ô¨Årst step involves constructing a high-qualityJapanese seed rubrics.
We referenced the handcrafted seedrubrics from PROMETHEUS and translated 50 of seedrubrics into Japanese through human translation to achievenative-level quality.
Subsequently, we can utilize GPT-4oto directly generate high-quality Japanese score rubric datagiven the seeds as few-shot examples.


A.2 Score Rubrics Augment

In this step, we augment the high-quality Japanese seedrubrics.
We construct a comprehensive prompt with spe-ciÔ¨Åc requirements to guide GPT-4o in automatically gener-ating new high-performance score rubrics.
In the prompt,we emphasize that these score rubrics should demonstratecreative and diverse evaluation capabilities, and assessproblems from multiple perspectives.
Finally, the rubricsspecify answer criteria for each score level from 1 to 5.During the self-instruct generation process, to ensure thediversity of generated score rubrics, we implement variousdiversity optimization methods to Ô¨Ålter and optimize thegenerated score rubrics.
For each generation round, wecompare the newly generated score rubrics with those inthe rubrics pool using two primary methods:‚Ä¢ We employ ROUGE-L [16] to evaluate the similaritybetween newly generated instructions and all previ-ously generated data in the instruction pool.
If thegenerated data exhibit a ROUGE-L score exceeding0.7 with any existing rubric in the rubrics pool, wesubject these data to a second optimization phase.‚Ä¢
The second optimization phase primarily involvesparaphrasing to restructure the generated scorerubrics.
Through constructing guiding prompts, wedirect GPT-4o to paraphrase the score rubrics.
Thisprocess focuses on modifying word choice and sen-tence structure while preserving the inherent meaningof the generated score rubrics.
Through GPT-4o batch generation, we successfully gener-ated 1K diverse score rubrics.


A.3 Instruction and Reference Answer



Generation

In this step, we construct corresponding instructions andresponse answers for the generated score rubrics.
Foreach generated score rubric, we guide GPT-4o to gener-ate, through zero-shot learning, 20 instructions that can beevaluated using the score rubric, along with correspondingscore 5 reference answers.
Each score rubric can be usedto evaluate various questions, while the reference answersserve as examples of optimal responses earning a score of 5under the respective score rubric.
Ultimately, we obtained20K diverse instruction-score rubric pairs.


A.4 Response and Feedback Generation

The Ô¨Ånal step focuses on generating responses scoringfrom scores 1 to 5 and corresponding evaluation feedbackfor the instruction-score rubric pairs.
We establish instruc-tive prompts to guide GPT-4o in generating correspondingresponses and feedback for each instruction and rubricspair.
Each reference answer serves as a score 5 reference,acting as a upper-bound for generating response levels.
Finally, we obtained a dataset of 100K sets, each setcontaining unique score rubrics, instructions, reference re-sponses, responses, and feedback.
This data set will beused for future Japanese LLM evaluator training.


B Average Scores of LLMs Re-



sponses

In this part, we give the average scores of LLMs re-sponses.
It also show the score consistency with GPT-4obased evaluation.
Table 2 Average Scores of LLMs ResponsesLLM GPT-4o llmjp-3- Llama3- Swallow-Evaluator 13b Swallow- 7b-hf8B-v0.2LLaMA3 4.43 4.52 3.65 1.58llmjp 4.32 4.50 3.35 1.43swallow 4.37 4.55 3.45 1.40GPT-4o 4.38 3.91 3.78 2.25