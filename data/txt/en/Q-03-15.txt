How Much Can Large Language Models Guide BodyMovements of 3D Digital Human Agents?

Kunhang Li

1

Jason Naradowsky

1

Yansong Feng

2

Yusuke Miyao

1,31

The University of Tokyo

2

Peking University

3

NII LLMC



{kunhangli, narad, yusuke}@is.s.u-tokyo.ac.jp



fengyansong@pku.edu.cn



Abstract

We aim to explore the extent to which Large LanguageModels (LLMs) can guide 3D digital human agents inperforming body movements without supervised training.
Given an existing human model and a textual instruction,we prompt the LLM to generate a high-level plan decom-posing the whole motion into consecutive steps, followedby specifying the positions of every body part in each step.
We then render the animation by linearly interpolating theselected body part positions across steps.
We evaluatethe generated animations from a diverse set of motion in-structions through both automatic and human evaluation,and ï¬nd that LLMs generally struggle to recognize accu-rate body part positions.
Speciï¬cally, LLMs struggle withcomplex motions with multiple steps and body parts, andcomplex body parts with more possible positions.


1 Introduction

Recent studies on Large Language Model (LLM)-basedgenerative agents [1, 2] demonstrate their capability toproduce open-ended behaviors in simulated environments.
However, these agents typically express actions and statesusing text or emoji symbols in the absence of pre-deï¬nedanimations.
This limitation poses challenges for imple-menting digital human agents in 3D environments, wherebody movements are essential for natural interactions.
Modern text-conditioned human motion generation ap-proaches employ generative models to synthesize realistichuman body movements from natural language instruc-tions
[3, 4], but they often struggle with open-ended mo-tion instructions due to overï¬tting to limited language-motion paired datasets [5, 6].
Existing work attempts toimprove the generalization by using LLMs to extract spe-ciï¬c motion-relevant information, such as active body parts[7], detailed body part descriptions
[8] and keyframe co-ordinates
[9].
However, these approaches typically utilizeLLMs in a limited capacity, primar ily as auxiliar y com-ponents in their pipelines.
We aim to explore to whatextent we can generate animations using only the rich in-formation provided by LLMs, potentially opening up newways to create human motions in lack of pre-implementedanimations.
In this paper, we present a framework that purely lever-ages LLMs to generate animations on SMPL
[10], a stan-dard 3D human model compatible with the Unity computergraphics engine.1ï¼‰We provide a sketch of SMPL in Ap-pendix A.1.
Given the input motion instruction, the frame-work ï¬rst uses LLMs to generate a structured animationplan with speciï¬c body part movements in natural lan-guage, then translates these descriptions into Unity codesspecifying SMPL parameters using predeï¬ned rules, andï¬nally renders the animation in Unity.
We conduct both automatic evaluation, where we cal-culate the accuracy of the LLM-selected positions againstannotated oracle ones, and human evaluation, where anno-tators evaluate the animations both overall and body-part-wise.
We have the following ï¬ndings:(I) LLMs generally struggle to recognize accurate bodypart positions: Compared with oracle standards, all testedLLMs exhibit signiï¬cant shortcomings in body part posi-tion identiï¬cation.
The highest performer in human eval-uation, Claude 3.5 Sonnet, trails the oracleâ€™s overall scoreby 1.28 points on a 5-point scale.(II) LLMs struggle with human motion complexity:Our analysis reveals a negative correlation between motioncomplexity (deï¬ned by the number of moved body parts1ï¼‰
https://unity.com/across steps) and the accuracy of selected body part posi-tions.
LLMs demonstrate lower accuracy for body partswith more possible positions like the upper ar m, comparedto more constrained parts like the upper leg.
Moreover,accuracy remains consistently higher for lower body com-ponents than their upper body counterparts, highlightingLLMsâ€™ diï¬ƒculties with complex and ï¬‚exible movements.


2 Animation Generation

LLMs primarily learn about human motions throughnatural language descriptions, rather than exact spatial co-ordinates or temporal quantities.
We therefore evaluateLLMsâ€™ human motion knowledge by testing their abilityto recognize appropriate body part positions described innatural language.
Figure1 illustrates our pipeline of ani-mation generation.
Firstly, given the joint structure of the SMPL model ğ‘€(Appendix A.1), we deï¬ne a ï¬nite set of positions Text(ğ‘€)for preset body parts.
Following the natural hierarchy ofhuman motion from action sequences to body part move-ments [11], we implement a hierarchical querying frame-work ğ‘„ that ï¬rst decomposes the input motion instructionğ¼2ï¼‰into sequential high-level steps, then iteratively speci-ï¬es body par t positions from Text(ğ‘€).
The LLM uses thisframework to acquire the animation plan ğ‘ƒ.
While the po-sition querying is conducted hierarchically,3ï¼‰we discussdiï¬€erent querying strategies in Appendix A.3.ğ‘ƒ = LLMğ‘„(ğ¼, Text(ğ‘€))(1)Secondly, we use predeï¬ned Rules to convert ğ‘ƒ intoUnity codes ğ¶ by mapping the speciï¬ed body part positionsto local joint rotations on ğ‘€, and inserting them into a codetemplate ğ‘‡ .4ï¼‰ğ¶
= Rules(ğ‘ƒ, ğ‘‡ )(2)Finally, we render the animation ğ´ by executing ğ¶ onğ‘€ in Unity, where joint rotations are linearly interpolatedbetween consecutive steps.ğ´ = Unity(ğ¶, ğ‘€)(3)2ï¼‰
We show our tested motion instructions in Appendix A.2.3ï¼‰
For example, when we query the position of the left elbow, ï¬rst weask whether it is str ight or bent.
If it is bent, we further ask whetherit is slightly bent in, bent in 90 degrees or fully bent.4ï¼‰
We avoid a naive method of generating Unity codes from thegiven motion instruction in one go, since the codes can seldom besuccessfully executed in Unity, and the few generated animations aretoo low-quality for evaluation.


3 Evaluation



3.1 Automatic Evaluation

For each motion instruction
, we ï¬rst ï¬x an oracle high-level plan by calibrating one high-level plan generated fromGPT-4o, and manually annotate the oracle positions of allbody parts across steps.
Then we calculate the accuracyof the LLM-selected positions against the annotated oracleones (Body Part Position Accuracy).
We run each LLMthree times to take the averaged accuracy.
The complexity of an annotated oracle motion is decidedby the numbers of moved body parts across steps.
There-fore, we deï¬ne a new metric Motion Complexity as thesum of step-wise ratios between moved and unmoved bodyparts (Equation 4), where ğ‘  denotes the step number and| Â· | represents the count of body parts.ğ‘€ğ‘œğ‘¡ğ‘–ğ‘œğ‘› ğ¶ğ‘œğ‘š ğ‘ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘¡ğ‘¦ =ğ‘âˆ‘ğ‘  =1|ğ‘šğ‘œğ‘£ğ‘’ğ‘‘ğ‘ ||ğ‘¢ğ‘›ğ‘šğ‘œğ‘£ğ‘’ğ‘‘ğ‘ |(4)

3.2 Human Evaluation

While automatic evaluation ï¬xes the oracle high-levelplans, we conduct human evaluation of the unconstrainedgeneration, to account for multiple valid ways of perform-ing a motion.
Each animation is assessed by ï¬ve indepen-dent annotators both overall and body-part-wise.
Overall Score.
Given one animation and the corre-sponding motion instruction, the evaluator checks to whatextent the animation is following the instructed motion,and gives one integer overall score from one to ï¬ve.
Body Part Label.
We ask human evaluators to checksix body parts in the animations â€” Head, Torso, Left Arm,Right Arm, Left Leg and Right Leg.
Evaluators classifyeach body part using one of four labels â€” â€œGoodâ€, â€œPar-tially Goodâ€, â€œBadâ€, and â€œNot Relevantâ€.
We introduce theâ€œNot Relevantâ€ label to distinguish between motion-criticalbody par ts (e.g., arms during throwing) and those that havelittle involvement in the action (e.g., legs during a stand-ing wave), while still marking any unnatural movement asâ€œBadâ€.
This separation helps evaluators provide targetedfeedback on the quality of key motion components.
Instead of showing oracle animations alongside LLM-generated ones during evaluation, we separately includethem in the evaluation pool to avoid biasing annotatorstoward a single reference motion while still establishingFigure 1: Pipeline of animation generation.an upper performance bound.
The inter-rater agreementshows moderate levels (weighted kappa of 0.531 for over-all scores, average pairwise agreement of 0.510-0.638 forbody parts), which is reasonable given the inherent vari-ability in human motion.


4 Results and Analysis

We run our probing pipeline on selected LLMs, includ-ing Claude 3.5 Sonnet, GPT-4o, GPT-4o-mini, GPT-3.5-turbo and Llama-3.1-70B. As shown in Table 1, whileoracle animations achieve an averaged Overall Score of4.57, all tested LLMs demonstrate substantial shortcom-ings.
The closest competitor, Claude 3.5 Sonnet, scores1.28 points below the oracle.
Body Part Position Accuracyfollows a similar pattern â€” the highest performing LLMsClaude 3.5 Sonnet and GPT-4o only achieve 72.71% and70.25% respectively.
Given that humans are sensitive tominor inaccuracies in body movements [12], these substan-tial performance gaps suggest that LLMs generally strugglewith accurately recognizing body part positions.
Further comparison of body par t motions reveals gener-ally large performance gaps between LLM-generated andoracle animations across all body parts, with varying de-grees of deï¬cit (Table 2, Figure 2).
For body part labels(Table 2), head and torso movements show smaller deï¬cits,Table 1: Averaged Overall Score and Body Part PositionAccuracy for each tested LLM.LLMAveragedOverall ScoreBody Part PositionAccuracy (%)Claude 3.5 Sonnet 3.29 72.71GPT-4o 3.13 70.25GPT-4o-mini 2.87 67.82GPT-3.5-turbo 2.14 66.90Llama-3.1-70B 2.13 52.51(Oracle Annotation) 4.57 100.00while arm and leg motions display signiï¬cant inaccura-cies.
Claude 3.5 Sonnet and GPT-4 lead in â€Goodâ€ andâ€Partially Goodâ€ labels, while GPT-3.5-turbo and Llama-3.1-70B dominate â€Badâ€ labels across all body parts.
BodyPart Position Accuracy (Figure 2) reveals that lower bodyparts achieve higher accuracy than their upper body coun-terparts (e.g., Knee versus Elbow), and complex body partswith more possible positions tend to have lower accuracythan simpler body par ts (e.g., Upper Arm versus Elbow).Complex Motions.
We analyze the correlation be-tween Motion Complexity and Body Part Position Accu-racy (Figure 3), and ï¬nd that LLMs tend to have lower BodyPart Position Accuracy when predicting complex motions.
Complex Body Parts.
Our analysis of the correla-tion between position prediction accuracy and number ofTable 2: Percentage (%) of body part labels (excluding â€œNot Relevantâ€) across evaluated LLMs.
G, PG, and B respectivelystand for â€œGoodâ€, â€œPartially Goodâ€, and â€œBadâ€.
Highest percentages for each label are highlighted in pink (G), yellow(PG), and gray (B).LLMHead Torso Left Arm Right Arm
Left Leg Right LegG PG B G PG B G PG B G PG B G PG B G PG BClaude 3.5 Sonnet 74.1
22.2 3.7 72.6 17.7 9.7 25.0 53.9 21.1 29.3 53.3 17.3 38.6 31.8 29.5 31.7 29.3 39.0GPT-4o 63.8 19.1 17.0 60.7 25.0 14.3 15.2 58.2 26.6 16.9 64.9 18.2 46.8 36.2 17.0 29.5 47.7 22.7GPT-4o-mini 80.7 8.8 10.5 59.4 28.1 12.5 12.8 47.4 39.7 12.2 52.7 35.1 17.9 33.3 48.7 11.1 33.3 55.6GPT-3.5-turbo 34.2 13.2 52.6 29.1 16.4 54.5 3.8 41.8 54.4 3.8 46.2 50.0 10.3 30.8 59.0 5.4 18.9 75.7Llama-3.1-70B 44.0 32.0 24.0 34.8 34.8 30.4 6.9 41.4 51.7 9.4 38.8 51.8 15.5 7.0 77.5 5.9 5.9 88.2(Average)
59.4 19.0 21.6 51.3 24.4 24.3 12.8 48.6 38.7 14.3 51.2 34.5 25.8 27.8 46.3 16.7 27.0 56.2(Oracle) 89.6
10.4 0.0 80.3 18.2 1.5 74.0 19.5 6.5 76.3 19.7 4.0 76.6 14.9 8.5 76.1 13.0 10.9Figure 2: Body Part Position Accuracy for each body partand tested LLM.
We average the accuracy for paired bodyparts, e.g., â€œElbowâ€ for â€œLeftElbowâ€ and â€œRightElbowâ€.Figure 3: Motion-wise correlation between Motion Com-plexity and the averaged Body Part Position Acccuracy.
Figure 4: Body-part-wise correlation between number ofpossible positions and the averaged Body Part PositionAcccuracy.possible positions for diï¬€erent body parts (Figure 4) re-veals two key patterns.
First, prediction accuracy tendsto inversely correlate with movement ï¬‚exibility â€” bodyparts with more possible positions (e.g., Upper Ar m) showlower accuracy compared to more constrained parts (e.g.,Upper Leg).
Second, comparison of the lower body perfor-mance (green line) and upper body performance (red line)demonstrates that LLMs achieve higher accuracy for lowerbody parts versus their upper body counterpar ts.


5 Conclusion

In this work, we explore the human motion knowledgeembedded in LLMs, and verify it from the generated ani-mations on the 3D human model SMPL.
We ï¬nd that LLMsunderstand human motions in natural language space to acertain degree, but struggle with accurate body part posi-tions, especially complex motions and body parts.



Acknowledgement

This work was partially supported by the â€œR&D HubAimed at Ensuring Transparency and Reliability of Gen-erative AI Modelsâ€ project of the Ministry of Education,Culture, Sports, Science and Technology.

References


[1] Joon Sung Park, Joseph C. Oâ€™Brien, Carrie J. Cai, Mered-ith Ringel Morris, Percy Liang, and Michael S. Bernstein.Generative agents: Interactive simulacra of human be-havior. In In the 36th Annual ACM Symp osiumon User Interface Software and Technology (UISTâ€™23), UIST â€™23, New York, NY, USA, 2023. Associationfor Computing Machinery.
[2] Zhilin Wang, Yu Ying Chiu, and Yu Cheung Chiu. Hu-manoid agents: Platform for simulating human-like gen-erative agents. In Yansong Feng and Els Lefever, edi-tors, Proceedings of the 2023 Conference on Em-pirical Methods in Natural Language Processing:System Demonstrations, pp. 167â€“176, Singapore, De-cember 2023. Association for Computational Linguistics.
[3] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,Xingyu Li, and Li Cheng. Generating diverse and natural3d human motions from text. 2022 IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition(CVPR), pp. 5142â€“5151, 2022.
[4] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shaï¬r, DanielCohen-or, and Amit Haim Bermano. Human motion dif-fusion model. In The Eleventh International Confer-ence on Learning Representations, 2023.
[5] Kunhang Li and Yansong Feng. Motion generation fromï¬ne-grained textual descriptions. In Proceedings of the2024 Joint International Conference on Computa-tional Linguistics, Language Resources and Evalua-tion (LREC-COLING 2024), pp. 11625â€“11641, Torino,Italy, May 2024. ELRA and ICCL.
[6] Ke Fan, Jiangning Zhang, Ran Yi, Jingyu Gong, YabiaoWang, Yating Wang, Xin Tan, Chengjie Wang, andLizhuang Ma. Textual decomposition then sub-motion-space scattering for open-vocabulary motion generation,2024.
[7] Nikos Athanasiou, Mathis Petrovich, Michael J. Black,and GÂ¨ul Varol. SINC: Spatial composition of 3D humanmotions for simultaneous action generation. ICCV, 2023.
[8] Yiming Huang, Weilin Wan, Yue Yang, Chris Callison-Burch, Mark Yatskar, and Lingjie Liu. Como: Controllablemotion generation through language guided pose code edit-ing, 2024.
[9] Han Huang, Fernanda De La Torre, Cathy MengyingFang, Andrzej Banburski-Fahey, Judith Amores, and JaronLanier. Real-time animation generation and control onrigged models via large language models, 2024.
[10] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-ard Pons-Moll, and Michael J. Black. SMPL: A skinnedmulti-person linear model. ACM Trans. Graphics(Proc. SIGGRAPH Asia), Vol. 34, No. 6, pp. 248:1â€“248:16, October 2015.
[11] Tamar Flash and Binyamin Hochner. Motor primitivesin vertebrates and invertebrates. Current Opinion inNeurobiology, Vol. 15, No. 6, pp. 660â€“666, 2005. Motorsytems / Neurobiology of behaviour.
[12] Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, JinluZhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang.Human motion generation: A survey. IEEE Transac-tions on Pattern Analysis and Machine Intelligence,Vol. 46, pp. 2430â€“2449, 2023.



A Appendix



A.1 3D Human Model SMPL

SMPL
[10] accurately generates the corresponding hu-man body shape given speciï¬ed pose parameters, i.e., jointlocal rotations.
We can manipulate SMPL by modifyingthese parameters.
For example, suppose that SMPL startsfrom an initial state extending two arms to the sides (FigureA1a), when we change the local rotation of the left elbowjoint m avg L Elbow from (0, 0, 0) to (0, 90, 0),SMPL bends the left elbow at 90 degrees (Figure A1b).(a) Initial (b) Left Elbow BentFigure A1: Overview of SMPL

A.2 Tested Motion Instructions

We collect 20 diverse motion instructions, covering dif-ferent body parts in a balanced way.5ï¼‰To show the potentialof application to an open-world game, we devise each mo-tion instruction to be a ï¬nite motion6ï¼‰related to a speciï¬cpractical scene, while avoiding commonly implementedanimations in games like walking.
Each instruction speci-ï¬es necessary elements to avoid ambiguity, while we alsoprevent it from being verbose.
The tested motion instruc-tions are shown in Table A1.


A.3 Impact of Querying Strategies

We investigate into the eï¬€ect of diï¬€erent LLM-queryingstrategies with GPT-4o.
After changing high-level plan-ning from generating piece-by-piece to in-one-go, the av-eraged overall score drops by 0.34.
For step-based iterativereï¬nement, we try selecting the body-part position frompredeï¬ned positions all at once or one-by-one, instead of5ï¼‰
We manually label involved body parts in all motion instructions.
The involved body parts and their counts are: Head (15), Torso (16),Arms (16 each), Legs (13 each).6ï¼‰
For example, â€œwalkingâ€ without constraints like â€œthree stepsâ€ canbe inï¬nite.hierarchically.
The performance drops respectively by 0.22and 0.31.Table A1: Tested Motion InstructionsMotion ID Motion Instruction1 Slide the window open from the center to the sideswith both hands.2 Water a 30-centimeter-tall plant using the wateringcan in the right hand.3 Look down to check the time of the watch on theleft wrist.4 Pat a 30-centimeter-tall dog in front of you on thehead with the right hand.5 Lean back fully and toss the ball into the air at a45-degree angle using both hands.6 Wipe down the 1-meter-high table in front of youwith a cloth in the left hand.7 Hold the glass with the left hand and pour the juicewith the right hand.8 Put a book on the 2-meter-high shelf with bothhands.9
Lift a 20-centimeter-high box from the ground tothe table on your left with both hands.10 Swing the golf club from right to left.11 Close the 2-meter-high store shutter door from topto bottom.12 Squat to pick up litter by the right foot with theright hand.13 Lift the right shoe with both hands and put it on inthe air.14 Perform a left-leg high side kick in Karate.15 Kneel in a traditional Japanese bow.16 Roll out a yoga mat on the ground.17 Crouch to check
a car tyre.18 Arch the back 60 degrees to relieve tension in thelower back muscles with two hands on the waist.19 Bend to the left to reach for an item by the left footwithout moving or bending the left leg.20 Walk through while ducking under a low-hangingbranch.