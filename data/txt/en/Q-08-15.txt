Sparse Autoencoders as a Tool for Steering the OutputLanguage of Large Language Models


Sebastian Zwirner

1,âˆ—

Wentao Hu

1,âˆ—

Koshiro Aoki

1

Daisuke Kawahara

1,21

Waseda University

2

NII LLMC



zwirner.seba@moegi.waseda.jp, huwentao@asagi.waseda.jp



aokikoshiro@akane.waseda.jp, dkw@waseda.jp

âˆ—

Equal contributors



Abstract

Recent advancements in Sparse Autoencoders (SAEs)have uncovered insightful features in large language models(LLMs).
In this study, we identify language-speciï¬c SAEfeatures, which are predominantly found in the later layersof the LLM.
Using these features, we steer the output lan-guage of an LLM.
In an experiment based on a translationtask, our method achieves a 49% accuracy in generating thedesired target language, outperforming a previous methodusing individual language neurons for steering.
This workdemonstrates the potential for SAE features for languagesteering.


1 Introduction

Large language models (LLMs) process information ina complex and compressed manner, making it diï¬ƒcult forhumans to understand.
This challenge extends to the ï¬eldof multilinguality, where multilinguality in LLMs is cur-rently being studied.
Recent research has shown the ex-istence of language neurons that can be used to steer theoutput language [1].
In parallel, recent progress in mech-anistic interpretability includes the development of SparseAutoencoders (SAEs)[2, 3], which help to break down thehidden activations of an LLM into simpler and more in-terpretable components, called features.
In this work, webuild on these advances and show that there are language-speciï¬c SAE features.
We then use these features to steeran LLMâ€™s output language.


2 Related work

This work builds on advances in research into multilin-guality in LLMs, activation steering, and SAEs.
Severalrecent studies have researched multilinguality in LLMs,providing insights into how these models handle multiplelanguages.
Muller et al.
[4] demonstrated that the mul-tilingual capabilities of LLMs are primarily concentratedin the ï¬rst and last layers, with a language-agnostic spaceoccupying the middle layers.
Wendler et al.
[5] found thatthe representations in the middle layers lie close to English.
In the area of activation steering, Suau et al.
[6] intro-duced a method to identify individual neurons associatedwith speciï¬c concepts and demonstrated how these neu-rons can be used to steer model outputs.
Building on this,Kojima et al.
[1] applied the concept of activation steeringto multilinguality, identifying language neurons and usingthem to steer a modelâ€™s output language.
A key challenge in using individual neurons for steeringis the problem of â€œpolysemanticityâ€ [7] and â€œsuperposi-tionâ€
[8], where a single neuron can represent multipleunrelated concepts simultaneously.
This complicates pre-cise control over the modelâ€™s behavior, as modifying oneneuron might unintentionally aï¬€ect other unrelated fea-tures.
In contrast, SAE features decompose the internalactivations into more interpretable components, therebypotentially reducing the risk of unintentionally activatingunrelated features.
Speciï¬cally, an SAE is a weak dictio-nary learning method applied to the internal activations ofa model, which allows us to decompose the residual streaminto largely human-understandable features [2, 3].
Thesefeatures can be used to steer a model output, as demon-strated and further improved by Chalnev et al.
[9].In this work, we use SAE features to investigate a newapproach to language steering, combining insights frommultilinguality, activation steering, and mechanistic inter-pretability.


3
Our method

Our steering method is fairly straightforward.
First, weï¬nd language-speciï¬c features in an SAE trained on theresidual stream of a layer of the target model.
Next, we usethese features to steer the modelâ€™s output language.


3.1 Finding language-speciï¬c features

In our ï¬rst step, we ï¬nd language-speciï¬c features ina series of pre-trained SAEs.
We employ the followingtwo individual approaches to identify language-speciï¬cfeatures.
Language classiï¬er approach We begin by observ-ing all features of a given SAE.
To determine language-speciï¬c features, we examine the contexts in which a fea-ture has its highest activations.
We then use a languageclassiï¬er to classify the language of each context.
If aplurality of the contexts belongs to a speciï¬c language, weclassify the feature as a language-speciï¬c feature for thatlanguage.
Feature description approach To identifylanguage-speciï¬c features based on their feature de-scription, we use Neuronpedia1ï¼‰.
Neuronpedia providesautointerpretability explanations generated by an LLM.This autointerpretability explanation is generated byshowing a featureâ€™s top activating contexts to an LLMand letting the LLM generate a likely explanation forthe featureâ€™s role in the model.
By searching theseexplanations for the names of the steering languages, weare able to ï¬nd language-speciï¬c features.


3.2 Steering model output

Numerous methods have been proposed to control thebehavior of LLMs through steering by intervening in theirinternal activations [10, 11, 12, 13].
In this study, we optfor the most common approach, which involves adding asteering vector to the activations [14].
In this method, thedecoder weights from a sparse autoencoder are extractedat the index corresponding to the desired language-speciï¬c1ï¼‰ https://www.neuronpedia.org/feature for constructing the steering vector.
During theforward pass, the steering vector is added to the residualstream, mathematically represented as:residâ€²= resid + ğ›¼ Â· steering vector,where ğ›¼ is a scaling factor that adjusts the intensity of thesteering, and resid refers to the residual stream, which is thesum of the outputs of all previous layers in the model.
Thisscaling factor allows the modelâ€™s output to be ï¬ne-tunedto align with the target language.
Notably, this minimallyinvasive approach hooks into the residual stream withoutmodifying the modelâ€™s architecture.


4 Experiments



4.1 Finding language-speciï¬c features

Training an SAE requires substantial LLM activationdata.
For example, the Gemmascope project2ï¼‰saved 20Pebibytes of activation data while training their SAEs [15].To avoid handling such large volumes of data, we used thepre-trained SAEs from the Gemmascope project.
Specif-ically, we based our research on SAEs trained on Gemma2 2B3ï¼‰.
These SAEs are trained on the residual streamsof each of the 26 layers of the model, resulting in 26 in-dividual SAEs.
Each SAE is conï¬gured with a hiddenlayer width of 214.
For our feature description approach,we also searched SAEs with a width of 216.
The SAEscome with a list of contexts for each featureâ€™s highest acti-vations.
Using the langid classiï¬er
[16], we classiï¬ed thelanguage of these contexts.
To cover an array of languagesfrom diï¬€erent language families, we focused on language-speciï¬c features from German, French, Spanish, Chinese,and Japanese.
By using the language classiï¬er approachexplained in Section 3.1, we found the language-speciï¬cfeatures shown in Figure 1.
@NOTE: include smth like:ï¬nally, bc other had too many features, we used manualThe language classiï¬er approach yielded many features,so we used our feature description approach explained inSection 3.1 to ï¬nd individual features to use in our steer-ing experiment.
We found the language-speciï¬c featuresshown in Table 3 (in Appendix) to be eï¬€ective in steering.
We did not identify English language features, a limita-tion that we further discuss in Section 5.2ï¼‰ https://ai.google.dev/gemma/docs/gemma scope3ï¼‰ https://huggingface.co/google/gemma-2-2b0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25Layer Number0100200300400500600Number of FeaturesNon-English Features per LayerFigure 1 Amount of language-speciï¬c features per layer foundby the language classiï¬er approach.


4.2 Steering model output

Experimental design For our steering experiments,we followed a setup similar to that of Kojima et al.
[ 1].We conducted two types of experiments: unconditionalgeneration and conditional generation.
For each experi-ment, we generated 100 samples.
For unconditional gen-eration, we used a simple â€œ<bos>â€ token (beginning-of-sequence token) as the prompt to initiate text generation.
For conditional generation, we employed the FLORES-200dataset
[17] to create a controlled translation task.
In thistask, we used a prompt of the following format:Translate an English sentence into a targetlanguage.
English: {source text}.
TargetLanguage:
In both experiments, we applied the language-speciï¬c fea-tures described in Table 3 (in Appendix) for steering.
To evaluate the eï¬€ectiveness of our method, we mea-sured two aspects: the accuracy of producing the desiredtarget language and the quality of the translations per-formed by the model.
In the unconditional generationtask, we only measured the accuracy, while in the condi-tional generation task, we calculated both accuracy and theBLEU score.
To calculate the accuracy, we classiï¬ed thelanguage of the generated text using the language identiï¬-cation classiï¬er FastText
[18].
Mirroring Kojima et al.
[1],we used a classiï¬cation score threshold of 0.5 and calcu-lated the ratio of the target language occurrence, leavingus with an accuracy value.
For the BLEU score in theconditional generation task, we calculated it between eachgenerated text and the corresponding ground-truth text.
Toensure comparability with Kojima et al.
[1], we mirroredthe settings used in his study.
To ï¬nd the optimal steering strength for each feature, weTable 1 Comparison of unconditional and conditional genera-tion results.
UnconditionalGenerationConditionalGenerationLanguage Strength Accuracy Accuracy BLEUSpanish (es) 80 74% 77% 0.6French (fr) 80 49% 74% 0.5Chinese (zh) 75 65% 74% 0.2German (de) 90 5% 14% 0.4Japanese (ja) 95
3% 4% 0.1ran unconditional generations of various steering strengthsand selected the steering strength that produced the highestaccuracy while still maintaining coherent output.
Steering results Table 1 shows the results for uncon-ditional and conditional generation after we selected theoptimal steering strength for each language.
A positive correlation was observed between steeringstrength and accuracy across all tasks, with a strongersteering strength resulting in model outputs that were moreclosely aligned with the intended language.
Increasing thesteering strength increased the accuracy until reaching aplateau, after which the accuracy did not improve further,as seen in Figure 2 (in Appendix).Table 2 shows some text generation examples.
Lowcoherence is noticeable.


5 Discussion

Comparison with Kojima et al.
Table 4 (in Ap-pendix) shows a comparison between our language steeringmethod based on SAE features, and the language neuron-based method introduced by Kojima et al.
[1].
For a cor-rect comparison, we implemented Kojima et al.â€™s steer-ing method on the Gemma 2 2B model.
In terms of ac-curacy, our approach outperformed the language neuron-based method across most languages.
However, both meth-ods struggled to output coherent text, as seen in the verylow BLEU scores in Table 4 (in Appendix), as well asin our example generations in Table 2.
Other generationsshowed even lower coherence than the ones presented inthis chart.
In contrast to the low BLEU scores of bothour and Kojima et al.â€™s method for steering on Gemma2 2B, in Kojima et al.â€™s work, higher BLEU scores wereachieved using a larger model, Llama 7B. However, sincewe used the comparatively small Gemma 2 2B, coherentoutput was not achieved by either method, and thus weTable 2 Generation Text ExamplesLanguage Conditional Generation Unconditional Generationde Enceladus ist im die von der unzenartigenobj auf.
In der Vorbereitungszeit fÂ¨ur den neuen Film StarWars: Das Erwachen der Macht" hat sich RegisseurJ.J. Abrams mit denes En 11:20, el policÂ´Ä±a espaËœnol en el policioespaËœnol en la policio espaËœnola
enlapoliciembreelpoliciembreelpolicThe 2019-20 temporada de baloncesto,
laÂ´unica encurso que no ha sido cancelada por la pandemia delCOVID-fr Construction estË‹a la cible pour cinq nouveauxmË‹etres carrÂ´es carrÂ´esË‹a
la hauteur de cette nouvelleconstruction rÂ´evolutionnelle du cË†otÂ´e, avec untrans port centre et memorial1.
Lâ€™application de lâ€™instance en appel est leprocË‹es-verbal de la rÂ´eunion du 25 mars 2016 ;ja TheãŸãˆã®ã†ãŸã„ã®ãã¦ãŠã†ã‚ˆã†ãªã„ã¦ãŠã†ã‹ãã«æ¬¡ã®â€ƒâ€ƒâ€ƒâ€ƒã“ã®ã“ã®ã“ã®ã“ã®ã€Œã€ã€ã“ã‚Œã‚’è¨€ã†2020å¹´1æœˆ19æ—¥ï¼ˆåœŸï¼‰ã®æ”¾é€å†…å®¹ä¸–ç•Œä¸€å¹¸ã›ãªç”·zh Lead ç ”ç©¶ã€å¯èƒ½æ—©ç™Œç—‡ã€åˆ†æ–‡é™é™ç—…å¯æ‚£è€…åœ¨ä½æ”¶å…¥å›½å®¶ã€å¯èƒ½æ—©æœŸç™Œç—‡16 å¹´ã€åœ¨ä¸æ—¥æœ¬æŸçŸ¥åå“ç‰Œåˆä½œçš„ç›®ä¸Šã€æˆ‘æ•´ä¸ªç›®è¡Œå…¨é¢ã€‚ä»è‰²ã€å¤–åˆ°å†…éƒ¨ç©ºcould not meaningfully compare the BLEU scores.
Absence of English language steering A notablelimitation of our approach is the absence of identiï¬edlanguage-speciï¬c features for English.
This is due to thefact that except for our language-speciï¬c features, nearlyall features activate on English tokens, making it diï¬ƒcultto isolate distinct English-only features.
Future researchcould focus on ï¬nding English-only features by checkingif a given feature activates only on English input and noother input.
Steering strength vs. output quality Kojima etal.
[1] discussed a trade-oï¬€ between the number of lan-guage neurons used for steering and the quality of thegeneration, as measured by the BLEU score.
In our ex-perimental setup, it is likely that the strength of steeringinï¬‚uenced the quality of t he generated output.
We veriï¬edthis manually by checking the coherence of the generatedtext.
However, due to the low quality of the generatedoutput, we could not investigate this relationship compre-hensively.
Coherence of the generated output It is notewor-thy that our SAE steering method failed to produce coher-ent output, as seen in the low BLEU scores and generationexamples.
We speculate that there are multiple reasonsfor this.
First, we measured the performance of steeringon the comparatively small Gemma 2 2B.
We speculatethat our method would produce more coherent output inlarger models, as the same trend can be seen in Kojima etal.
[1].
Second, although Gemma 2 2B can generate co-herent text when steered with other features, these featuresare typically English.
This suggests that the modelâ€™s lim-ited size and its predominantly English training data limitits steering success.
Third, SAE features may inï¬‚uencethe modelâ€™s behavior in unintended ways, as explored byChalnev et al.
[9].To address these challenges, we propose several potentialimprovements to our approach.
The most critical improve-ment is a better feature selection.
Future research should fo-cus on reï¬ning the method of identifying language-speciï¬cfeatures.
For instance, instead of classifying the languageof the entire contextâ€• much of which may not actuallyactivate the featureâ€• a higher weight could be given tothe tokens in the context that actually lead to an activationof the feature.
Also, to further investigate the reason forthe inability to produce coherent output, investigating thelanguage features with the method introduced by Chalnevet al.
[9] could prove fruitful.
Apart from an improved fea-ture selection, improvements in the steering methods canalso be explored.
For example, instead of single features,an average of multiple features could be used.


6 Conclusion

This study has demonstrated that language-speciï¬c SAEfeatures exist.
Although our method based on languagefeatures cannot generate coherent text, its accuracy is com-parable or superior to the method proposed by Kojima et al.
As highlighted in Section 5, there remain opportunities forimprovement.
We hope that this study will lead to furtherresearch into language-speciï¬c SAE features.
Understand-ing these language-speciï¬c features better will allow us tofurther uncover how multilinguality in LLMs works.



Acknowledgments

This work was supported by the â€œR&D Hub Aimed atEnsuring Transparency and Reliability of Generative AIModelsâ€ project of the Ministry of Education, Culture,Sports, Science and Technology.

References


[1] Takeshi Kojima, Itsuki Okimura, Yusuke Iwasawa, Hitomi Yanaka,and Yutaka Matsuo. On the multilingual ability of decoder-basedpre-trained language models: Finding and controlling language-speciï¬c neurons. In Proceedings of the 2024 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Technolo-gies (Volume 1: Long Papers), pp. 6912â€“6964, 2024.
[2] Robert Huben, Hoagy Cunningham, Logan Riggs Smith, AidanEwart, and Lee Sharkey. Sparse autoencoders ï¬nd highly inter-pretable features in language models. In The Twelfth Interna-tional Conference on Learning Representations, 2024.
[3] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen,Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Car-son Denison, Amanda Askell, Robert Lasenby, Yifan Wu,Shauna Kravec, Nicholas Schiefer, Tim Maxwell, NicholasJoseph, Zac Hatï¬eld-Dodds, Alex Tamkin, Karina Nguyen, Bray-den McLean, Josiah E Burke, Tristan Hume, Shan Carter,Tom Henighan, and Christopher Olah. Towards monoseman-ticity: Decomposing language models with dictionary learn-ing. Transformer Circuits Thread, 2023. https://transformer-circuits.pub/2023/monosemantic-features/index.html.
[4] Benjamin Muller, Yanai Elazar, BenoË†Ä±t Sagot, and DjamÂ´e Seddah.First align, then predict: Understanding the cross-lingual abilityof multilingual bert. In Proceedings of the 16th Conferenceof the European Chapter of the Association for Computa-tional Linguistics: Main Volume, pp. 2214â€“2231, 2021.
[5] Chris Wendler, Veniamin Veselovsky, Giovanni Monea, and RobertWest. Do llamas work in English? on the latent language ofmultilingual transformers. In Proceedings of the 62nd AnnualMeeting of the Association for Computational Linguistics(Volume 1: Long Papers), pp. 15366â€“15394, 2024.
[6] Xavier Suau Cuadros, Luca Zappella, and Nicholas Apostoloï¬€.Self-conditioning pre-trained language models. In InternationalConference on Machine Learning, pp. 4455â€“4473. PMLR,2022.
[7] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh,Michael Petrov, and Shan Carter. Zoom in: An introduction tocircuits. Distill, 2020. https://distill.pub/2020/circuits/zoom-in.
[8] Nelson Elhage, Tristan Hume, Catherine Olsson, NicholasSchiefer, Tom Henighan, Shauna Kravec, Zac Hatï¬eld-Dodds,Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse,Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wat-tenberg, and Christopher Olah. Toy models of superposi-tion. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/toy model/index.html.
[9] Sviatoslav Chalnev, Matthew Siu, and Arthur Conmy. Improvingsteering vectors by targeting sparse autoencoder features. arXivpreprint arXiv:2411.02193, 2024.
[10] Sheng Liu, Haotian Ye, Lei Xing, and James Y. Zou. In-contextvectors: Making in context learning more eï¬€ective and control-lable through latent space steering. In Forty-ï¬rst InternationalConference on Machine Learning, 2024.
[11] Eric Todd, Millicent Li, Arnab Sen Shar ma, Aaron Mueller, By-ron C Wallace, and David Bau. Function vectors in large languagemodels. In The Twelfth International Conference on Learn-ing Representations, 2024.
[12] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo,Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J.Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo,Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks.Representation engineering: A top-down approach to ai trans-parency. arXiv preprint arXiv:2310.01405, 2023.
[13] Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hub-inger, and Alexander Turner. Steering llama 2 via contrastive acti-vation addition. In Proceedings of the 62nd Annual Meetingof the Association for Computational Linguistics (Volume1: Long Papers), pp. 15504â€“15522, 2024.
[14] Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell,Juan J. Vazquez, Ulisse Mini, and Monte MacDiarmid. Steer-ing language models with activation engineering. arXiv preprintarXiv:2308.10248, 2024.
[15] Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, LewisSmith, Nicolas Sonnerat, Vikrant Var ma, JÂ´anos KramÂ´ar, Anca Dra-gan, Rohin Shah, and Neel Nanda. Gemma scope: Open sparseautoencoders everywhere all at once on gemma 2. In Proceed-ings of the 7th BlackboxNLP Workshop: Analyzing andInterpreting Neural Networks for NLP, pp. 278â€“300, 2024.
[16] Marco Lui and Timothy Baldwin. Cross-domain feature selectionfor language identiï¬cation. In Haifeng Wang and David Yarowsky,editors, Proceedings of 5th International Joint Conferenceon Natural Language Processing, pp. 553â€“561, Chiang Mai,Thailand, November 2011. Asian Federation of Natural LanguageProcessing.
[17] NLLB Team, Marta R. Costa-juss`a, James Cross, Onur CÂ¸ elebi,Maha Elbayad, Kenneth Heaï¬eld, Kevin Heï¬€ernan, ElaheKalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun,Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,John Hoï¬€man, Semarley Jarrett, Kaushik Ram Sadagopan, DirkRowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip FazilAyan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao,Vedanuj Goswami, Francisco GuzmÂ´an, Philipp Koehn, Alexan-dre Mourachko, Christophe Ropers, Saï¬yyah Saleem, HolgerSchwenk, and Jeï¬€ Wang. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672,2022.
[18] Armand Joulin, Edouard Grave, Piotr Bojanowski, and TomasMikolov. Bag of tricks for eï¬ƒcient text classiï¬cation. In MirellaLapata, Phil Blunsom, and Alexander Koller, editors, Proceed-ings of the 15th Conference of the European Chapter ofthe Association for Computational Linguistics: Volume 2,Short Papers, pp. 427â€“431, Valencia, Spain, April 2017. Asso-ciation for Computational Linguistics.



A Appendix

Table 3 Features used for steering.
Language SAE width Layer Feature IndexGerman 16k 23 3923French 16k 20 12332Spanish 16k 20 8590Chinese 65k 20 25936Japanese 16k 23 13998âˆ’10 0 10 20 30 4050 6070 80 90 1000102030405060708090100Steering strengthAccuracy (%)Figure 2 Correlation between steer ing strength versus accuracy (Spanish feature)Table 4 Performance of our method (SAE features) compared with the language neurons introduced by Kojima et al.
[1]Language Neurons SAE FeaturesLanguage Accuracy BLEU Accuracy BLEUGerman 3.0 0.0 14.0 0.4French 14.0 0.3 74.0 0.5Spanish 6.0 0.1 77.0 0.6Chinese 24.0 2.1 74.0 0.2Japanese 34.0 1.6 4.0 0.1