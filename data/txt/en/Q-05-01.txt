Low-Overhead Disambiguation for Generative LinguisticSteganography via Tokenization Consistency

Ruiyi Yan Yugo Murawaki



Kyoto University



ruiyi@nlp.ist.i.kyoto-u.ac.jp murawaki@i.kyoto-u.ac.jp



Abstract

Generative linguistic steganog raphy aims at embeddinginformation into natural language texts for covert transmis-sion.
However, in most tokenizer-based language modelapproaches, segmentation ambiguity during extraction canresult in errors or extraction failures.
Despite several ex-isting countermeasures (or disambiguation) that have beenproposed, none address this issue from the perspective oftokenization consistency.
Speciﬁcally, previous methodsexcessively modify candidate pools, compromising imper-ceptibility or embedding capacity.
To address it, we pro-pose a stepwise tokenization-veriﬁcation method whichprecisely removes error tokens for each step, ensuring100% tokenization consistency in the ﬁnal output.
Ex-perimental results demonstrate that our method surpassesbaseline approaches in text quality, imperceptibility, andanti-steganalysis capacity across various embedding rates.


1 Introduction

Linguistic steganography, a promising approach to safe-guarding information, involves concealing messages withintext.
Generative linguistic steganography (GLS)[1, 2, 3]has emerged as a dominant technique, enabling generatedsteganographic texts (referred to as stegotexts) across di-verse genres with consistent context, high ﬂuency, highnaturalness, high imperceptibility, and high embeddingcapacity, especially with advances in large language mod-els (LLMs)[4, 5].
However, in most existing GLS ap-proaches (except tokenization-free methods [6, 7]), thesender must detokenize stegotexts while the receiver mustretokenize them, leading to potential segmentation ambi-guity
[8, 9, 10, 11, 12].Several previous preﬁx-based methods [9, 10, 12] haveaddressed segmentation ambiguity, while the limitation ofSenderReceiverSecret message to be !embedded: 0111… …Language modelCandidate poolCodewordquestion00questionnaire01… …10… …11Update stegotext… …Stegotext to be extracted: questionnaires… …Language modelCandidate poolCodewordquestion00questionnaire01… …10… …11Segmentation ambiguityCase 1Case 2Figure 1 Example for segmentation ambiguity of generatingtokens in 4-token candidate pools coded by block encoding [1],where the receiver ﬁnds 2 tokens’ words matching the remainingstegotext ‘questionnaire...’, respectively ‘question’ and ‘question-naire’.
Thus, there are more than one extracting cases, only oneof which is true.them is over-preventively eliminating or merging tokensagainst extraction errors, at the expense of imperceptibility,embedding capacity, or eﬃciency.
Motivated by achieving100% disambiguation with minimal negative impact (lowoverheads on various performances), we propose a pre-cise disambiguating approach based on tokenization con-sistency between the sender-receiver pair.
The key idea isthat the sender runs the tokenizer during stegotext genera-tion, pre-emptively ensuring that the receiver can replicatethe original tokens without ambiguity.
The main contribu-tions of our method are as follows:1.
We propose a stepwise tokenization-veriﬁcationmethod that ensures that the receiver obtains tokens identi-cal to those generated by the sender.
The receiver tokenizesthe raw stegotext into tokens, allowing the extraction pro-cess to operate directly on tokens rather than on raw text.2.
At each generation step, through verifying and remov-ing only those tokens that do cause tokenization inconsis-tencies, we aim to minimize disruption to the candidatepools’ probability distribution.
As token candidates areassociated with codewords, multiple calls to the tokenizerare required.
Even if this seems ineﬃcient, ours operateswith linear complexity, and still oﬀers some advantagesover the O(𝑛2)(at least) complexity of previous methods.3.
Experimental results show that, at medium and higherembedding-rate intervals, our method outperforms existingapproaches, achieving at least 11.29% lower perplexity,7.53% lower candidate-level KL divergence, and 8.07%lower detection accuracy by steganalysis.
They show lowoverheads on various performances of our method.


2 Preliminaries



2.1 Notation of Linguistic Steganography

Alice (the sender) wants to communicate a secret mes-sage 𝑚 ∼ Unif({0, 1}𝐿) with Bob (the receiver) by em-bedding it in a choice of natural language cover text 𝑇𝑠(astegotext).
Alice and Bob have agreed on an embeddingfunction 𝑓𝑒𝑚𝑏and an extracting function 𝑓𝑒𝑥𝑡that performsteganography.
Alice and Bob also have access to the exactsame language model, LM𝑜, which can be used during em-bedding and extraction.
These two functions are supposedto be invertible.
In other words, 𝑓𝑒𝑚𝑏(LM𝑜, 𝑚) = 𝑇𝑠,𝑓𝑒𝑥𝑡(LM𝑜, 𝑇𝑠) = 𝑚′, and 𝑚′should be equal to 𝑚.

2.2 Generative Linguistic Steganography

At the micro level, during token-by-token generation,we denote the sequence text as 𝑆𝑒𝑞 = {𝑡𝑜𝑘𝑒𝑛𝑖}𝑛𝑖=1,where 𝑡𝑜𝑘𝑒𝑛𝑖represents the 𝑖𝑡 ℎtoken in the 𝑛-tokensentences.
To generate the next token (𝑡𝑜𝑘𝑒𝑛𝑛+1), thelanguage model predicts the candidate pool (CP) of𝑡𝑜𝑘𝑒𝑛𝑛+1through 𝑘 historical tokens (if any) of 𝑆𝑒𝑞,where 𝑃(𝑡𝑜𝑘𝑒𝑛𝑛+1|𝑡𝑜𝑘𝑒𝑛𝑛−𝑘+1, ..., 𝑡𝑜𝑘𝑒𝑛𝑛) is the transi-tion probability.
The 𝑡𝑜𝑘𝑒𝑛𝑛+1candidate pool is: CP𝑜𝑛+1={𝑐1𝑛+1, 𝑐2𝑛+1, ..., 𝑐|𝑉 |𝑛+1} with its corresponding probabilitydistribution: P𝑜𝑛+1= {𝑝1𝑛+1, 𝑝2𝑛+1, ..., 𝑝|𝑉 |𝑛+1} where 𝑉 is thewhole vocabulary of LM𝑜, andÍ|𝑉 |𝑗=1𝑝𝑗𝑛+1= 1.GLS utilizes redundancy of candidate pools to achievesteganography.
Through further sampling (e.g. top-k)and encoding P𝑜𝑛+1with Huﬀman coding [2] or arithmeticcoding [3] and so on, a steganographic candidate poolCP𝑠𝑛+1is obtained, with its probability distribution P𝑠𝑛+1.At the macro level, during embedding process, the lan-guage model in turn chooses a token in CP𝑠𝑡(𝑡 = 1, 2, ...)until it encodes the whole secret message; during extractionprocess, the language model in turn chooses and extracts atoken in CP𝑠𝑡(𝑡 = 1, 2, ...)
till the stegotext’s end.


2.3 Segmentation Ambiguity of GLS

The stegotext generated by 𝑓𝑒𝑚𝑏is essentially a sequencecomposed of tokens.
The sender must detokenize it us-ing a tokenizer into a stegotext before transmission.
Asshown in Figure 1, if the sender generates a token map-ping to “ question” and “naire”, the sender needs to deto-kenize them into the text “questionnaire” before sendingit to Bob.
However, the issue is that common wordslike “ questionnaire” often exist as independent tokens“ question” in the model’s vocabulary as well.
As a re-sult, a single piece of text can correspond to two or evenmore diﬀerent token representations.
Therefore, duringextraction 𝑓𝑒𝑥𝑡(LM𝑜, 𝑇𝑠), since both “ questionnaire” and“ question” exist in the candidate pool, Bob cannot deter-mine which token the sender embedded the message into.
This phenomenon is referred to as segmentation ambiguity.


2.4 Related Disambiguating Approaches

Recently, several solutions have emerged to address seg-mentation ambiguity in GLS.1) Basic Solution: Nozaki et al.
[9] proposed a simpledisambiguating approach, which removes tokens whosemapping subwords are preﬁxes of others during every gen-eration and extraction step.2) MWIS-based Solution:Yanet al.[10] considered theinﬂuence of removing candidate words on the probabilitydistributions and decided to process only if candidate-levelambiguity occurred.
Their solution identiﬁes the max-imum weight independent set (MWIS) in the candidatepool to reduce probability distortion.3)
SyncPool Solution: Qi et al.
[12] designed provablysecure disambiguating linguistic steganography based onambiguity pool grouping and synchronous sampling to ad-dress information loss and token synchronization issuesduring steganography, eliminating segmentation ambigu-ity without altering the distribution.


3 Methodology

The diﬀerences between the previous methods and ourproposed method are outlined as follows:1.
In previous methods, the receiver performs recurrentSenderReceiverCandidate poolConsistency?tokenk1✔tokenk2✔tokenk3✔tokenk4✘Secret message: !1011… …Candidate pooltoken1token2… …token|vocabulary|Language modelTop-k sampling (k=4)Candidate poolCodewordtokenk10tokenk210tokenk311Filtering & EncodingSelecting & Generatingtokenk2… …Token list114232…4Token-by-token generationDetokenizingTokenizingStegotext“Research on biology has …”
Token list114232…4Token-by-token extractionSecret message: !1011… …Secret message: !1011… …Candidate poolConsistency?tokenk1✔tokenk2✔tokenk3✔tokenk4✘Token list: !
[tokenk2, …]Candidate pooltoken1token2… …token|vocabulary|Language modelTop-k sampling (k=4)Candidate poolCodewordtokenk10tokenk210tokenk311Filtering & EncodingSelecting & Extracting10… …Figure 2 Overview and procedures of generative linguistic steganography with our tokenization-veriﬁcation approach.
For somesimplicity in this example, Huﬀman encoding [2] and top-4 sampling in each candidate pool is adopted.
Our stepwise tokenization-ver-iﬁcation step is implemented before steganographic encoding for both the sender and receiver.
Algorithm 1 Consistency Veriﬁcation for One TokenInput:𝑡𝑜:
Token to be veriﬁed;𝐿: Previously generated token list;Output:𝑅𝑒𝑠𝑢𝑙𝑡: Tokenization consistency or not (Tr ue or False);1:
𝐿𝑜← 𝐿.append(𝑡𝑜); /*
Token list to be veriﬁed*/2: 𝑇 𝑒𝑥𝑡 ← Tokenizer.decode(𝐿𝑜);3: 𝐿′←
Tokenizer.encode(𝑇 𝑒𝑥𝑡);4: 𝑅𝑒𝑠𝑢𝑙𝑡 ← (𝐿𝑜== 𝐿′);5: return 𝑅𝑒𝑠𝑢𝑙𝑡preﬁx stripping to reconstruct the token sequence, whereasin our method, the receiver simply calls the tokenizer.2.
Previous methods are proposed because directly call-ing the tokenizer can result in segmentation mismatches.
Our method avoids segmentation mismatches by stepwisetokenization veriﬁcation on the sender’s side.3. Candidate pool selection in previous methods isoverly pre-emptive, while in our method it remains pre-emptive but is more restrained.


3.1 Overall Steganographic System

Our proposed disambiguating method focuses on en-suring tokenization consistency between the sender andreceiver while keeping all processes on candidate poolsfully accessible for extraction.
As shown in Figure 2,the tokenization-consistency veriﬁcation step is placed be-tween the sampling and steganographic encoding steps.
Both the sender and receiver can verify whether each tokenin the candidate pool maintains tokenization consistency,allowing them to perform steganographic encoding on thesame ﬁltered candidate pools.
This guarantees that thereceiver can accurately extract the secret messages.

3.2 Tokenization-Veriﬁcation Method

The core challenge of this method lies in identifyingwhether tokens in the candidate pool cause tokenization in-consistency.
To address this, we propose a straightforwardand lightweight approach, detailed in Algorithm 1.
Thealgorithm veriﬁes the tokenization consistency for a singletoken by ﬁrst appending the token 𝑡𝑜to the existing tokenlist 𝐿 to form 𝐿𝑜(Line 1).
Next, 𝐿𝑜is detokenized intoa text string 𝑇 𝑒𝑥𝑡 using tokenizer.decode(), and then reto-kenized into a token list 𝐿′using tokenizer.encode()(Line2-3).
Finally, whether 𝐿𝑜is identical to 𝐿′is checked,returning a Boolean result (Line 4).The process simulates detokenization and retokenizationof the generated stegotext transmitted from the sender tothe receiver.
Tokens that cause tokenization inconsisten-cies are removed from the candidate pool, as they coulddisrupt the receiver’s extraction process.
By eliminatingsuch problematic tokens, the approach ensures that boththe sender and receiver maintain identical token sequences,enabling consistent and reliable steganographic extraction.
Table 1 Comparison among disambiguating approaches, Basic [9], MWIS
[10], SyncPool
[12] and our tokenization-consistencyapproach in perplexity, KL divergences, steganalysis accuracy and running time (seconds) under various embedding-capacity intervals1.0 ≤ BPT < 1.5 1.5 ≤ BPT < 2.0 2.0 ≤ BPT < 2.5 2.5 ≤ BPT <
3.0 3.0 ≤ BPT < 3.5PPL↓ KLD↓ ACC↓
Time↓
PPL↓ KLD↓
ACC↓
Time↓
PPL↓ KLD↓
ACC↓
Time↓
PPL↓ KLD↓
ACC↓
Time↓
PPL↓ KLD↓
ACC↓ Time↓Basic 5.780 0.823 0.943 1.265 9.098 0.867 0.761 0.886 13.799 0.945 0.703 0.704 21.363 0.985 0.621 0.600 31.554 1.012 0.708 0.558MWIS 4.662 0.590 0.855 2.930 6.758 0.567 0.648 1.747 9.218 0.506 0.635 0.835 12.963 0.453 0.583 0.731 18.169 0.432 0.750 0.810SyncPool 8.523 0.388 0.646 4.053 12.330 0.338 0.590 2.519 17.788 0.272 0.547 1.452 23.284 0.294 0.603 0.998 33.068 0.312 0.707 0.884Ours 4.847 0.593 0.926 3.317 6.692 0.546 0.830 2.322 9.203 0.539 0.741 1.558 12.281 0.413 0.491 0.895 16.813 0.362 0.583
0.8983.5 ≤
BPT < 4.0 4.0 ≤ BPT < 4.5 4.5 ≤ BPT < 5.0 5.0 ≤ BPT < 5.5 5.5 ≤ BPT <
6.0PPL↓ KLD↓
ACC↓
Time↓
PPL↓ KLD↓
ACC↓
Time↓
PPL↓ KLD↓
ACC↓
Time↓
PPL↓ KLD↓
ACC↓
Time↓
PPL↓ KLD↓
ACC↓ Time↓Basic 44.782 1.031 0.754 0.545 62.667 1.032 0.777 0.613 94.422 1.054 0.920 1.320 131.430 1.056 0.935 1.907 183.049 1.063 0.957 3.384MWIS 24.460 0.384 0.690 1.426 33.580 0.360 0.792 2.123 47.849 0.353 0.871 3.171 66.997 0.367 0.806 2.952
− − − −SyncPool − − − − − − − − − − − − − − − − − − − −Ours 22.824 0.318 0.521 1.075 30.592 0.259 0.669 1.555 42.278 0.189 0.759 2.375 57.104 0.172 0.746 2.595 76.858 0.158 0.726 2.814

4 Experiments and Discussion



4.1 Experimental Setup

To ensure fairness, all the following GLS experimentsusing various disambiguating approaches are conductedwith the same language model, llm-jp-3-1.8b1）[13], em-bedding a random 128-bit secret message, i.e. 𝑚 ∼Unif({0, 1}128).
All methods employ arithmetic cod-ing
[3].
We compare our method with three existing dis-ambiguating approaches ― Basic
[9], MWIS
[10], andSyncPool
[12] ― used as baselines.
To evaluate perfor-mance under varying embedding capacities, experimentsare conducted with diﬀerent top-k sampling values (k ∈ {4,8, 16, 32, 48, 64, 128, 256, 512, 1024, 2048, 4096}).
Foreach top-k value and for each disambiguating method, 500samples are generated and collected for analysis.


4.2 Primary Metrics

Bits per token (BPT) is a fundamental metric in linguis-tic steganography, measuring the embedding capacity.
Per-plexity (PPL) assesses the quality and ﬂuency of the gen-erated text.
KL divergence (KLD) between modiﬁed andoriginal candidate pools quantiﬁes statistical disparities, re-ﬂecting imperceptibility.
Steganalysis accuracy (ACC) isevaluated using a discriminator ﬁne-tuned from bert-base-Japanese2）, with further details provided in Appendix C.Finally, the running time (Time, in seconds) to embed asecret message indicates the steganographic eﬃciency.


4.3 Results

For each disambiguating method, experimental dataobtained under various top-k values are grouped into1）
Access: https://huggingface.co/llm-jp/llm-jp-3-1.8b2）
Access: https://github.com/cl-tohoku/bert-japaneseembedding-capacity intervals (1.0≤BPT<6.0).
Ta-ble 1 shows the average performance across these intervalsfor each approach.
Note that when the sample size in anygroup is 20 or fewer, the data is considered insuﬃcient andmarked as “−” in Table 1.For experimental groups with suﬃcient data inhigh embedding-capacity intervals (BPT ≥ 3.5), ourtokenization-consistency approach consistently achievesthe best performance in PPL, KLD, and ACC.
Althoughthe Basic approach [9] generally demonstrates the highesteﬃciency due to its lowest Time, our approach remainscompetitive and even surpasses Basic when BPT exceeds5.5.
This is because, for smaller top-k candidate pools,the detokenization and retokenization processes for eachtoken in candidate pools could make our method moretime-consuming than the most eﬃcient baseline.
How-ever, when top-k candidate pools are large, our method’slinear time complexity becomes more eﬃcient comparedto the O(𝑛2)(at least) complexity of other methods.
Overall, as shown in Table 1, our method outperformsthe baselines from moderate to high embedding-capacityintervals (2.0 ≤ BPT < 6.0).
When compared to the bestbaseline method for each metric in each interval, our ap-proach achieves an average reduction of 11.29% in PPL,7.53% in KLD, and 8.07% in ACC.


5 Conclusion

This paper addresses segmentation ambiguity in gen-erative linguistic steganography from the perspective oftokenization consistency, with the goal of minimizing thenegative impact of disambiguation.
Experiments demon-strate the advantages of our method over baselines acrossvarious metrics.
Furthermore, our proposed disambiguat-ing approach oﬀers generalizability to facilitate the broadﬁeld of reliable linguistic steganography.



Acknowledgment

This work was supported by JST SPRING, Grant Num-ber JPMJSP2110.

References


[1] Tina Fang, Martin Jaggi, and Katerina Argyraki. Gen-erating steganographic text with LSTMs. In Allyson Et-tinger, Spandana Gella, Matthieu Labeau, Cecilia Oves-dotter Alm, Marine Carpuat, and Mark Dredze, editors,Proceedings of ACL 2017, Student Research Work-shop, pp. 100–106, Vancouver, Canada, July 2017. Asso-ciation for Computational Linguistics.
[2] Zhong-Liang Yang, Xiao-Qing Guo, Zi-Ming Chen, Yong-Feng Huang, and Yu-Jin Zhang. Rnn-stega: Linguisticsteganography based on recurrent neural networks. IEEETransactions on Information Forensics and Security,Vol. 14, No. 5, pp. 1280–1295, 2019.
[3] Zachary Ziegler, Yuntian Deng, and Alexander Rush. Neu-ral linguistic steganography. In Kentaro Inui, Jing Jiang,Vincent Ng, and Xiaojun Wan, editors, Proceedings ofthe 2019 Conference on Empirical Methods in Nat-ural Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pp. 1210–1215, Hong Kong, China,November 2019. Association for Computational Linguis-tics.
[4] Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Bap-tiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,et al. Llama: Open and eﬃcient foundation language mod-els. arXiv preprint arXiv:2302.13971, 2023.
[5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, et al. Qwen technical report. arXiv preprintarXiv:2309.16609, 2023.
[6] Lingyun Xiang, Shuanghui Yang, Yuhang Liu, Qian Li,and Chengzhang Zhu. Novel linguistic steganographybased on character-level text generation. Mathematics,Vol. 8, No. 9, 2020.
[7] Ruiyi Yan, Tianjun Song, and Yating Yang. Token-free: A tokenization-free generative linguistic stegano-graphic approach with enhanced imperceptibility. Au-thorea Preprints, 2023.
[8] Honai Ueoka, Yugo Murawaki, and Sadao Kurohashi.Frustratingly easy edit-based linguistic steganography witha masked language model. In Kristina Toutanova, AnnaRumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Belt-agy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,and Yichao Zhou, editors, Proceedings of the 2021Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pp. 5486–5492, Online, June2021. Association for Computational Linguistics.
[9] Jumon Nozaki and Yugo Murawaki. Addressing segmenta-tion ambiguity in neural linguistic steganography. In YulanHe, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang,editors, Proceedings of the 2nd Conference of theAsia-Paciﬁc Chapter of the Association for Com-putational Linguistics and the 12th InternationalJoint Conference on Natural Language Processing(Volume 2: Short Papers), pp. 109–116, Online only,November 2022. Association for Computational Linguis-tics.
[10] Ruiyi Yan, Yating Yang, and Tian Song. A secure and dis-ambiguating approach for generative linguistic steganog-raphy. IEEE Signal Processing Letters, Vol. 30, pp.1047–1051, 2023.
[11] Ruiyi Yan, Tianjun Song, and Yating Yang. Segfree:Segmentation-free generative linguistic steganographicapproach for unsegmented languages. AuthoreaPreprints, 2023.
[12] Yuang Qi, Kejiang Chen, Kai Zeng, Weiming Zhang,and Nenghai Yu. Provably secure disambiguat-ing neural linguistic steganography. arXiv preprintarXiv:2403.17524, 2024.
[13] Akiko Aizawa, Eiji Aramaki, Bowen Chen, Fei Cheng, Hi-royuki Deguchi, Rintaro Enomoto, Kazuki Fujii, KensukeFukumoto, Takuya Fukushima, Namgi Han, et al. Llm-jp: A cross-organizational project for the research anddevelopment of fully open japanese llms. arXiv preprintarXiv:2407.03963, 2024.
[14] Falcon Dai and Zheng Cai. Towards near-imperceptiblesteganographic text. In Anna Korhonen, David Traum,and Llu´ıs M`arquez, editors, Proceedings of the 57thAnnual Meeting of the Association for Computa-tional Linguistics, pp. 4303–4308, Florence, Italy, July2019. Association for Computational Linguistics.
[15] Jiaming Shen, Heng Ji, and Jiawei Han. Near-imperceptible neural linguistic steganography via self-adjusting arithmetic coding. In Bonnie Webber, TrevorCohn, Yulan He, and Yang Liu, editors, Proceedings ofthe 2020 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pp. 303–313,Online, November 2020. Association for ComputationalLinguistics.
[16] A.A. Fedotov, P. Harremoes, and F. Topsoe. Reﬁnementsof pinsker’s inequality. IEEE Transactions on Informa-tion Theory, Vol. 49, No. 6, pp. 1491–1498, 2003.
[17] Diederik P. Kingma and Jimmy Ba. Adam: A method forstochastic optimization, 2017.



A Imperceptibility of GLS

Following the previous formulation [14, 15]
, statisti-cal imperceptibility refers to the similarity between thetrue language model LM𝑡in the monitored channel andLM𝑠which is the language model LM𝑜integrated withsteganographic algorithms.
Speciﬁcally, the total variationdistance (TVD) is used to measure statistical impercep-tibility.
Consider the TVD between LM𝑡and LM𝑠, i.e.𝑑(LM𝑡, LM𝑠), by triangle inequality:𝑑(LM𝑡, LM𝑠) ≤
𝑑(LM𝑡, LM𝑜), 𝑑 (LM𝑜, LM𝑠)(1)As 𝑑(LM𝑡, LM𝑜) is a criterion to measure the original lan-guage model, which is limited by the research on languagemodels.
Thus, 𝑑(LM𝑜, LM𝑠) is the main focus of GLStechniques.
According to Pinskers inequality [16] and additivity ofKL divergence, 𝑑 (LM𝑜, LM𝑠) can be further decomposedin each step, that is3）:𝑑(LM𝑜, LM𝑠) ≤vtln 22∞Õ𝑡=1𝐷𝐾 𝐿(P𝑜𝑡||P𝑠𝑡)(2)where P𝑜𝑡is the original probability distribution at 𝑡𝑡 ℎstep,and P𝑠𝑡is transformed from P𝑜𝑡via sampling and encoding.
Hence, GLS could aim to minimize 𝐷𝐾 𝐿(P𝑜𝑡||P𝑠𝑡), in orderto obtain relative near-imperceptibility.


B Computational Resources

All experiments are implemented in Python 3.12.7 withTorch 2.5.0, running on a 2.0 GHz CPU and accelerated byusing 8 × NVIDIA RTX A6000 GPUs.


C Details of Steganalysis

Positive samples are collected from stegotexts gener-ated using various top-k samplings, while negative sam-ples are sourced from non-steganographic texts.
All textsare generated from the same prompt, “それで”.
Dur-ing the training phase, both positive and negative samplesconsist of 19,200 instances each.
For testing, 4,800 un-trained positive samples are used, categorized into diﬀerentembedding-capacity intervals as shown in Table 1.
In eachembedding-capacity interval and for each disambiguatingapproach, only stegotexts with a sample size greater than3）
Some derivation is omitted here, as details are veriﬁed in [14, 15,16].Table 2 Examples generated texts using the prompt ‘それで’by llm-jp-3-1.8bA stegotext generated by our proposed methodそれで、この状況が起きてしまう時に最初に考えるべきことは「リスクの洗い出し」です。リスクが本当に想定した状況の範囲内で起き(Perplexity = 27.778; Bits per token = 4.414)それでいて、どこか開放的で華やかな彩のある小物たちは、時代と共に流行り廃りはあるものの、決して色あせない(Perplexity = 49.150; Bits per token = 5.333)A non-steganographic text generatedそれで「次に読む本が無い問題」「読んでいる本を人に薦める方法が無い問題」に対応するため、(Perplexity
=
16.653)それで今は、私が好きな作家さんの作品の一部を借りて描き出す形でご一緒しています。作品の解釈を伝える(Perplexity = 56.774)20 are included in the tests; otherwise, “−” is marked toindicate insuﬃcient data.
Given the signiﬁcant variation in the lengths of positivesamples, we adjust the negative samples to vary between20 and 128 tokens to ensure that the trained discriminatoris not sensitive to text length.
Additionally, all texts arepadded or truncated to 128 tokens, so that positive samplescannot be distinguished as steganographic based solely ontheir length.
For ﬁne-tuning the BERT model, we useAdam [17] as the optimizer with a learning rate of 5×10−5.The batch size is set to 2048, and the discriminator istrained for 20 epochs, running time of the whole trainingprocess is approximately 10 minutes.


D Text Samples

Table 2 presents examples of stegotexts generated by ourproposed method alongside non-steganographic texts, allbased on the same prompt, “それで”.
Each generatedtext embeds a 128-bit random secret message.
Followingthe approach of Ziegler et al.
[3], we ter minate the gen-eration process once the proposed method has completedembedding the message.