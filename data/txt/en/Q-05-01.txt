Low-Overhead Disambiguation for Generative LinguisticSteganography via Tokenization Consistency

Ruiyi Yan Yugo Murawaki



Kyoto University



ruiyi@nlp.ist.i.kyoto-u.ac.jp murawaki@i.kyoto-u.ac.jp



Abstract

Generative linguistic steganog raphy aims at embeddinginformation into natural language texts for covert transmis-sion.
However, in most tokenizer-based language modelapproaches, segmentation ambiguity during extraction canresult in errors or extraction failures.
Despite several ex-isting countermeasures (or disambiguation) that have beenproposed, none address this issue from the perspective oftokenization consistency.
Speciï¬cally, previous methodsexcessively modify candidate pools, compromising imper-ceptibility or embedding capacity.
To address it, we pro-pose a stepwise tokenization-veriï¬cation method whichprecisely removes error tokens for each step, ensuring100% tokenization consistency in the ï¬nal output.
Ex-perimental results demonstrate that our method surpassesbaseline approaches in text quality, imperceptibility, andanti-steganalysis capacity across various embedding rates.


1 Introduction

Linguistic steganography, a promising approach to safe-guarding information, involves concealing messages withintext.
Generative linguistic steganography (GLS)[1, 2, 3]has emerged as a dominant technique, enabling generatedsteganographic texts (referred to as stegotexts) across di-verse genres with consistent context, high ï¬‚uency, highnaturalness, high imperceptibility, and high embeddingcapacity, especially with advances in large language mod-els (LLMs)[4, 5].
However, in most existing GLS ap-proaches (except tokenization-free methods [6, 7]), thesender must detokenize stegotexts while the receiver mustretokenize them, leading to potential segmentation ambi-guity
[8, 9, 10, 11, 12].Several previous preï¬x-based methods [9, 10, 12] haveaddressed segmentation ambiguity, while the limitation ofSenderReceiverSecret message to be !embedded: 0111â€¦ â€¦Language modelCandidate poolCodewordquestion00questionnaire01â€¦ â€¦10â€¦ â€¦11Update stegotextâ€¦ â€¦Stegotext to be extracted: questionnairesâ€¦ â€¦Language modelCandidate poolCodewordquestion00questionnaire01â€¦ â€¦10â€¦ â€¦11Segmentation ambiguityCase 1Case 2Figure 1 Example for segmentation ambiguity of generatingtokens in 4-token candidate pools coded by block encoding [1],where the receiver ï¬nds 2 tokensâ€™ words matching the remainingstegotext â€˜questionnaire...â€™, respectively â€˜questionâ€™ and â€˜question-naireâ€™.
Thus, there are more than one extracting cases, only oneof which is true.them is over-preventively eliminating or merging tokensagainst extraction errors, at the expense of imperceptibility,embedding capacity, or eï¬ƒciency.
Motivated by achieving100% disambiguation with minimal negative impact (lowoverheads on various performances), we propose a pre-cise disambiguating approach based on tokenization con-sistency between the sender-receiver pair.
The key idea isthat the sender runs the tokenizer during stegotext genera-tion, pre-emptively ensuring that the receiver can replicatethe original tokens without ambiguity.
The main contribu-tions of our method are as follows:1.
We propose a stepwise tokenization-veriï¬cationmethod that ensures that the receiver obtains tokens identi-cal to those generated by the sender.
The receiver tokenizesthe raw stegotext into tokens, allowing the extraction pro-cess to operate directly on tokens rather than on raw text.2.
At each generation step, through verifying and remov-ing only those tokens that do cause tokenization inconsis-tencies, we aim to minimize disruption to the candidatepoolsâ€™ probability distribution.
As token candidates areassociated with codewords, multiple calls to the tokenizerare required.
Even if this seems ineï¬ƒcient, ours operateswith linear complexity, and still oï¬€ers some advantagesover the O(ğ‘›2)(at least) complexity of previous methods.3.
Experimental results show that, at medium and higherembedding-rate intervals, our method outperforms existingapproaches, achieving at least 11.29% lower perplexity,7.53% lower candidate-level KL divergence, and 8.07%lower detection accuracy by steganalysis.
They show lowoverheads on various performances of our method.


2 Preliminaries



2.1 Notation of Linguistic Steganography

Alice (the sender) wants to communicate a secret mes-sage ğ‘š âˆ¼ Unif({0, 1}ğ¿) with Bob (the receiver) by em-bedding it in a choice of natural language cover text ğ‘‡ğ‘ (astegotext).
Alice and Bob have agreed on an embeddingfunction ğ‘“ğ‘’ğ‘šğ‘and an extracting function ğ‘“ğ‘’ğ‘¥ğ‘¡that performsteganography.
Alice and Bob also have access to the exactsame language model, LMğ‘œ, which can be used during em-bedding and extraction.
These two functions are supposedto be invertible.
In other words, ğ‘“ğ‘’ğ‘šğ‘(LMğ‘œ, ğ‘š) = ğ‘‡ğ‘ ,ğ‘“ğ‘’ğ‘¥ğ‘¡(LMğ‘œ, ğ‘‡ğ‘ ) = ğ‘šâ€², and ğ‘šâ€²should be equal to ğ‘š.

2.2 Generative Linguistic Steganography

At the micro level, during token-by-token generation,we denote the sequence text as ğ‘†ğ‘’ğ‘ = {ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘–}ğ‘›ğ‘–=1,where ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘–represents the ğ‘–ğ‘¡ â„token in the ğ‘›-tokensentences.
To generate the next token (ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘›+1), thelanguage model predicts the candidate pool (CP) ofğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘›+1through ğ‘˜ historical tokens (if any) of ğ‘†ğ‘’ğ‘,where ğ‘ƒ(ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘›+1|ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘›âˆ’ğ‘˜+1, ..., ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘›) is the transi-tion probability.
The ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘›+1candidate pool is: CPğ‘œğ‘›+1={ğ‘1ğ‘›+1, ğ‘2ğ‘›+1, ..., ğ‘|ğ‘‰ |ğ‘›+1} with its corresponding probabilitydistribution: Pğ‘œğ‘›+1= {ğ‘1ğ‘›+1, ğ‘2ğ‘›+1, ..., ğ‘|ğ‘‰ |ğ‘›+1} where ğ‘‰ is thewhole vocabulary of LMğ‘œ, andÃ|ğ‘‰ |ğ‘—=1ğ‘ğ‘—ğ‘›+1= 1.GLS utilizes redundancy of candidate pools to achievesteganography.
Through further sampling (e.g. top-k)and encoding Pğ‘œğ‘›+1with Huï¬€man coding [2] or arithmeticcoding [3] and so on, a steganographic candidate poolCPğ‘ ğ‘›+1is obtained, with its probability distribution Pğ‘ ğ‘›+1.At the macro level, during embedding process, the lan-guage model in turn chooses a token in CPğ‘ ğ‘¡(ğ‘¡ = 1, 2, ...)until it encodes the whole secret message; during extractionprocess, the language model in turn chooses and extracts atoken in CPğ‘ ğ‘¡(ğ‘¡ = 1, 2, ...)
till the stegotextâ€™s end.


2.3 Segmentation Ambiguity of GLS

The stegotext generated by ğ‘“ğ‘’ğ‘šğ‘is essentially a sequencecomposed of tokens.
The sender must detokenize it us-ing a tokenizer into a stegotext before transmission.
Asshown in Figure 1, if the sender generates a token map-ping to â€œ questionâ€ and â€œnaireâ€, the sender needs to deto-kenize them into the text â€œquestionnaireâ€ before sendingit to Bob.
However, the issue is that common wordslike â€œ questionnaireâ€ often exist as independent tokensâ€œ questionâ€ in the modelâ€™s vocabulary as well.
As a re-sult, a single piece of text can correspond to two or evenmore diï¬€erent token representations.
Therefore, duringextraction ğ‘“ğ‘’ğ‘¥ğ‘¡(LMğ‘œ, ğ‘‡ğ‘ ), since both â€œ questionnaireâ€ andâ€œ questionâ€ exist in the candidate pool, Bob cannot deter-mine which token the sender embedded the message into.
This phenomenon is referred to as segmentation ambiguity.


2.4 Related Disambiguating Approaches

Recently, several solutions have emerged to address seg-mentation ambiguity in GLS.1) Basic Solution: Nozaki et al.
[9] proposed a simpledisambiguating approach, which removes tokens whosemapping subwords are preï¬xes of others during every gen-eration and extraction step.2) MWIS-based Solution:Yanet al.[10] considered theinï¬‚uence of removing candidate words on the probabilitydistributions and decided to process only if candidate-levelambiguity occurred.
Their solution identiï¬es the max-imum weight independent set (MWIS) in the candidatepool to reduce probability distortion.3)
SyncPool Solution: Qi et al.
[12] designed provablysecure disambiguating linguistic steganography based onambiguity pool grouping and synchronous sampling to ad-dress information loss and token synchronization issuesduring steganography, eliminating segmentation ambigu-ity without altering the distribution.


3 Methodology

The diï¬€erences between the previous methods and ourproposed method are outlined as follows:1.
In previous methods, the receiver performs recurrentSenderReceiverCandidate poolConsistency?tokenk1âœ”tokenk2âœ”tokenk3âœ”tokenk4âœ˜Secret message: !1011â€¦ â€¦Candidate pooltoken1token2â€¦ â€¦token|vocabulary|Language modelTop-k sampling (k=4)Candidate poolCodewordtokenk10tokenk210tokenk311Filtering & EncodingSelecting & Generatingtokenk2â€¦ â€¦Token list114232â€¦4Token-by-token generationDetokenizingTokenizingStegotextâ€œResearch on biology has â€¦â€
Token list114232â€¦4Token-by-token extractionSecret message: !1011â€¦ â€¦Secret message: !1011â€¦ â€¦Candidate poolConsistency?tokenk1âœ”tokenk2âœ”tokenk3âœ”tokenk4âœ˜Token list: !
[tokenk2, â€¦]Candidate pooltoken1token2â€¦ â€¦token|vocabulary|Language modelTop-k sampling (k=4)Candidate poolCodewordtokenk10tokenk210tokenk311Filtering & EncodingSelecting & Extracting10â€¦ â€¦Figure 2 Overview and procedures of generative linguistic steganography with our tokenization-veriï¬cation approach.
For somesimplicity in this example, Huï¬€man encoding [2] and top-4 sampling in each candidate pool is adopted.
Our stepwise tokenization-ver-iï¬cation step is implemented before steganographic encoding for both the sender and receiver.
Algorithm 1 Consistency Veriï¬cation for One TokenInput:ğ‘¡ğ‘œ:
Token to be veriï¬ed;ğ¿: Previously generated token list;Output:ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡: Tokenization consistency or not (Tr ue or False);1:
ğ¿ğ‘œâ† ğ¿.append(ğ‘¡ğ‘œ); /*
Token list to be veriï¬ed*/2: ğ‘‡ ğ‘’ğ‘¥ğ‘¡ â† Tokenizer.decode(ğ¿ğ‘œ);3: ğ¿â€²â†
Tokenizer.encode(ğ‘‡ ğ‘’ğ‘¥ğ‘¡);4: ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ â† (ğ¿ğ‘œ== ğ¿â€²);5: return ğ‘…ğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡preï¬x stripping to reconstruct the token sequence, whereasin our method, the receiver simply calls the tokenizer.2.
Previous methods are proposed because directly call-ing the tokenizer can result in segmentation mismatches.
Our method avoids segmentation mismatches by stepwisetokenization veriï¬cation on the senderâ€™s side.3. Candidate pool selection in previous methods isoverly pre-emptive, while in our method it remains pre-emptive but is more restrained.


3.1 Overall Steganographic System

Our proposed disambiguating method focuses on en-suring tokenization consistency between the sender andreceiver while keeping all processes on candidate poolsfully accessible for extraction.
As shown in Figure 2,the tokenization-consistency veriï¬cation step is placed be-tween the sampling and steganographic encoding steps.
Both the sender and receiver can verify whether each tokenin the candidate pool maintains tokenization consistency,allowing them to perform steganographic encoding on thesame ï¬ltered candidate pools.
This guarantees that thereceiver can accurately extract the secret messages.

3.2 Tokenization-Veriï¬cation Method

The core challenge of this method lies in identifyingwhether tokens in the candidate pool cause tokenization in-consistency.
To address this, we propose a straightforwardand lightweight approach, detailed in Algorithm 1.
Thealgorithm veriï¬es the tokenization consistency for a singletoken by ï¬rst appending the token ğ‘¡ğ‘œto the existing tokenlist ğ¿ to form ğ¿ğ‘œ(Line 1).
Next, ğ¿ğ‘œis detokenized intoa text string ğ‘‡ ğ‘’ğ‘¥ğ‘¡ using tokenizer.decode(), and then reto-kenized into a token list ğ¿â€²using tokenizer.encode()(Line2-3).
Finally, whether ğ¿ğ‘œis identical to ğ¿â€²is checked,returning a Boolean result (Line 4).The process simulates detokenization and retokenizationof the generated stegotext transmitted from the sender tothe receiver.
Tokens that cause tokenization inconsisten-cies are removed from the candidate pool, as they coulddisrupt the receiverâ€™s extraction process.
By eliminatingsuch problematic tokens, the approach ensures that boththe sender and receiver maintain identical token sequences,enabling consistent and reliable steganographic extraction.
Table 1 Comparison among disambiguating approaches, Basic [9], MWIS
[10], SyncPool
[12] and our tokenization-consistencyapproach in perplexity, KL divergences, steganalysis accuracy and running time (seconds) under various embedding-capacity intervals1.0 â‰¤ BPT < 1.5 1.5 â‰¤ BPT < 2.0 2.0 â‰¤ BPT < 2.5 2.5 â‰¤ BPT <
3.0 3.0 â‰¤ BPT < 3.5PPLâ†“ KLDâ†“ ACCâ†“
Timeâ†“
PPLâ†“ KLDâ†“
ACCâ†“
Timeâ†“
PPLâ†“ KLDâ†“
ACCâ†“
Timeâ†“
PPLâ†“ KLDâ†“
ACCâ†“
Timeâ†“
PPLâ†“ KLDâ†“
ACCâ†“ Timeâ†“Basic 5.780 0.823 0.943 1.265 9.098 0.867 0.761 0.886 13.799 0.945 0.703 0.704 21.363 0.985 0.621 0.600 31.554 1.012 0.708 0.558MWIS 4.662 0.590 0.855 2.930 6.758 0.567 0.648 1.747 9.218 0.506 0.635 0.835 12.963 0.453 0.583 0.731 18.169 0.432 0.750 0.810SyncPool 8.523 0.388 0.646 4.053 12.330 0.338 0.590 2.519 17.788 0.272 0.547 1.452 23.284 0.294 0.603 0.998 33.068 0.312 0.707 0.884Ours 4.847 0.593 0.926 3.317 6.692 0.546 0.830 2.322 9.203 0.539 0.741 1.558 12.281 0.413 0.491 0.895 16.813 0.362 0.583
0.8983.5 â‰¤
BPT < 4.0 4.0 â‰¤ BPT < 4.5 4.5 â‰¤ BPT < 5.0 5.0 â‰¤ BPT < 5.5 5.5 â‰¤ BPT <
6.0PPLâ†“ KLDâ†“
ACCâ†“
Timeâ†“
PPLâ†“ KLDâ†“
ACCâ†“
Timeâ†“
PPLâ†“ KLDâ†“
ACCâ†“
Timeâ†“
PPLâ†“ KLDâ†“
ACCâ†“
Timeâ†“
PPLâ†“ KLDâ†“
ACCâ†“ Timeâ†“Basic 44.782 1.031 0.754 0.545 62.667 1.032 0.777 0.613 94.422 1.054 0.920 1.320 131.430 1.056 0.935 1.907 183.049 1.063 0.957 3.384MWIS 24.460 0.384 0.690 1.426 33.580 0.360 0.792 2.123 47.849 0.353 0.871 3.171 66.997 0.367 0.806 2.952
âˆ’ âˆ’ âˆ’ âˆ’SyncPool âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’Ours 22.824 0.318 0.521 1.075 30.592 0.259 0.669 1.555 42.278 0.189 0.759 2.375 57.104 0.172 0.746 2.595 76.858 0.158 0.726 2.814

4 Experiments and Discussion



4.1 Experimental Setup

To ensure fairness, all the following GLS experimentsusing various disambiguating approaches are conductedwith the same language model, llm-jp-3-1.8b1ï¼‰[13], em-bedding a random 128-bit secret message, i.e. ğ‘š âˆ¼Unif({0, 1}128).
All methods employ arithmetic cod-ing
[3].
We compare our method with three existing dis-ambiguating approaches â€• Basic
[9], MWIS
[10], andSyncPool
[12] â€• used as baselines.
To evaluate perfor-mance under varying embedding capacities, experimentsare conducted with diï¬€erent top-k sampling values (k âˆˆ {4,8, 16, 32, 48, 64, 128, 256, 512, 1024, 2048, 4096}).
Foreach top-k value and for each disambiguating method, 500samples are generated and collected for analysis.


4.2 Primary Metrics

Bits per token (BPT) is a fundamental metric in linguis-tic steganography, measuring the embedding capacity.
Per-plexity (PPL) assesses the quality and ï¬‚uency of the gen-erated text.
KL divergence (KLD) between modiï¬ed andoriginal candidate pools quantiï¬es statistical disparities, re-ï¬‚ecting imperceptibility.
Steganalysis accuracy (ACC) isevaluated using a discriminator ï¬ne-tuned from bert-base-Japanese2ï¼‰, with further details provided in Appendix C.Finally, the running time (Time, in seconds) to embed asecret message indicates the steganographic eï¬ƒciency.


4.3 Results

For each disambiguating method, experimental dataobtained under various top-k values are grouped into1ï¼‰
Access: https://huggingface.co/llm-jp/llm-jp-3-1.8b2ï¼‰
Access: https://github.com/cl-tohoku/bert-japaneseembedding-capacity intervals (1.0â‰¤BPT<6.0).
Ta-ble 1 shows the average performance across these intervalsfor each approach.
Note that when the sample size in anygroup is 20 or fewer, the data is considered insuï¬ƒcient andmarked as â€œâˆ’â€ in Table 1.For experimental groups with suï¬ƒcient data inhigh embedding-capacity intervals (BPT â‰¥ 3.5), ourtokenization-consistency approach consistently achievesthe best performance in PPL, KLD, and ACC.
Althoughthe Basic approach [9] generally demonstrates the highesteï¬ƒciency due to its lowest Time, our approach remainscompetitive and even surpasses Basic when BPT exceeds5.5.
This is because, for smaller top-k candidate pools,the detokenization and retokenization processes for eachtoken in candidate pools could make our method moretime-consuming than the most eï¬ƒcient baseline.
How-ever, when top-k candidate pools are large, our methodâ€™slinear time complexity becomes more eï¬ƒcient comparedto the O(ğ‘›2)(at least) complexity of other methods.
Overall, as shown in Table 1, our method outperformsthe baselines from moderate to high embedding-capacityintervals (2.0 â‰¤ BPT < 6.0).
When compared to the bestbaseline method for each metric in each interval, our ap-proach achieves an average reduction of 11.29% in PPL,7.53% in KLD, and 8.07% in ACC.


5 Conclusion

This paper addresses segmentation ambiguity in gen-erative linguistic steganography from the perspective oftokenization consistency, with the goal of minimizing thenegative impact of disambiguation.
Experiments demon-strate the advantages of our method over baselines acrossvarious metrics.
Furthermore, our proposed disambiguat-ing approach oï¬€ers generalizability to facilitate the broadï¬eld of reliable linguistic steganography.



Acknowledgment

This work was supported by JST SPRING, Grant Num-ber JPMJSP2110.

References


[1] Tina Fang, Martin Jaggi, and Katerina Argyraki. Gen-erating steganographic text with LSTMs. In Allyson Et-tinger, Spandana Gella, Matthieu Labeau, Cecilia Oves-dotter Alm, Marine Carpuat, and Mark Dredze, editors,Proceedings of ACL 2017, Student Research Work-shop, pp. 100â€“106, Vancouver, Canada, July 2017. Asso-ciation for Computational Linguistics.
[2] Zhong-Liang Yang, Xiao-Qing Guo, Zi-Ming Chen, Yong-Feng Huang, and Yu-Jin Zhang. Rnn-stega: Linguisticsteganography based on recurrent neural networks. IEEETransactions on Information Forensics and Security,Vol. 14, No. 5, pp. 1280â€“1295, 2019.
[3] Zachary Ziegler, Yuntian Deng, and Alexander Rush. Neu-ral linguistic steganography. In Kentaro Inui, Jing Jiang,Vincent Ng, and Xiaojun Wan, editors, Proceedings ofthe 2019 Conference on Empirical Methods in Nat-ural Language Processing and the 9th InternationalJoint Conference on Natural Language Processing(EMNLP-IJCNLP), pp. 1210â€“1215, Hong Kong, China,November 2019. Association for Computational Linguis-tics.
[4] Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, TimothÂ´ee Lacroix, Bap-tiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,et al. Llama: Open and eï¬ƒcient foundation language mod-els. arXiv preprint arXiv:2302.13971, 2023.
[5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, et al. Qwen technical report. arXiv preprintarXiv:2309.16609, 2023.
[6] Lingyun Xiang, Shuanghui Yang, Yuhang Liu, Qian Li,and Chengzhang Zhu. Novel linguistic steganographybased on character-level text generation. Mathematics,Vol. 8, No. 9, 2020.
[7] Ruiyi Yan, Tianjun Song, and Yating Yang. Token-free: A tokenization-free generative linguistic stegano-graphic approach with enhanced imperceptibility. Au-thorea Preprints, 2023.
[8] Honai Ueoka, Yugo Murawaki, and Sadao Kurohashi.Frustratingly easy edit-based linguistic steganography witha masked language model. In Kristina Toutanova, AnnaRumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Belt-agy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,and Yichao Zhou, editors, Proceedings of the 2021Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pp. 5486â€“5492, Online, June2021. Association for Computational Linguistics.
[9] Jumon Nozaki and Yugo Murawaki. Addressing segmenta-tion ambiguity in neural linguistic steganography. In YulanHe, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang,editors, Proceedings of the 2nd Conference of theAsia-Paciï¬c Chapter of the Association for Com-putational Linguistics and the 12th InternationalJoint Conference on Natural Language Processing(Volume 2: Short Papers), pp. 109â€“116, Online only,November 2022. Association for Computational Linguis-tics.
[10] Ruiyi Yan, Yating Yang, and Tian Song. A secure and dis-ambiguating approach for generative linguistic steganog-raphy. IEEE Signal Processing Letters, Vol. 30, pp.1047â€“1051, 2023.
[11] Ruiyi Yan, Tianjun Song, and Yating Yang. Segfree:Segmentation-free generative linguistic steganographicapproach for unsegmented languages. AuthoreaPreprints, 2023.
[12] Yuang Qi, Kejiang Chen, Kai Zeng, Weiming Zhang,and Nenghai Yu. Provably secure disambiguat-ing neural linguistic steganography. arXiv preprintarXiv:2403.17524, 2024.
[13] Akiko Aizawa, Eiji Aramaki, Bowen Chen, Fei Cheng, Hi-royuki Deguchi, Rintaro Enomoto, Kazuki Fujii, KensukeFukumoto, Takuya Fukushima, Namgi Han, et al. Llm-jp: A cross-organizational project for the research anddevelopment of fully open japanese llms. arXiv preprintarXiv:2407.03963, 2024.
[14] Falcon Dai and Zheng Cai. Towards near-imperceptiblesteganographic text. In Anna Korhonen, David Traum,and LluÂ´Ä±s M`arquez, editors, Proceedings of the 57thAnnual Meeting of the Association for Computa-tional Linguistics, pp. 4303â€“4308, Florence, Italy, July2019. Association for Computational Linguistics.
[15] Jiaming Shen, Heng Ji, and Jiawei Han. Near-imperceptible neural linguistic steganography via self-adjusting arithmetic coding. In Bonnie Webber, TrevorCohn, Yulan He, and Yang Liu, editors, Proceedings ofthe 2020 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pp. 303â€“313,Online, November 2020. Association for ComputationalLinguistics.
[16] A.A. Fedotov, P. Harremoes, and F. Topsoe. Reï¬nementsof pinskerâ€™s inequality. IEEE Transactions on Informa-tion Theory, Vol. 49, No. 6, pp. 1491â€“1498, 2003.
[17] Diederik P. Kingma and Jimmy Ba. Adam: A method forstochastic optimization, 2017.



A Imperceptibility of GLS

Following the previous formulation [14, 15]
, statisti-cal imperceptibility refers to the similarity between thetrue language model LMğ‘¡in the monitored channel andLMğ‘ which is the language model LMğ‘œintegrated withsteganographic algorithms.
Speciï¬cally, the total variationdistance (TVD) is used to measure statistical impercep-tibility.
Consider the TVD between LMğ‘¡and LMğ‘ , i.e.ğ‘‘(LMğ‘¡, LMğ‘ ), by triangle inequality:ğ‘‘(LMğ‘¡, LMğ‘ ) â‰¤
ğ‘‘(LMğ‘¡, LMğ‘œ), ğ‘‘ (LMğ‘œ, LMğ‘ )(1)As ğ‘‘(LMğ‘¡, LMğ‘œ) is a criterion to measure the original lan-guage model, which is limited by the research on languagemodels.
Thus, ğ‘‘(LMğ‘œ, LMğ‘ ) is the main focus of GLStechniques.
According to Pinskerî¢Ÿs inequality [16] and additivity ofKL divergence, ğ‘‘ (LMğ‘œ, LMğ‘ ) can be further decomposedin each step, that is3ï¼‰:ğ‘‘(LMğ‘œ, LMğ‘ ) â‰¤vtln 22âˆÃ•ğ‘¡=1ğ·ğ¾ ğ¿(Pğ‘œğ‘¡||Pğ‘ ğ‘¡)(2)where Pğ‘œğ‘¡is the original probability distribution at ğ‘¡ğ‘¡ â„step,and Pğ‘ ğ‘¡is transformed from Pğ‘œğ‘¡via sampling and encoding.
Hence, GLS could aim to minimize ğ·ğ¾ ğ¿(Pğ‘œğ‘¡||Pğ‘ ğ‘¡), in orderto obtain relative near-imperceptibility.


B Computational Resources

All experiments are implemented in Python 3.12.7 withTorch 2.5.0, running on a 2.0 GHz CPU and accelerated byusing 8 Ã— NVIDIA RTX A6000 GPUs.


C Details of Steganalysis

Positive samples are collected from stegotexts gener-ated using various top-k samplings, while negative sam-ples are sourced from non-steganographic texts.
All textsare generated from the same prompt, â€œãã‚Œã§â€.
Dur-ing the training phase, both positive and negative samplesconsist of 19,200 instances each.
For testing, 4,800 un-trained positive samples are used, categorized into diï¬€erentembedding-capacity intervals as shown in Table 1.
In eachembedding-capacity interval and for each disambiguatingapproach, only stegotexts with a sample size greater than3ï¼‰
Some derivation is omitted here, as details are veriï¬ed in [14, 15,16].Table 2 Examples generated texts using the prompt â€˜ãã‚Œã§â€™by llm-jp-3-1.8bA stegotext generated by our proposed methodãã‚Œã§ã€ã“ã®çŠ¶æ³ãŒèµ·ãã¦ã—ã¾ã†æ™‚ã«æœ€åˆã«è€ƒãˆã‚‹ã¹ãã“ã¨ã¯ã€Œãƒªã‚¹ã‚¯ã®æ´—ã„å‡ºã—ã€ã§ã™ã€‚ãƒªã‚¹ã‚¯ãŒæœ¬å½“ã«æƒ³å®šã—ãŸçŠ¶æ³ã®ç¯„å›²å†…ã§èµ·ã(Perplexity = 27.778; Bits per token = 4.414)ãã‚Œã§ã„ã¦ã€ã©ã“ã‹é–‹æ”¾çš„ã§è¯ã‚„ã‹ãªå½©ã®ã‚ã‚‹å°ç‰©ãŸã¡ã¯ã€æ™‚ä»£ã¨å…±ã«æµè¡Œã‚Šå»ƒã‚Šã¯ã‚ã‚‹ã‚‚ã®ã®ã€æ±ºã—ã¦è‰²ã‚ã›ãªã„(Perplexity = 49.150; Bits per token = 5.333)A non-steganographic text generatedãã‚Œã§ã€Œæ¬¡ã«èª­ã‚€æœ¬ãŒç„¡ã„å•é¡Œã€ã€Œèª­ã‚“ã§ã„ã‚‹æœ¬ã‚’äººã«è–¦ã‚ã‚‹æ–¹æ³•ãŒç„¡ã„å•é¡Œã€ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€(Perplexity
=
16.653)ãã‚Œã§ä»Šã¯ã€ç§ãŒå¥½ããªä½œå®¶ã•ã‚“ã®ä½œå“ã®ä¸€éƒ¨ã‚’å€Ÿã‚Šã¦æãå‡ºã™å½¢ã§ã”ä¸€ç·’ã—ã¦ã„ã¾ã™ã€‚ä½œå“ã®è§£é‡ˆã‚’ä¼ãˆã‚‹(Perplexity = 56.774)20 are included in the tests; otherwise, â€œâˆ’â€ is marked toindicate insuï¬ƒcient data.
Given the signiï¬cant variation in the lengths of positivesamples, we adjust the negative samples to vary between20 and 128 tokens to ensure that the trained discriminatoris not sensitive to text length.
Additionally, all texts arepadded or truncated to 128 tokens, so that positive samplescannot be distinguished as steganographic based solely ontheir length.
For ï¬ne-tuning the BERT model, we useAdam [17] as the optimizer with a learning rate of 5Ã—10âˆ’5.The batch size is set to 2048, and the discriminator istrained for 20 epochs, running time of the whole trainingprocess is approximately 10 minutes.


D Text Samples

Table 2 presents examples of stegotexts generated by ourproposed method alongside non-steganographic texts, allbased on the same prompt, â€œãã‚Œã§â€.
Each generatedtext embeds a 128-bit random secret message.
Followingthe approach of Ziegler et al.
[3], we ter minate the gen-eration process once the proposed method has completedembedding the message.