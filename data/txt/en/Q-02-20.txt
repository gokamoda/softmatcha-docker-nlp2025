Evaluating Robustness of LLMs to Numerical Variations inMathematical Reasoning

Yuli Yang

1

â€ƒHiroaki Yamada

1

â€ƒTakenobu Tokunaga

11

Institute of Science Tokyo



{yang.y.aw@m, yamada@c, take@c}.titech.ac.jp



Abstract

Evaluating an LLMâ€™s robustness against numer ical per-turbation is a good way to know if the LLM actually per-forms reasoning or just replicates patterns learned.
Wepropose a novel method to augment math word problems(MWPs), producing numerical variations at a large scaleutilizing templates.
We also propose an automated errorclassiï¬cation framework for scalable error analysis, distin-guishing calculation errors from reasoning errors.
Our ex-periments using the methods show LLMs are weak againstnumerical variations, suggesting they are not fully capableof generating valid reasoning steps, often failing in arith-metic operations.


1 Introduction

Recent LLMs
[1, 2, 3, 4] have repor ted high accuracyrates on mathematical reasoning benchmarks, includingGSM8K and MATH [5, 6].
However, a natural concernis that the models just follow surface patterns observed intheir pretraining data rather than performing mathematicalreasoning [7, 8, 9, 10, 11].Perturbing superï¬cial elements like names of individu-als or speciï¬c numbers does not change how the problemshould be solved.
If an LLM can perform reasoning insolving a math question, it should give correct answerswith similar reasoning steps for both the question and itsperturbed one.
Recent studies [12, 13, 14, 15] evaluatedmodelsâ€™ robustness against the perturbations based on thishypothesis.
These studies have the following limitations: a) the sizeof the introduced variations was limited, b) they did notdiscuss ranges of numerical values such as digit sizes, andc) they did not distinguish reasoning er rors and computa-tional errors and could not explain the source of errors.
To address the limitations, we propose a scalable methodto augment a math word problem (MWP) dataset by chang-ing numerical values based on template questions.
To an-alyze the impact of digit sizes on modelsâ€™ mathematicalreasoning, we controlled the range of the replaced val-ues and generated two distinct subsets, one with questionscontaining a small number of digits (1-99) and one withquestions containing a large number of digits (1-9,999).Using our method, we constructed a new dataset, GSM-ALT, generating 2,000 variants for each original questionfrom GSM8K.
Moreover, we propose a novel frameworkfor automated error analysis to identify whether a source ofincorrect prediction stems from errors in logical reasoningor numerical calculation.


2 Related Work

Despite strong performance on math benchmarks, re-searchers are questioning whether current benchmarks canadequately evaluate reasoning abilities and language mod-els demonstrate them.
Levy [7] expanded questions by adding non-essentialcontents, showing that modelsâ€™ performance decreaseswhen the number of tokens in a problem increases.
Plan-Bench [8, 9] is a benchmark to evaluate planning and rea-soning capabilities.
Their ï¬ndings suggest that even state-of-the-art models still struggle with this.
Srivastava
[12]functionalized the math questions to create a dynamicdataset, providing a robust evaluation metric against poten-tial data leakage to modelsâ€™ pretraining.
Jiang
[10] demon-strated that the modelsâ€™ high accuracy depends on a speciï¬ctoken bias, and the modelsâ€™ reasoning capability dependson recognizing certain superï¬cial patterns.
Berglund [16]and Guo
[11] gave the answer to the questions and reversedto infer one of the variables to constr uct reversal versionsof the original questions.
They showed that the currentâ€• 851 â€•This work is licensed by the author(s) under CC BY 4.0models performed poorly on the reversal ones.


3 Method



3.1 Question Template Development

To develop a new dataset to assess the robustness ofnumerical variations, we manually generate new variantsbased on templates (Figure 1) composed from the originalquestions of an existing dataset.
A question from an existing dataset (e.g., GSM8K) hastuples of (question ğ‘„, solution ğ‘†).
ğ‘„ is a natural languagetext describing a question to be solved.
ğ‘† contains theprocess ğ‘ƒ and the ï¬nal answer A. ğ‘ƒ shows a gold processfor solving the question ğ‘„ step by step, including equations.
A stores a numerical value as a gold outcome from the ğ‘ƒ.Given (ğ‘„, ğ‘†), we ï¬rst replace all the numerical values inthe ğ‘„ with variables to get ğ‘„ğ‘ğ‘ğ‘ , which is the abstractedğ‘„. We apply the same operation to ğ‘† and get ğ‘†ğ‘ğ‘ğ‘ . Wekeep variables consistent between ğ‘„ğ‘ğ‘ğ‘ and ğ‘†ğ‘ğ‘ğ‘ . ğ‘†ğ‘ğ‘ğ‘ contains ğ‘ƒğ‘ğ‘ğ‘ and Ağ‘ğ‘ğ‘ , representing the abstracted ğ‘ƒ andA. The ğ‘„ğ‘ğ‘ğ‘ and ğ‘†ğ‘ğ‘ğ‘ constitute a question template ğ‘‡ .

3.2 Variant Set Generation

Given a template ğ‘‡ of an original question, we generatevariants by replacing the variables in ğ‘‡ with random values.ğ‘¡ğ‘–denotes a variant generated from ğ‘‡ , consisting of questionğ‘„ğ‘–, solution ğ‘†ğ‘–. ğ‘†ğ‘–contains the process ğ‘ƒğ‘–and ï¬nal answerğ´ğ‘–. To ensure the variants are valid, the replaced valuesneed to satisfy some constraints (Figure1).
For example,an answer should be positive and whole when it representsthe number of objects.
Intermediate values appearing inthe process ğ‘ƒğ‘–also need to satisfy the constraints as well.
We manually deï¬ne constraints for each template.
We onlyaccept a variant if it satisï¬es the constraints.
If models do only superï¬cial pattern-based inference anddo not conduct reasoning, they perform poorly in solvingquestions containing numbers that are rare in their training,such as large digit numbers.
To examine this hypothesis, foreach question template, we controlled the replaced valueswithin two diï¬€erent ranges and subsequently resulted intwo diï¬€erent variant sets: 1-99 (namely, the Easy variantset) and 1-9,999 (namely, the Hard variant set).


4 Experimental Settings

We use GSM8K as the base dataset for our experiment.
GSM8K consists of MWPs for primary and secondaryschool students and involves only the four basic arithmeticoperations.
We randomly sampled 92 questions from theGSM8K training set, from which we manually created 92question
templates1ï¼‰.
Given the templates, we generated1,000 hard variants and 1,000 easy variants for each tem-plate.
As a result, our new dataset GSM-ALT consistsof the Hard and Easy variant set, each containing 92,000variants.
We use accuracy as a pr imary evaluation metric.
For theoriginal instances from the base dataset (original GSM8K),we use a standard accuracy.
For generated variants fromour dataset, we ï¬rst calculate the accuracy for each templatevariant set containing 1,000 variants, and then we averagethem over all 92 templates.
The target models to be evaluated include generic mod-els (Llama-3-8b-Instruct, Llama-3.1-8b-Instruct, Llama-3.1-70b-Instruct, Mistral-7b-Instruct-v0.3) and math mod-els that were ï¬ne-tuned on mathematical contents(Deepseekmath-7b-rl, Wizardmath-7b-v1.1)Regarding the generation settings, we used greedy searchto maximize the reproducibility and stability of results.
Tominimize the inï¬‚uence of few-shot examples while ensur-ing that the model can perform mathematical reasoning, weadopted the zero-shot Chain-of-Thought (CoT) promptingfor solution generation and extracted the ï¬nal answer in thesame way as Kojima [17] for generic models.
As for mathmodels, we adopted the speciï¬cally designed prompts,which are recommended on their Web pages.
The promptsused in the experiment can be found in Appendix C.

5 Results

Table 1 shows the results of each modelâ€™s accuracy eval-uated on the original GSM8K and our GSM-ALT.
Thelowest scores are highlighted in boldface.
GSM-ALT re-sults show scores from the Easy variant set and the Hardvariant set.
All models showed a signiï¬cant performancedrop in GSM-ALT from the base GSM8K.
The drop wasobserved in both the Easy and the Hard variant sets.
Eventhe two math-specialized models, especially Wizardmath-1ï¼‰
We initially created 250 templates but the number of possiblevariants is limited in some of the templates, so we removed thosetemplates to ensure there is no duplicated variantâ€• 852 â€•This work is licensed by the author(s) under CC BY 4.0Fabian is shopping at a nearby supermarket.
He wants to buy 5 kg of apples and 3 packs of sugar.
One kilogram of apples costs $2, and one pack of sugar is $1 cheaper than one kilogram of apples.
How much Fabian needs to pay for the items he wants to buy ?
Final Answer ğ‘¨: 13Process ğ‘·:
The apples cost Fabian 5 kg * $2/kg = $<<5*2=10>>10.One pack of sugar costs $2 - $1 = $<<2-1=1>>1.So, Fabian will pay $1/pack * 3 = $<<1*3=3>>3 for sugar.
In total, Fabian needs to pay $10 + $3 = $<<10+3=13>>13.OriginalTemplateFabian is shopping at a nearby supermarket.
He wants to buy x kg of apples and y packs of sugar.
One kilogram of apples costs $z, and one pack of sugar is $p cheaper than one kilogram of apples.
How much Fabian needs to pay for the items he wants to buy ?
Process ğ‘·ğ’‚ğ’ƒğ’”:
The apples cost Fabian x kg * $z/kg = $(x*z).One pack of sugar costs $z - $p = $(z-p).So, Fabian will pay $(z-p)/pack *
y = $((z-p)*y) for sugar.
In total, Fabian needs to pay $(x*z)
+ $((z-p)*y) = $(x*z + (z-p)*y).Final Answer ğ‘¨ğ’‚ğ’ƒğ’”: x*z + (z-p)*yzâ€“ p > 0Constraints:
In this question template, we have the constraint that the price of one pack of sugar should be a positive number, thusSolution ğ‘ºQuestion ğ‘¸Question ğ‘¸ğ’‚ğ’ƒğ’”Solution ğ‘ºğ’‚ğ’ƒğ’”Figure 1 Example of Question Template DevelopmentTable 1
Accuracy scoresModelsGSM8K GSM-ALTBase Easy HardLlama-3-8b-Instruct 0.840 0.507 0.156Llama-3.1-8b-Instruct 0.880 0.604 0.193Llama-3.1-70b-Instruct
0.978 0.819 0.355Mistral-7b-Instruct-v0.3 0.587 0.238 0.104Deepseek-math-7b-rl 0.957 0.706 0.307Wizardmath-7b-v1.1 0.891 0.489 0.2237b-v1.1, showed lower scores by more than 0.4 on the Easyand more than 0.6 on the Hard.
This result shows that numerical variations always de-grade performance in both the Hard and the Easy variantsets.
The fact that the Easy variant set degrades the per-formance indicates that the models are weak even againstthe numbers whose range is similar to the base GSM8K.Moreover, we found clearer score drops from the GSM8Kscores in the Hard variant set than in the Easy variantset, suggesting the computational diï¬ƒculty aï¬€ects modelsâ€™reasoning.


6 Error Analysis on Solutions

To identify the source of errors, we classify errors intotwo types: calculation errors and reasoning errors.
If anincorrect solution only contains failures in calculations, wecall it a calculation error.
If an incorrect solution containsincorrect reasoning steps, we label it a reasoning errorregardless of its incorrect calculations.
As GSM-ALT will be larger than its original dataset,manually checking each generated solution is not practical,and thus, we propose a novel framework that automaticallyclassiï¬es errors into calculation or reasoning errors.


6.1 Error Analysis Framework

To classify errors, we ï¬rst transform a predicted solu-tionË†ğ‘†ğ‘–into its abstracted formË†ğ‘†ğ‘–ğ‘ğ‘ğ‘ , which contains theabstractedË†ğ‘ƒğ‘–ğ‘ğ‘ğ‘ andË†ğ´ğ‘–ğ‘ğ‘ğ‘ .
IfË†ğ‘†ğ‘–is incorrect because ofa reasoning error, its transformedË†ğ‘ƒğ‘–ğ‘ğ‘ğ‘ should contain areasoning error resulting in incorrectË†ğ´ğ‘–ğ‘ğ‘ğ‘ . IfË†ğ‘†ğ‘–containsa calculation error, but its reasoning steps are correct,Ë†ğ‘ƒğ‘–ğ‘ğ‘ğ‘ andË†ğ´ğ‘–ğ‘ğ‘ğ‘ should be correct.
Thus, checking ifË†ğ´ğ‘–ğ‘ğ‘ğ‘ iscorrect should give a proxy to determine the sources oferrors.
In our framework, an LLM transforms aË†ğ‘†ğ‘–into theË†ğ‘†ğ‘–ğ‘ğ‘ğ‘ ,as shown in Figure 2.
Then, we can automatically check ifË†ğ´ğ‘–ğ‘ğ‘ğ‘ is correct by comparing it with its gold answer ğ´ğ‘ğ‘ğ‘ from our templates.
An input to the LLM is a modelâ€™s pre-dicted solutionË†ğ‘†ğ‘–, its question ğ‘„ğ‘–, and its abstracted ques-tion ğ‘„ğ‘ğ‘ğ‘ available from our templates.
We auxiliarylyinput the ğ‘„ğ‘ğ‘ğ‘ guiding the LLM to use variables consis-tently, inspired by Gaur [18].
An output from the LLM isan abstracted solutionË†ğ‘†ğ‘–ğ‘ğ‘ğ‘ .
We show our prompt for thisframework in Appendix D. We employ Qwen2-math-72b-instruct [19] for this transformation.
We manually checkedthe outputs and conï¬rmed the LLM could obtain the ab-stracted solutions at 90% success rate on average in ourpreliminary experiment.


6.2 Results of Error Analysis

Table 2 shows the results of er ror classiï¬cation by ourframework.
Values in the table indicate the proportion ofâ€• 853
â€•This work is licensed by the author(s) under CC BY 4.0TemplateBuy X pens of $Y.How much in total?QuestionSolutionEach pen costs $Y.So, total expense is $Y*X.Y*XProcess ğ‘ƒ!"#Answer ğ´!"#Buy 4 pens of $3.How much in total?QuestionVariantLLMPredicted solutionSolutionEach pen costs $3.So, total expense is $3*4=$12.12Process ğ‘ƒ#$Answer ğ´$$Abstracted solutionEach pen costs $Y.So, total expense is $Y*X.Y*XProcess ğ‘ƒ#!"#$Answer ğ´$!"#$Match?ReasoningerrorCalculationerrorYesğ‘†"!ğ‘†""#$!ğ‘„!ğ‘„"#$ğ‘†"#$Figure 2 Error classiï¬cation frameworkTable 2 Error rate per error type and variant setBase set Easy variant set Hard variant setcalculation err.
reasoning err.
calculation err.
reasoning err.
calculation err.
reasoning err.
Llama-3-8b-Instruct .033 (20.0%) .130
(80.0%) .279
(56.6%) .214
(43.4%) .573
(67.9%) .271
(32.1%)Llama-3.1-8b-Instruct .033 (27.3%) .087
(72.7%) .252
(63.6%) .144
(36.4%) .601
(74.5%) .206
(25.5%)Llama-3.1-70b-Instruct .000 (00.0%) .022
(100.0%) .125
(69.1%) .056
(30.9%) .516 (80.0%) .129
(20.0%)Mistral-7b-Instruct-v0.3 .098
(23.7%) .315 (76.3%) .385
(50.5%) .377 (49.5%) .477
(53.2%) .419
(46.8%)Deepseek-math-7b-rl .011 (25.0%) .033
(75.0%) .217
(73.8%) .077
(26.2%) .529
(76.4%) .163
(23.6%)Wizardmath-7b-v1.1 .043 (40.0%) .065
(60.0%) .383
(75.0%) .128
(25.0%) .586
(75.4%) .191
(24.6%)Macro avg.
.036
(24.8%) .109
(75.2%) .274
(62.3%) .166
(37.7%) .547 (70.4%) .230
(29.6%)solutions classiï¬ed as calculation errors or reasoning errorsout of all solutions predicted by the models.
Values inparentheses indicate the proportion of solutions classiï¬edas calculation errors or reasoning errors out of incorrectsolutions.
In the Base set, the majority of incorrect solutions weredue to reasoning errors, while they changed to calculationerrors in the Easy and Hard variant sets.
This trend wasespecially evident in the Hard variant set, and more than70% were because of calculation errors.
This result sug-gests that the limited capability of arithmetic calculationis indeed a major issue of LLMs in solving mathematicalproblems rather than the reasoning capability of generat-ing a valid process of solving steps especially when thenumerical values in the questions are large.
Looking at the reasoning errors, all the models got moreerrors in both the Easy and Hard variant sets than the baseset.
The same as calculation errors, the trend was evident inthe Hard variant set.
This result suggests that variants alsointroduce harmful changes in reasoning steps in addition tocomplex calculations, which result in incorrect solutions.
Moreover, variants with larger digit sizes are more likelyto introduce errors in reasoning steps.


7 Conclusion

We proposed a novel method to augment MWP datasets,which produces a dataset for evaluating LLMsâ€™ robustnessagainst numerical variations at a reliable scale.
Usingour templates, anyone can easily generate thousands ofvariants from one original question in the GSM8K, whichwas not possible with any preceding proposals.
We alsoproposed an automated error classiï¬cation framework forscalable error analysis, distinguishing calculation errorsfrom reasoning errors.
Using the methods, we empirically showed that the sixLLMs we tested were weak against numerical variations,especially when the numerical values were large.
Thisï¬nding is consistent with previous studies [12, 13, 14, 15],but we conï¬rm it with more variants.
Our error analysisuniquely identiï¬ed that calculation errors contributed toa substantial proportion of incorrect solutions, suggest-ing LLMsâ€™ incapability of arithmetic operations is themain source of limited capabilities in math word problems.
Moreover, we found that LLMs still fail in their reasoningsteps, especially when they encounter variants with largernumerical values.
Given our ï¬ndings, it is still hard to saythat current LLMs are robust against numerical variations.â€• 854 â€•This work is licensed by the author(s) under CC BY 4.0



Acknowledgement

This work was partly supported by JST, PRESTO GrantNumber JPMJPR236B, Japan.

References


[1] Josh Achiam, Steven Adler, Sandhini Agarwal, LamaAhmad, Ilge Akkaya, Florencia Leoni Aleman, DiogoAlmeida, Janko Altenschmidt, Sam Altman, ShyamalAnadkat, et al. Gpt-4 technical report. arXiv preprintarXiv:2303.08774, 2023.
[2] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,et al. The llama 3 herd of models. arXiv preprintarXiv:2407.21783, 2024.
[3] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalk-wyk, Andrew M Dai, Anja Hauth, Katie Millican, et al.Gemini: a family of highly capable multimodal models.arXiv preprint arXiv:2312.11805, 2023.
[4] Gemma Team, Morgane Riviere, Shreya Pathak,Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju,LÂ´eonard Hussenot, Thomas Mesnard, Bobak Shahriari,Alexandre RamÂ´e, et al. Gemma 2: Improving openlanguage models at a practical size. arXiv preprintarXiv:2408.00118, 2024.
[5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, MarkChen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christo-pher Hesse, and John Schulman. Training veriï¬ers to solvemath word problems, 2021.
[6] Dan Hendrycks, Collin Burns, Saurav Kadavath, AkulArora, Steven Basart, Eric Tang, Dawn Song, and JacobSteinhardt. Measuring mathematical problem solving withthe math dataset. arXiv preprint arXiv:2103.03874,2021.
[7] Mosh Levy, Alon Jacoby, and Yoav Goldberg. Same task,more tokens: the impact of input length on the reason-ing performance of large language models. In Lun-WeiKu, Andre Martins, and Vivek Sr ikumar, editors, Pro-ceedings of the 62nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pp. 15339â€“15353, Bangkok, Thailand,August 2024. Association for Computational Linguistics.
[8] Karthik Valmeekam, Matthew Marquez, Alberto Olmo,Sarath Sreedharan, and Subbarao Kambhampati. Plan-bench: An extensible benchmark for evaluating large lan-guage models on planning and reasoning about change.Advances in Neural Information Processing Sys-tems, Vol. 36, , 2024.
[9] Karthik Valmeekam, Kaya Stechly, and Subbarao Kamb-hampati. Llms still canâ€™t plan; can lrms? a preliminaryevaluation of openaiâ€™s o1 on planbench, 2024.
[10] Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, XiaomengWang, Tanwi Mallick, Weijie J Su, Camillo Jose Taylor,and Dan Roth. A peek into token bias: Large languagemodels are not yet genuine reasoners. In Yaser Al-Onaizan,Mohit Bansal, and Yun-Nung Chen, editors, Proceedingsof the 2024 Conference on Empirical Methods inNatural Language Pro cessing, pp. 4722â€“4756, Miami,Florida, USA, November 2024. Association for Computa-tional Linguistics.
[11] Pei Guo, WangJie You, Juntao Li, Yan Bowen, and MinZhang. Exploring reversal mathematical reasoning abilityfor large language models. In Lun-Wei Ku, Andre Martins,and Vivek Srikumar, editors, Findings of the Associa-tion for Computational Linguistics: ACL 2024, pp.13671â€“13685, Bangkok, Thailand, August 2024. Associ-ation for Computational Linguistics.
[12] Saurabh Srivastava, Annarose M B, Anto P V, ShashankMenon, Ajay Sukumar, Adwaith Samod T, Alan Philipose,Stevin Prince, and Sooraj Thomas. Functional benchmarksfor robust evaluation of reasoning performance, and thereasoning gap, 2024.
[13] Kun Qian, Shunji Wan, Claudia Tang, Youzhi Wang, Xuan-ming Zhang, Maximillian Chen, and Zhou Yu. VarBench:Robust language model benchmarking through dynamicvariable perturbation. In Yaser Al-Onaizan, Mohit Bansal,and Yun-Nung Chen, editors, Findings of the Associa-tion for Computational Linguistics: EMNLP 2024,pp. 16131â€“16161, Miami, Florida, USA, November 2024.Association for Computational Linguistics.
[14] Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong,and Wei Bi. GSM-plus: A comprehensive benchmark forevaluating the robustness of LLMs as mathematical prob-lem solvers. In Lun-Wei Ku, Andre Martins, and VivekSrikumar, editors, Proceedings of the 62nd AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pp. 2961â€“2984,Bangkok, Thailand, August 2024. Association for Compu-tational Linguistics.
[15] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi,Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathemati-cal reasoning in large language models. arXiv preprintarXiv:2410.05229, 2024.
[16] Lukas Berglund, Meg Tong, Max Kaufmann, MikitaBalesni, Asa Cooper Stickland, Tomasz Korbak, andOwain Evans. The reversal curse: Llms trained on â€ais bâ€ fail to learn â€b is aâ€, 2024.
[17] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, YutakaMatsuo, and Yusuke Iwasawa. Large language models arezero-shot reasoners. Advances in neural informationprocessing systems, Vol. 35, pp. 22199â€“22213, 2022.
[18] Vedant Gaur and Nikunj Saunshi. Reasoning in largelanguage models through symbolic math word problems,2023.
[19] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, BowenYu, Chang Zhou, Chengpeng Li, Chengyuan Li, DayihengLiu, Fei Huang, et al. Qwen2 technical report. arXivpreprint arXiv:2407.10671.â€• 855 â€•This work is licensed by the author(s) under CC BY 4.0



A Necessity of Manual Operations



in Creating Question Templates

Although we have considered using regular expressions andrule-based approaches to automate template creation, they havethe following problems: a) Not all numerical values in the originalinstance are â€œsymbolizable.â€
Some numbers in the instance arespeciï¬c, and altering them would make the instance ill-deï¬ned.b) As shown in Figure 1, when generating the template, it isnecessary to keep the usage of variable consistent between ğ‘„ğ‘ğ‘ğ‘ and ğ‘†ğ‘ğ‘ğ‘ .
It is hard to catch the relationship with rule-basedreplacement and requires human insight.
Therefore, we createdthe question templates manually.


B Large Language Models

We list all of the LLMs used in our experiments.
Generic LLMsâ€¢ Llama-3-8b-Instruct (https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)â€¢ Llama-3.1-8b-Instruct ï¼ˆhttps://huggingface.co/meta-llama/Llama-3.1-8B-Instructï¼‰â€¢ Llama-3.1-70b-Instruct ï¼ˆhttps://huggingface.co/meta-llama/Llama-3.1-70B-Instructï¼‰â€¢ Mistral-7b-Instruct-v0.3 ï¼ˆhttps://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)LLMs for mathematical domainâ€¢ Deepseekmath-7b-rl ï¼ˆhttps://huggingface.co/deepseek-ai/deepseek-math-7b-rlï¼‰â€¢ Wizardmath-7b-v1.1 ï¼ˆhttps://huggingface.co/WizardLMTeam/WizardMath-7B-V1.1)

C Prompts Design for Main Exper-



iments

For the generic LLMs, we developed prompts for solution gen-eration (Figure 3) and answer extraction (Figre 4) based on theprompts used in [17].Generation Prompt â€“ generic modelsSYSTEM:You are an assistant that solves math word problems.
USER: {question} + Letâ€™s think step by step.
Figure 3
The prompt for generic models (generating solutions)Answer Extraction Prompt â€“ generic modelsSYSTEM:
You are an assistant that solves math word problems.
USER: {question} + Letâ€™s think step by step.
ASSISTANT: {modelâ€™s completion}USER:
Therefore, what is the final answer?
Only write the final answer without any texts.
Figure 4 The prompt for generic models (extracting ï¬nal an-swer)For Deepseekmath-7b-rl and Wizardmath-7b-v1.1, we em-ployed prompts based on templates suggested on their web pages.
Figure 5 and 6 show them.
In extracting answers from solutionsgenerated by the two math models, we could simply use regularexpressions since they always generate solutions in a ï¬xed format.
Generation Prompt â€“ Deepseekmath-7b-rlUSER: {question} Please reason step by step and put your final answer within \boxed{}.Figure 5 The prompt for Deepseekmath-7b-rl (genearting so-lutions)â€“ Wizardmath-7b-v1.1USER: Below is an instruction that describes a task.
Write a response that appropriately completes the request.
### Instruction:{question}### Response: Let's think step by step.
Figure 6 The prompt for Wizardmath-7b-v1.1 (generating so-lutions)

D Prompt Design for Error Analysis



Framework

Figure 7 presents the prompt used to transform a predictedsolution into the abstracted form.
Transform ation PromptSYSTEM:
Given the numeric version of a math question and its solution as references, you are a helpful assistant designed to copy the numeric solution to get a solution to the symbolic version of that question.
Instructions:- Symbolic solution should strictly copy the numeric solution no matter whether it is correct or not.-After completion of the solution, output the final answer with "###".
The final answer should be a sole mathematical expression represented by variables appear in the symbolic question.-Mathematical expression in the symbolic solution should not be represented in the format of LaTeX.{few-shot examples}USER: {target solution}Figure 7 The prompt for solution transformationâ€• 856 â€•This work is licensed by the author(s) under CC BY 4.0