Emotion-aware Speech-to-text Translationwith Generative Error Correction

Zhengdong Yang Chenhui Chu



Kyoto University



zd-yang@nlp.ist.i.kyoto-u.ac.jp  chu@i.kyoto-u.ac.jp



Abstract

This paper explores emotion-aware speech-to-text trans-lation (ST) using generative error correction (GER) bylarge language models (LLMs).
Despite recent advance-ments in ST, the impact of the emotional content has beenoverlooked.
First, we enhance the translation of emotionalspeech by adopting the GER paradigm: Finetuned an LLMto generate the translation based on the decoded 𝑁-besthypotheses.
Next, we combine the emotion labels into theLLM ﬁnetuning process to enable the model to considerthe emotion content.
Experiments show that GER and theintegration of emotion labels are eﬀective on the English-Japanese language pair.
This research lays the foundationfor more sophisticated models that consider emotional nu-ances in speech.


1 Introduction

Speech-to-text translation (ST) is a task where the modeltakes speech in one language as input and translates itinto text in another language.
ST performance has greatlyimproved over the recent years with signiﬁcant eﬀorts ondatasets [1, 2, 3, 4, 5, 6] and models [7, 8, 9, 10].
However,an essential aspect often overlooked in speech translationis the emotion of speech.
Human speech naturally includes emotions.
In real-lifeconversations, a listener often uses cues from the speaker’svoice tone to grasp what is being said.
Therefore, emotioncan signiﬁcantly inﬂuence the results of translating speech.
As the instance shown in Figure 1, the phrase “I can’t be-lieve this” can convey a range of emotions, from surpriseand shock to awe and excitement, which can alter its trans-lation in another language.
In Japanese, the translationmight vary from “信じられない” (expressing surprise) to“どうしてこんなことに” (expressing frustration).“I can’t believe this” (Surprise)“I can’t believe this” (Frustration)信じられないどうしてこんなことにEmotion-awareSTModelFigure 1
The expectation for an emotion-aware ST model.
Itcan generate appropriate translation based on the emotion of theinput speech.
Emotion has been studied in machine translation (ortext-to-text translation) studies
[11] and other tasks in nat-ural language processing (NLP), such as sentiment analysisand recognizing emotions in conversations [12].
However,there has been little eﬀort focusing on emotion in ST.
Seam-less Expressive [8] examines the preservation of emotionalstates in speech-to-speech translation, without addressingthe inﬂuence of emotions on the semantic aspects of trans-lation.
Chen et al.
[13] constructed the MELD-ST datasetfor emotion-aware ST, but further community eﬀort inves-tigating the methodology for this task is required.
Meanwhile, recent advancements in large language mod-els (LLMs) leads to growing interest in leveraging theircapabilities in modalities beyond text including speech.
Training end-to-end ST models often face challenges dueto insuﬃcient speech-text parallel data.
However, LLMsare trained on vast amounts of textual data and obtain pow-erful textual generation abilities, which can enhance theST performance.
This has been proven by recent stud-ies that use LLMs as decoders for ST systems [14] or asGenerative Error Correction (GER) models to improve STqualities [15].Speech-text parallel data is scarce, and it is even scarcerwhen it includes emotion annotations.
Therefore, leverag-ing external models like LLMs to help the system under-stand the correlation between emotion and language canbe greatly beneﬁcial.
However, to the best of our knowl-edge, there have not been studies on utilizing LLMs foremotion-aware ST.
Therefore, this research aims to pio-

neer the exploration of the eﬀectiveness of emotion-awareST by: (a) adopting the LLM GER paradigm, (b) addingemotion labels into the GER ﬁnetuning process.
We willintroduce these two proposals in detail in the next section.


2 Method



2.1 Generation Error Correction

As shown in Figure 2, the GER paradigm consists oftwo main parts: a pre-trained ST model for generating 𝑁-best hypotheses, and an LLM ﬁnetuned to work as a GERmodel to re-generate the translation prediction.2.1.1 𝑁-best Hypotheses GenerationIn order to generate inputs for the LLM GER model, Apre-trained ST model is utilized to decode 𝑁-best hypothe-ses from input speech with beam search.
More speciﬁcally,given an input speech 𝑆 in source language, the ST modeltranslates it into target language text by beam search de-coding with a beam size of 𝑀, which generates 𝑁-besthypotheses listT𝑁= {𝑇1, 𝑇2, ..., 𝑇𝑁}(𝑁 ≤ 𝑀).
In prac-tice, we set 𝑁 = 𝑀. The list serves as the preliminaryprediction and a part of the input for the LLM GER model.2.1.2 GER FinetuningInspired by the methods proposed by [15], we leverageLLMs to generate a ﬁnal translation result based on thedecoded 𝑁-best hypotheses.
We expect our model canutilize the strong linguistic and reasoning ability of LLMsto integrate the rich information in the inputs to generatea higher-quality translation result.
This new generativeparadigm can be formulated as:𝑇 = 𝑀𝐸𝑆𝑇(T𝑁, 𝐼)(1)where 𝐼 is a proper instruction for LLM prompting.
Thegoal of our model is to learn a mapping 𝑀𝐸𝑆𝑇from 𝑁-besthypotheses to the true translation.
Following the typi-cal sequence-to-sequence learning strategy, we employ theground-truth translation 𝑇∗as the supervision signal andoptimize the LLM to learn 𝑀𝐸𝑆𝑇in an auto-regressivemanner.
The cross-entropy-based training loss is deﬁnedas:L𝐶𝐸=𝐿∑𝑙=1− log ℙ𝜃(𝑡∗𝑙|𝑡∗𝑙−1, ..., 𝑡∗1;T𝑁, 𝐼)(2)STModelN-bestHypothesesInstructionGERModel(LLM)Adapter信じられないありえないどうしてこんなことに…まじかよEmotionLabelDisappointmentSpeechIcanʼtbelievethisEmotionLabelasInputEmotionLabelasOutputTranslationFigure 2 Overview architecture of our proposed model.where 𝑡∗𝑙is the 𝑙-th token of 𝑇∗, 𝐿 denotes the sequencelength, and 𝜃 denotes the learnable parameters in LLM(i.e., adapter).2.1.3 Parameter-Eﬃcient FinetuningConsidering the large model size of LLMs, we adoptthe eﬃcient ﬁnetuning strategy LLaMA Adapter
[16].
Itinserts a set of learnable adaptation prompts into the top-𝐿of total 𝐻 Transformer layers [17] in a pretrained LLM tolearn high-level semantics.
We denote the prompt for 𝑙-thlayer as 𝑃𝑙∈ ℝ𝑈×𝐷, where 𝑈 is the prompt length and 𝐷is embedding size.
Assume we have 𝑀 tokens 𝑇𝑙∈ ℝ𝑀 ×𝐷including in-struction and already generated response, now we aim topredict the (𝑀 + 1)-th token as the response.
The learnableadaptation prompt is concatenated with 𝑇𝑙as the preﬁx, i.e.,[𝑃𝑙; 𝑇𝑙] ∈ ℝ(𝑈+𝑀) ×𝐷, which provides learned instructionknowledge to guide the subsequent response generation.
Furthermore, considering the prompt 𝑃𝑙is randomlyinitialized and thus could disturb the LLM tuning at theearly training stage, a zero-initialized attention mechanismis devised to mitigate such disturbance.

2.2 Integration of the Emotion Labels

We incorporate emotion labels into the GER ﬁne-tuningprocess to investigate how emotional content inﬂuencestranslation outcomes.
We evaluate the following two ap-

Table 1 Statistics of MELD-ST dataset for diﬀerent language pairs (Lang.) and splits.
There are 7 types of emotion labels: Neutral(Neu.), Joy (Joy.), Sadness (Sad.), Fear (Fea.), Anger (Ang.), Surprise (Sur.), Disgust (Dis.); and 3 types of sentiment labels: Neutral(Neu.), Positive (Pos.), Negative (Neg.)Lang.
Split Total Neu.
Joy. Sad.
Fea.
Ang. Sur.
Dis.
Neu.
Pos.
Neg.
Train 8,069 3,836 1,284 603 209 982 917 238 2,518 1,715 3,836en-ja Validation 1,008 482 176 84 31 116 97 22 482 229 297Test 1,008 479 186 73 25 85 121 39 479 253 276Train 9,314 4,402 1,571 656 232 1,096 1,096 261 4,402 2,084 2,828en-de Validation 1,164 550 202 99 31 127 130 25 550 271 343Test 1,164 550 218 92 32 102 131 39 550 288 326proaches:2.2.1 Emotion Labels as GER InputsWe conﬁgure the GER model to generate translationsbased not only on the decoded 𝑁-best hypotheses but alsoon the ground-truth emotion labels.
This approach allowsus to assess the upper bound of the improvement by in-corporating emotional content.
Then the paradigm can beformulated as:𝑇 = 𝑀𝐸𝑆𝑇(𝐸,T𝑁, 𝐼)(3)where 𝐸 is the emotion label.
The cross-entropy-basedtraining loss is deﬁned as:L𝐶𝐸=𝐿∑𝑙=1− log ℙ𝜃(𝑡∗𝑙|𝑡∗𝑙−1, ..., 𝑡∗1; 𝐸 ,T𝑁, 𝐼)(4)2.2.2 Emotion Labels as GER OutputsIn practice, ground-truth emotion labels are unavailable,necessitating their prediction.
We propose using the GERmodel to directly predict these labels.
Consequently, basedon the hypotheses, the model ﬁrst generates emotion labelsand then the translation.
This approach can be consideredmultitask learning for the GER model.
Then paradigm willbe:𝑂𝐸,𝑇= 𝑀𝐸𝑆𝑇(T𝑁, 𝐼)(5)where𝑂𝐸,𝑇is the concatenated sequence of𝐸and𝑇. Thecross-entropy-based training loss is:L𝐶𝐸=𝐿∑𝑙=1− log ℙ𝜃(𝑜∗𝑙|𝑜∗𝑙−1, ..., 𝑜∗1;T𝑁, 𝐼)(6)where 𝑜∗𝑙is the 𝑙-th token of the ground truth of 𝑂𝐸,𝑇.

3 Experiments



3.1 Dataset

In this study, we use the MELD-ST dataset
[13], anST dataset in an emotionally rich situation, which con-tains both English-Japanese and English-German languagepairs.
The dataset is constructed from translations obtainedfrom a Blu-ray disk of TV series Friends and emotion la-bels from the MELD dataset
[18].As in MELD, the utterances are labeled with 7 diﬀerentemotions and 3 diﬀerent sentiments.
We added both typesof labels into the LLM instructions in our experiments.
The dataset statistics are summarized in Table 1.

3.2 Settings

Models used for diﬀerent parts in our proposed architec-ture includes:• ST Model: We use the state-of-the-art Seam-lessM4T2 [7], a Transformer-based model that sup-ports speech-to-text translation for up to 100 lan-guages.
Experiments are conducted with two modelsizes: medium and large.• LLM GER Model: We select the popular LLaMA-2[19] for our architecture.• Adapter:
We follow the default settings of LLaMAAdapter [16].
The number of tunable Transformerlayers 𝐿 is set to 𝐻 − 1, which means all layersexcept the ﬁrst one are tunable with inserted prompts.
The prompt length 𝑈 is set to 10.The batch size is set to 4, with accumulation iterationsset to 8 (i.e., the real batch size is 32).
We train for 2epochs with the AdamW optimizer [20], with the learningrate initialized at 1𝑒−2and then linearly decreased to 1𝑒−5

Table 2 ST results on MELD-ST dataset.
Language Pair ST Model Size GER Model Emotion Labels BLEU BERTScore BLEURTen-jaMediumno no 2.14 71.7 28.9yes
no 2.50 74.0 25.6yes GER Input 3.50 74.1 26.5yes GER Output 2.90 73.9 25.2Largeno no 1.95 71.0 28.3yes no 3.02 74.1 26.4yes GER Input 3.58 74.1 25.8yes GER Output 3.43 73.7 25.3en-deMediumnono10.25 75.8 50.3yes
no 10.02 76.7 52.2yes GER Input 10.13 76.7 52.2yes GER Output 10.54 76.9 52.7Largeno no 11.73 76.2 52.7yes
no 10.96 77.0 54.0yes GER Input 11.28 77.1 54.3yes GER Output 11.07 76.8 53.5during training.
We conducted experiments for two approaches foradding the emotion labels: adding them as GER inputs toprovide an ideal scenario that represents the performanceupper bound, and adding them as GER outputs to providea more practical scenario.
The proposed approaches arecompared with two baselines: one using the LLM GERmodel without emotion labels and one without using theLLM GER model at all.


3.3 Results

The results are presented in Table 2.
To evaluate thequality of translations, we use several evaluation metrics,including BLEU, BERTScore, and BLEURT.For the en-ja language pair, BLEU scores indicate thatthe GER model shows an improvement over the originalSeamlessM4T model when both ST model sizes are con-sidered.
Incorporating emotion labels further enhancesthis improvement.
Using emotion labels as inputs to theGER model provides a performance upper bound whilepredicting these labels with the GER model results in aslight reduction in performance gains.
Although this pat-tern aligns with our initial assumptions, it does not holdacross other evaluation metrics, such as BERTScore andBLEURT.Conversely, for the en-de language pair, we fail to ob-serve similar trends, suggesting that the proposed methoddoes not apply universally across language pairs.
A po-tential explanation for this discrepancy is the greater cul-tural diﬀerences between English and Japanese comparedto English and German, which might make emotions moreinﬂuential in translations between the former pair.
Upon reviewing the generated 𝑁-best list, we infer thatthe lack of diversity in the list (e.g. a list of “ほんとに”,“本当に ??”, “本当に”, “ホントに”, and “本当に ?”)
.may
limit the GER model’s ability to select appropriatetranslations, even when taking emotional information intoaccount.
This leads to the future direction of increasing thediversity of the𝑁-best list (e.g. by using temperature-basedsampling).


4 Conclusion

In this paper, we pioneer the investigation of emotion-aware ST using LLMs.
We propose to adopt the GERmethod and integrate emotion labels into the GER ﬁne-tuning process.
The experimental results show the eﬀec-tiveness on certain language pairs.
We propose severalpotential future directions to improve our method includ-ing increasing the diversity of the 𝑁-best list and injectingmore acoustic information from the speech into the GERﬁnetuning process.



Acknowledgment

This work was supported by the SPRING program ofKyoto University and JSPS KAKENHI Grant NumberJP23K28144.

References


[1] Mattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli,Matteo Negri, and Marco Turchi. MuST-C: a MultilingualSpeech Translation Corpus. In Proceedings of the 2019Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, Volume 1 (Long and ShortPapers), 2019.
[2] Changhan Wang, Anne Wu, Jiatao Gu, and Juan Pino.CoVoST 2 and Massively Multilingual Speech Translation.In Pro c. Interspeech 2021, 2021.
[3] Ye Jia, Michelle Tadmor Ramanovich, Quan Wang, andHeiga Zen. CVSS Corpus and Massively Multilin-gual Speech-to-Speech Translation. In Proceedings ofLanguage Resources and Evaluation Conference(LREC), 2022.
[4] Guoguo Chen, Shuzhou Chai, Guan-Bo Wang, Jiayu Du,Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey,Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur,Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li,Xuchen Yao, Yongqing Wang, Zhao You, and ZhiyongYan. GigaSpeech: An Evolving, Multi-Domain ASR Cor-pus with 10,000 Hours of Transcribed Audio. In Proc.Interspeech 2021, 2021.
[5] Rong Ye, Chengqi Zhao, Tom Ko, Chutong Meng, TaoWang, Mingxuan Wang, and Jun Cao. GigaST: A 10,000-hour Pseudo Speech Translation Corpus. In Proc. IN-TERSPEECH 2023, 2023.
[6] Agarwal et al. FINDINGS OF THE IWSLT 2023 EVAL-UATION CAMPAIGN. In Proceedings of the 20th In-ternational Conference on Spoken Language Trans-lation (IWSLT 2023), 2023.
[7] Seamless Communication et al. SeamlessM4T: MassivelyMultilingual & Multimodal Machine Translation, 2023.
[8] Seamless Communication et al. Seamless: MultilingualExpressive and Streaming Speech Translation, 2023.
[9] Paul K. Rubenstein et al. AudioPaLM: A Large LanguageModel That Can Speak and Listen, 2023.
[10] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,Christine McLeavey, and Ilya Sutskever. Robust speechrecognition via large-scale weak supervision, 2022.
[11] Enrica Troiano, Roman Klinger, and Sebastian Pad´o. Lostin back-translation: Emotion preservation in neural ma-chine translation. In Proceedings of the 28th Inter-national Conference on Computational Linguistics,2020.
[12] Yao Fu, Shaoyang Yuan, Chi Zhang, and Juan Cao. Emo-tion Recognition in Conversations: A Survey Focusingon Context, Speaker Dependencies, and Fusion Methods.Electronics, 2023.
[13] Sirou Chen, Sakiko Yahata, Shuichiro Shimizu, Zheng-dong Yang, Yihang Li, Chenhui Chu, and Sadao Kuro-hashi. Meld-st: An emotion-aware speech translationdataset. arXiv preprint arXiv:2405.13233, 2024.
[14] Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, YimengZhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, LinquanLiu, et al. On decoder-only architecture for speech-to-text and large language model integration. In 2023 IEEEAutomatic Speech Recognition and UnderstandingWorkshop (ASRU), pp. 1–8. IEEE, 2023.
[15] Yuchen Hu, Chen Chen, Chao-Han Huck Yang, RuizheLi, Dong Zhang, Zhehuai Chen, and Eng Siong Chng.Gentranslate: Large language models are generative mul-tilingual speech and machine translators. arXiv preprintarXiv:2402.06894, 2024.
[16] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Ao-jun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, HongshengLi, and Yu Qiao. Llama-adapter: Eﬃcient ﬁne-tuning oflanguage models with zero-init attention. arXiv preprintarXiv:2303.16199, 2023.
[17] A Vaswani. Attention is all you need. Advances in Neu-ral Information Processing Systems, 2017.
[18] Soujanya Poria, Devamanyu Hazarika, Navonil Majumder,Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: Amultimodal multi-party dataset for emotion recognition inconversations. arXiv preprint arXiv:1810.02508, 2018.
[19] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.Llama 2: Open foundation and ﬁne-tuned chat models.arXiv preprint arXiv:2307.09288, 2023.
[20] Ilya Loshchilov and Frank Hutter. Decoupled weight de-cay regularization. In 7th International Conferenceon Learning Representations, ICLR 2019, New Or-leans, LA, USA, May 6-9, 2019. OpenReview.net,2019.