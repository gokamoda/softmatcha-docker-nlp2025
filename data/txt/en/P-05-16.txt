Emotion-aware Speech-to-text Translationwith Generative Error Correction

Zhengdong Yang Chenhui Chu



Kyoto University



zd-yang@nlp.ist.i.kyoto-u.ac.jp â€ƒchu@i.kyoto-u.ac.jp



Abstract

This paper explores emotion-aware speech-to-text trans-lation (ST) using generative error correction (GER) bylarge language models (LLMs).
Despite recent advance-ments in ST, the impact of the emotional content has beenoverlooked.
First, we enhance the translation of emotionalspeech by adopting the GER paradigm: Finetuned an LLMto generate the translation based on the decoded ğ‘-besthypotheses.
Next, we combine the emotion labels into theLLM ï¬netuning process to enable the model to considerthe emotion content.
Experiments show that GER and theintegration of emotion labels are eï¬€ective on the English-Japanese language pair.
This research lays the foundationfor more sophisticated models that consider emotional nu-ances in speech.


1 Introduction

Speech-to-text translation (ST) is a task where the modeltakes speech in one language as input and translates itinto text in another language.
ST performance has greatlyimproved over the recent years with signiï¬cant eï¬€orts ondatasets [1, 2, 3, 4, 5, 6] and models [7, 8, 9, 10].
However,an essential aspect often overlooked in speech translationis the emotion of speech.
Human speech naturally includes emotions.
In real-lifeconversations, a listener often uses cues from the speakerâ€™svoice tone to grasp what is being said.
Therefore, emotioncan signiï¬cantly inï¬‚uence the results of translating speech.
As the instance shown in Figure 1, the phrase â€œI canâ€™t be-lieve thisâ€ can convey a range of emotions, from surpriseand shock to awe and excitement, which can alter its trans-lation in another language.
In Japanese, the translationmight vary from â€œä¿¡ã˜ã‚‰ã‚Œãªã„â€ (expressing surprise) toâ€œã©ã†ã—ã¦ã“ã‚“ãªã“ã¨ã«â€ (expressing frustration).â€œI canâ€™t believe thisâ€ (Surprise)â€œI canâ€™t believe thisâ€ (Frustration)ä¿¡ã˜ã‚‰ã‚Œãªã„ã©ã†ã—ã¦ã“ã‚“ãªã“ã¨ã«Emotion-awareî˜STî˜ModelFigure 1
The expectation for an emotion-aware ST model.
Itcan generate appropriate translation based on the emotion of theinput speech.
Emotion has been studied in machine translation (ortext-to-text translation) studies
[11] and other tasks in nat-ural language processing (NLP), such as sentiment analysisand recognizing emotions in conversations [12].
However,there has been little eï¬€ort focusing on emotion in ST.
Seam-less Expressive [8] examines the preservation of emotionalstates in speech-to-speech translation, without addressingthe inï¬‚uence of emotions on the semantic aspects of trans-lation.
Chen et al.
[13] constructed the MELD-ST datasetfor emotion-aware ST, but further community eï¬€ort inves-tigating the methodology for this task is required.
Meanwhile, recent advancements in large language mod-els (LLMs) leads to growing interest in leveraging theircapabilities in modalities beyond text including speech.
Training end-to-end ST models often face challenges dueto insuï¬ƒcient speech-text parallel data.
However, LLMsare trained on vast amounts of textual data and obtain pow-erful textual generation abilities, which can enhance theST performance.
This has been proven by recent stud-ies that use LLMs as decoders for ST systems [14] or asGenerative Error Correction (GER) models to improve STqualities [15].Speech-text parallel data is scarce, and it is even scarcerwhen it includes emotion annotations.
Therefore, leverag-ing external models like LLMs to help the system under-stand the correlation between emotion and language canbe greatly beneï¬cial.
However, to the best of our knowl-edge, there have not been studies on utilizing LLMs foremotion-aware ST.
Therefore, this research aims to pio-

neer the exploration of the eï¬€ectiveness of emotion-awareST by: (a) adopting the LLM GER paradigm, (b) addingemotion labels into the GER ï¬netuning process.
We willintroduce these two proposals in detail in the next section.


2 Method



2.1 Generation Error Correction

As shown in Figure 2, the GER paradigm consists oftwo main parts: a pre-trained ST model for generating ğ‘-best hypotheses, and an LLM ï¬netuned to work as a GERmodel to re-generate the translation prediction.2.1.1 ğ‘-best Hypotheses GenerationIn order to generate inputs for the LLM GER model, Apre-trained ST model is utilized to decode ğ‘-best hypothe-ses from input speech with beam search.
More speciï¬cally,given an input speech ğ‘† in source language, the ST modeltranslates it into target language text by beam search de-coding with a beam size of ğ‘€, which generates ğ‘-besthypotheses listTğ‘= {ğ‘‡1, ğ‘‡2, ..., ğ‘‡ğ‘}(ğ‘ â‰¤ ğ‘€).
In prac-tice, we set ğ‘ = ğ‘€. The list serves as the preliminaryprediction and a part of the input for the LLM GER model.2.1.2 GER FinetuningInspired by the methods proposed by [15], we leverageLLMs to generate a ï¬nal translation result based on thedecoded ğ‘-best hypotheses.
We expect our model canutilize the strong linguistic and reasoning ability of LLMsto integrate the rich information in the inputs to generatea higher-quality translation result.
This new generativeparadigm can be formulated as:ğ‘‡ = ğ‘€ğ¸ğ‘†ğ‘‡(Tğ‘, ğ¼)(1)where ğ¼ is a proper instruction for LLM prompting.
Thegoal of our model is to learn a mapping ğ‘€ğ¸ğ‘†ğ‘‡from ğ‘-besthypotheses to the true translation.
Following the typi-cal sequence-to-sequence learning strategy, we employ theground-truth translation ğ‘‡âˆ—as the supervision signal andoptimize the LLM to learn ğ‘€ğ¸ğ‘†ğ‘‡in an auto-regressivemanner.
The cross-entropy-based training loss is deï¬nedas:Lğ¶ğ¸=ğ¿âˆ‘ğ‘™=1âˆ’ log â„™ğœƒ(ğ‘¡âˆ—ğ‘™|ğ‘¡âˆ—ğ‘™âˆ’1, ..., ğ‘¡âˆ—1;Tğ‘, ğ¼)(2)STî˜ModelN-bestî˜Hypothesesî˜Instructionî˜GERî˜Modelî˜î˜î˜î˜(LLM)Adapterî˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜î˜ä¿¡ã˜ã‚‰ã‚Œãªã„î˜ã‚ã‚Šãˆãªã„î˜ã©ã†ã—ã¦ã“ã‚“ãªã“ã¨ã«î˜â€¦î˜ã¾ã˜ã‹ã‚ˆî˜Emotionî˜Labelî˜Disappointmentî˜SpeechIî˜canÊ¼tî˜believeî˜thisî˜Emotionî˜Labelî˜asî˜Inputî˜Emotionî˜Labelî˜asî˜OutputTranslationFigure 2 Overview architecture of our proposed model.where ğ‘¡âˆ—ğ‘™is the ğ‘™-th token of ğ‘‡âˆ—, ğ¿ denotes the sequencelength, and ğœƒ denotes the learnable parameters in LLM(i.e., adapter).2.1.3 Parameter-Eï¬ƒcient FinetuningConsidering the large model size of LLMs, we adoptthe eï¬ƒcient ï¬netuning strategy LLaMA Adapter
[16].
Itinserts a set of learnable adaptation prompts into the top-ğ¿of total ğ» Transformer layers [17] in a pretrained LLM tolearn high-level semantics.
We denote the prompt for ğ‘™-thlayer as ğ‘ƒğ‘™âˆˆ â„ğ‘ˆÃ—ğ·, where ğ‘ˆ is the prompt length and ğ·is embedding size.
Assume we have ğ‘€ tokens ğ‘‡ğ‘™âˆˆ â„ğ‘€ Ã—ğ·including in-struction and already generated response, now we aim topredict the (ğ‘€ + 1)-th token as the response.
The learnableadaptation prompt is concatenated with ğ‘‡ğ‘™as the preï¬x, i.e.,[ğ‘ƒğ‘™; ğ‘‡ğ‘™] âˆˆ â„(ğ‘ˆ+ğ‘€) Ã—ğ·, which provides learned instructionknowledge to guide the subsequent response generation.
Furthermore, considering the prompt ğ‘ƒğ‘™is randomlyinitialized and thus could disturb the LLM tuning at theearly training stage, a zero-initialized attention mechanismis devised to mitigate such disturbance.

2.2 Integration of the Emotion Labels

We incorporate emotion labels into the GER ï¬ne-tuningprocess to investigate how emotional content inï¬‚uencestranslation outcomes.
We evaluate the following two ap-

Table 1 Statistics of MELD-ST dataset for diï¬€erent language pairs (Lang.) and splits.
There are 7 types of emotion labels: Neutral(Neu.), Joy (Joy.), Sadness (Sad.), Fear (Fea.), Anger (Ang.), Surprise (Sur.), Disgust (Dis.); and 3 types of sentiment labels: Neutral(Neu.), Positive (Pos.), Negative (Neg.)Lang.
Split Total Neu.
Joy. Sad.
Fea.
Ang. Sur.
Dis.
Neu.
Pos.
Neg.
Train 8,069 3,836 1,284 603 209 982 917 238 2,518 1,715 3,836en-ja Validation 1,008 482 176 84 31 116 97 22 482 229 297Test 1,008 479 186 73 25 85 121 39 479 253 276Train 9,314 4,402 1,571 656 232 1,096 1,096 261 4,402 2,084 2,828en-de Validation 1,164 550 202 99 31 127 130 25 550 271 343Test 1,164 550 218 92 32 102 131 39 550 288 326proaches:2.2.1 Emotion Labels as GER InputsWe conï¬gure the GER model to generate translationsbased not only on the decoded ğ‘-best hypotheses but alsoon the ground-truth emotion labels.
This approach allowsus to assess the upper bound of the improvement by in-corporating emotional content.
Then the paradigm can beformulated as:ğ‘‡ = ğ‘€ğ¸ğ‘†ğ‘‡(ğ¸,Tğ‘, ğ¼)(3)where ğ¸ is the emotion label.
The cross-entropy-basedtraining loss is deï¬ned as:Lğ¶ğ¸=ğ¿âˆ‘ğ‘™=1âˆ’ log â„™ğœƒ(ğ‘¡âˆ—ğ‘™|ğ‘¡âˆ—ğ‘™âˆ’1, ..., ğ‘¡âˆ—1; ğ¸ ,Tğ‘, ğ¼)(4)2.2.2 Emotion Labels as GER OutputsIn practice, ground-truth emotion labels are unavailable,necessitating their prediction.
We propose using the GERmodel to directly predict these labels.
Consequently, basedon the hypotheses, the model ï¬rst generates emotion labelsand then the translation.
This approach can be consideredmultitask learning for the GER model.
Then paradigm willbe:ğ‘‚ğ¸,ğ‘‡= ğ‘€ğ¸ğ‘†ğ‘‡(Tğ‘, ğ¼)(5)whereğ‘‚ğ¸,ğ‘‡is the concatenated sequence ofğ¸andğ‘‡. Thecross-entropy-based training loss is:Lğ¶ğ¸=ğ¿âˆ‘ğ‘™=1âˆ’ log â„™ğœƒ(ğ‘œâˆ—ğ‘™|ğ‘œâˆ—ğ‘™âˆ’1, ..., ğ‘œâˆ—1;Tğ‘, ğ¼)(6)where ğ‘œâˆ—ğ‘™is the ğ‘™-th token of the ground truth of ğ‘‚ğ¸,ğ‘‡.

3 Experiments



3.1 Dataset

In this study, we use the MELD-ST dataset
[13], anST dataset in an emotionally rich situation, which con-tains both English-Japanese and English-German languagepairs.
The dataset is constructed from translations obtainedfrom a Blu-ray disk of TV series Friends and emotion la-bels from the MELD dataset
[18].As in MELD, the utterances are labeled with 7 diï¬€erentemotions and 3 diï¬€erent sentiments.
We added both typesof labels into the LLM instructions in our experiments.
The dataset statistics are summarized in Table 1.

3.2 Settings

Models used for diï¬€erent parts in our proposed architec-ture includes:â€¢ ST Model: We use the state-of-the-art Seam-lessM4T2 [7], a Transformer-based model that sup-ports speech-to-text translation for up to 100 lan-guages.
Experiments are conducted with two modelsizes: medium and large.â€¢ LLM GER Model: We select the popular LLaMA-2[19] for our architecture.â€¢ Adapter:
We follow the default settings of LLaMAAdapter [16].
The number of tunable Transformerlayers ğ¿ is set to ğ» âˆ’ 1, which means all layersexcept the ï¬rst one are tunable with inserted prompts.
The prompt length ğ‘ˆ is set to 10.The batch size is set to 4, with accumulation iterationsset to 8 (i.e., the real batch size is 32).
We train for 2epochs with the AdamW optimizer [20], with the learningrate initialized at 1ğ‘’âˆ’2and then linearly decreased to 1ğ‘’âˆ’5

Table 2 ST results on MELD-ST dataset.
Language Pair ST Model Size GER Model Emotion Labels BLEU BERTScore BLEURTen-jaMediumno no 2.14 71.7 28.9yes
no 2.50 74.0 25.6yes GER Input 3.50 74.1 26.5yes GER Output 2.90 73.9 25.2Largeno no 1.95 71.0 28.3yes no 3.02 74.1 26.4yes GER Input 3.58 74.1 25.8yes GER Output 3.43 73.7 25.3en-deMediumnono10.25 75.8 50.3yes
no 10.02 76.7 52.2yes GER Input 10.13 76.7 52.2yes GER Output 10.54 76.9 52.7Largeno no 11.73 76.2 52.7yes
no 10.96 77.0 54.0yes GER Input 11.28 77.1 54.3yes GER Output 11.07 76.8 53.5during training.
We conducted experiments for two approaches foradding the emotion labels: adding them as GER inputs toprovide an ideal scenario that represents the performanceupper bound, and adding them as GER outputs to providea more practical scenario.
The proposed approaches arecompared with two baselines: one using the LLM GERmodel without emotion labels and one without using theLLM GER model at all.


3.3 Results

The results are presented in Table 2.
To evaluate thequality of translations, we use several evaluation metrics,including BLEU, BERTScore, and BLEURT.For the en-ja language pair, BLEU scores indicate thatthe GER model shows an improvement over the originalSeamlessM4T model when both ST model sizes are con-sidered.
Incorporating emotion labels further enhancesthis improvement.
Using emotion labels as inputs to theGER model provides a performance upper bound whilepredicting these labels with the GER model results in aslight reduction in performance gains.
Although this pat-tern aligns with our initial assumptions, it does not holdacross other evaluation metrics, such as BERTScore andBLEURT.Conversely, for the en-de language pair, we fail to ob-serve similar trends, suggesting that the proposed methoddoes not apply universally across language pairs.
A po-tential explanation for this discrepancy is the greater cul-tural diï¬€erences between English and Japanese comparedto English and German, which might make emotions moreinï¬‚uential in translations between the former pair.
Upon reviewing the generated ğ‘-best list, we infer thatthe lack of diversity in the list (e.g. a list of â€œã»ã‚“ã¨ã«â€,â€œæœ¬å½“ã« ??â€, â€œæœ¬å½“ã«â€, â€œãƒ›ãƒ³ãƒˆã«â€, and â€œæœ¬å½“ã« ?â€)
.may
limit the GER modelâ€™s ability to select appropriatetranslations, even when taking emotional information intoaccount.
This leads to the future direction of increasing thediversity of theğ‘-best list (e.g. by using temperature-basedsampling).


4 Conclusion

In this paper, we pioneer the investigation of emotion-aware ST using LLMs.
We propose to adopt the GERmethod and integrate emotion labels into the GER ï¬ne-tuning process.
The experimental results show the eï¬€ec-tiveness on certain language pairs.
We propose severalpotential future directions to improve our method includ-ing increasing the diversity of the ğ‘-best list and injectingmore acoustic information from the speech into the GERï¬netuning process.



Acknowledgment

This work was supported by the SPRING program ofKyoto University and JSPS KAKENHI Grant NumberJP23K28144.

References


[1] Mattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli,Matteo Negri, and Marco Turchi. MuST-C: a MultilingualSpeech Translation Corpus. In Proceedings of the 2019Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, Volume 1 (Long and ShortPapers), 2019.
[2] Changhan Wang, Anne Wu, Jiatao Gu, and Juan Pino.CoVoST 2 and Massively Multilingual Speech Translation.In Pro c. Interspeech 2021, 2021.
[3] Ye Jia, Michelle Tadmor Ramanovich, Quan Wang, andHeiga Zen. CVSS Corpus and Massively Multilin-gual Speech-to-Speech Translation. In Proceedings ofLanguage Resources and Evaluation Conference(LREC), 2022.
[4] Guoguo Chen, Shuzhou Chai, Guan-Bo Wang, Jiayu Du,Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey,Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur,Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li,Xuchen Yao, Yongqing Wang, Zhao You, and ZhiyongYan. GigaSpeech: An Evolving, Multi-Domain ASR Cor-pus with 10,000 Hours of Transcribed Audio. In Proc.Interspeech 2021, 2021.
[5] Rong Ye, Chengqi Zhao, Tom Ko, Chutong Meng, TaoWang, Mingxuan Wang, and Jun Cao. GigaST: A 10,000-hour Pseudo Speech Translation Corpus. In Proc. IN-TERSPEECH 2023, 2023.
[6] Agarwal et al. FINDINGS OF THE IWSLT 2023 EVAL-UATION CAMPAIGN. In Proceedings of the 20th In-ternational Conference on Spoken Language Trans-lation (IWSLT 2023), 2023.
[7] Seamless Communication et al. SeamlessM4T: MassivelyMultilingual & Multimodal Machine Translation, 2023.
[8] Seamless Communication et al. Seamless: MultilingualExpressive and Streaming Speech Translation, 2023.
[9] Paul K. Rubenstein et al. AudioPaLM: A Large LanguageModel That Can Speak and Listen, 2023.
[10] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,Christine McLeavey, and Ilya Sutskever. Robust speechrecognition via large-scale weak supervision, 2022.
[11] Enrica Troiano, Roman Klinger, and Sebastian PadÂ´o. Lostin back-translation: Emotion preservation in neural ma-chine translation. In Proceedings of the 28th Inter-national Conference on Computational Linguistics,2020.
[12] Yao Fu, Shaoyang Yuan, Chi Zhang, and Juan Cao. Emo-tion Recognition in Conversations: A Survey Focusingon Context, Speaker Dependencies, and Fusion Methods.Electronics, 2023.
[13] Sirou Chen, Sakiko Yahata, Shuichiro Shimizu, Zheng-dong Yang, Yihang Li, Chenhui Chu, and Sadao Kuro-hashi. Meld-st: An emotion-aware speech translationdataset. arXiv preprint arXiv:2405.13233, 2024.
[14] Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, YimengZhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, LinquanLiu, et al. On decoder-only architecture for speech-to-text and large language model integration. In 2023 IEEEAutomatic Speech Recognition and UnderstandingWorkshop (ASRU), pp. 1â€“8. IEEE, 2023.
[15] Yuchen Hu, Chen Chen, Chao-Han Huck Yang, RuizheLi, Dong Zhang, Zhehuai Chen, and Eng Siong Chng.Gentranslate: Large language models are generative mul-tilingual speech and machine translators. arXiv preprintarXiv:2402.06894, 2024.
[16] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Ao-jun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, HongshengLi, and Yu Qiao. Llama-adapter: Eï¬ƒcient ï¬ne-tuning oflanguage models with zero-init attention. arXiv preprintarXiv:2303.16199, 2023.
[17] A Vaswani. Attention is all you need. Advances in Neu-ral Information Processing Systems, 2017.
[18] Soujanya Poria, Devamanyu Hazarika, Navonil Majumder,Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: Amultimodal multi-party dataset for emotion recognition inconversations. arXiv preprint arXiv:1810.02508, 2018.
[19] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.Llama 2: Open foundation and ï¬ne-tuned chat models.arXiv preprint arXiv:2307.09288, 2023.
[20] Ilya Loshchilov and Frank Hutter. Decoupled weight de-cay regularization. In 7th International Conferenceon Learning Representations, ICLR 2019, New Or-leans, LA, USA, May 6-9, 2019. OpenReview.net,2019.