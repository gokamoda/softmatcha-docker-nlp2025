Representational Analysis of Binding in Language Models

Qin Dai

1

â€ƒ



Benjamin Heinzerling

2,1

â€ƒ



Kentaro Inui

3,1,21

Tohoku University â€ƒ

2

RIKEN AIP â€ƒ

3

MBZUAI



qin.dai.b8@tohoku.ac.jp â€ƒbenjamin.heinzerling@riken.jp



â€ƒkentaro.inui@mbzuai.ac.ae



Abstract

Entity tracking is essential for complex reasoning.
To per-form in-context entity tracking, language models (LMs)must bind an entity to its attribute (e.g., bind a container toits content) to recall attribute for a given entity.
For exam-ple, given a context mentioning â€œThe coï¬€ee is in Box Z,the stone is in Box M, the map is in Box Hâ€, to infer â€œBoxZ contains the coï¬€eeâ€ from the context, LMs must bindâ€œBox Zâ€ to â€œcoï¬€eeâ€.
To explain the binding behaviour ofLMs, Feng and Steinhardt (2023) introduce a Binding IDmechanism and state that LMs use a abstract concept calledBinding ID (BI) to internally mark entity-attribute pairs.
However, they have not captured Ordering ID (OI), namelyordering index of entity, from entity activations that directlydetermines the binding behaviour.
In this work, we pro-vide a novel view of the BI mechanism by localizing OI andproving the causality between OI and binding behaviour.
Speciï¬cally, we discover the OI subspace and reveal causaleï¬€ect of OI on binding that when editing activations alongthe OI encoding direction, LMs tend to bind a given entityto other attributes (e.g., â€œstoneâ€ for â€œBox Zâ€) accordingly.
The code and datasets used in this paper are available athttps://github.com/cl-tohoku/OI-Subspace.


1 Introduction

The ability of a model to track and maintain informa-tion associated with an entity in a context is essential forcomplex reasoning (1; 2; 3; 4; 5).
To recall attribute infor-mation for a given entity in a context, a model must bindentities to their attributes (6).
For example, given a Sample1, a model must bind the entities (e.g., â€œBox Zâ€, â€œBox Mâ€and â€œBox Hâ€) to their corresponding attributes (e.g., â€œcof-feeâ€, â€œstoneâ€ and â€œmapâ€) so as to recall (or answer) suchas what is in â€œBox Zâ€.
Binding has also been studied as afundamental problem in Psychology (7).â€œThe  coffee is   in   Box   Z ,   the   stone is   in   Box   M ,   the   map is   in   Box   H, â€¦ Box   Z   contains   theâ€The coffee Z theâ€¦â€¦â€¦â€¦â€¦â€¦Boxâ€¦â€¦â€¦â€¦â€¦â€œThe coffee  â€¦ Box  Z  contains  theâ€â€œThe coffee  â€¦ Box  Z  contains  theâ€Z theâ€¦â€¦â€¦â€¦â€¦Z theâ€¦â€¦â€¦â€¦â€¦PC1PC2+=PC1PC2+=PatchingPatchingContextiContextiContextiâ€œcoffeeâ€â€œmapâ€â€œstoneâ€AnswerAnswerAnswerOriginal LMLM with OI PatchLM with OI PatchQueryiQueryiQueryiâ€¦PCAMrOI Subsapce Extractionxixixi*xi*â€¦â€¦â€¦Figure 1: Our main ï¬nding on Ordering ID (OI) subspaceintervention.
Patching entity (e.g., "Z") representationsalong OI direction (i.e., PC1) in activation space yieldscorresponding changes in model output.
To uncover how Language Models (LMs) realize bindingin term of internal representation, an existing research (6)introduces the Binding ID mechanism that LMs apply anabstract concept called Binding ID (BI) to bind and markEntity-Attribute (EA) pairs (e.g., â€œBox Zâ€ and â€œcoï¬€eeâ€ inSample 1, where BI is denoted as a numbered square).However, they have not captured the Ordering ID (OI)information from the entity (or attribute) activations thatcausally aï¬€ects binding behaviour and thus BI informa-tion as well.
Here, OI is deï¬ned as the input order (orordering index) of entities and attr ibutes, no matter theyare bound by a relation (e.g., â€œis_inâ€ in Sample 1) or not,such as the indexing number in Sample 1 and Sample 2.We can observe that in a 1E-to-1A bound context, such asin Sample 1, BI and OI are interchangeable.(1) Context: The coï¬€eeÃœ0is in Box ZÃœ0, the stoneÃœ1is in Box MÃœ1, the mapÃœ2is in Box HÃœ2.Query: Box ZÃœ0contains the(2) Non-related Context: The coï¬€eeÃœ0and Box ZÃœ0are scattered around, the stoneÃœ1is here and BoxMÃœ1is there, the mapÃœ2and Box HÃœ2are in diï¬€er-ent place.
Query:
Box ZÃœ0contains theSince binding is the foundational skill that underlies en-tity tracking (6), in this work, we take the entity trackingtask (8; 9) as a benchmark to analyze the LMâ€™s bindingbehaviour.
Based on the analysis of internal representa-tion on this task, we localize the OI information from theactiviations and provide a novel view of the BI mecha-nism.
Speciï¬cally, we apply Principle Component Anal-ysis (PCA) as well as other dimension reduction methodssuch as Partial Least Squares to analyze the activations ofLMs, and which are empirically proven to be eï¬€ective.
Wediscover that LMs encode (or store) the OI informationinto a low-rank subspace (called OI subspace hereafter),and the discovered OI subspace can causally aï¬€ect bindingbehaviour and thus BI information as well.
That is, we ï¬ndthat by causally intervening along the OI encoding Prin-ciple Component (PC), LMs swap the binding and infer anew attribute for a given entity accordingly.
For example,as shown in Figure 1, by patching activations along thedirection (i.e., PC1), we can make the LMs to infer â€œBox Zcontains the stoneâ€ and â€œBox Z contains the mapâ€ insteadof â€œBox Z contains the coï¬€eeâ€.
Therefore, our ï¬ndingsextend the previous BI based understanding of binding inLMs (6) by revealing the causality between OI and binding.
In addition, we ï¬nd that such OI subspace that deter-mines binding is prevalent across multiple LM familiessuch as Llama2 (10)(and Llama3 (11)), Qwen1.5 (12)and Pythia (13), and the code ï¬ne-tuned LM Float-7B (9).Please see our paper (14) for more details.


2 Finding OI Subspace

In this section we describe our Principle ComponentAnalysis (PCA) based method to localize the OI subspacein activations of LMs.
As shown in Figure 1, given a LM(e.g., Llama2), and a collection of texts which describesa set of EA pairs related by a relation such as â€œis_inâ€ inSample 1, we extract the activation of entity token (e.g.,â€œZâ€) in query (denoted as xğ‘–) from a certain layer1ï¼‰.
Wethen construct a activation matrix ğ‘€ğ‘ŸP ğ‘…ğ‘›Ë†ğ‘‘for a relationğ‘Ÿ, where ğ‘› denotes the number of entities and ğ‘‘ denotesthe dimension of the activation.
The row ğ‘– of ğ‘€ğ‘Ÿis theactivation of an entity token (i.e., xğ‘–).PCA has been applied for identifying various subspace(or direction) such as the subspace encoding languagebias (15), truth value of assertions (16) and sentiment (17).1ï¼‰
The layer is determined by a development setBI_1 BI_2 BI_3 BI_4 BI_5 BI_6
BI_7layer0 layer1 layer2 layer3layer4 layer5 layer6 layer7layer8 layer9 layer10 layer11layer12 layer13 layer14 layer15layer16 layer17 layer18 layer19layer20 layer21 layer22 layer23layer24 layer25 layer26 layer27layer28 layer29 layer30 layer31Figure 2: Layer-wise OI subspace visualization on Llama2-7B, where â€œBIâ€ primarily denotes OI.Inspired by these studies, we choose PCA as our ï¬rst at-tempt to localize OI subspace.
Speciï¬cally, the PCA of aactivation matrix is ğ‘€ğ‘Ÿâ€œ ğ‘ˆğ‘ŸÎ£ğ‘Ÿğ‘‰ğ‘‡ğ‘Ÿ, where the columns ofğ‘‰ğ‘ŸP ğ‘…ğ‘‘Ë†ğ‘‘are principle directions of ğ‘€ğ‘Ÿ.
We takes ï¬rst ğ‘columns of ğ‘‰ğ‘Ÿas the OI direction, denoted as ğµğ‘ŸP ğ‘…ğ‘‘Ë†ğ‘.We adopt a subset of the entity tracking dataset (8; 9),which contains ğ‘› â€œ 1000 samples, to create layer (ğ‘™) wiseactivation matrix ğ‘€ğ‘™ğ‘Ÿ.
We then use the ğ‘€ğ‘™ğ‘Ÿto extract thelayer-wise OI subspace projection matrix ğµğ‘™ğ‘ŸP ğ‘…ğ‘‘Ë†2tovisualize the activations.
Figure 2 shows the embeddingvisualization on Llama2-7B, where each point representsthe activation of an entity projected via the ğµğ‘™ğ‘Ÿ, and thecolors represent OIs.
From which, we can observe thatmiddle layers, such as layer 8, have a clearly visible di-rection along which OI increases, while the others havetangled distribution.
We also observe similar pattern of distribution onLlama3-8B, Float-7B and other LM families such asQwen1.5 and Pythia.
This indicates that LMs use themiddle layers to encode OI information, and the ï¬nding isprevalent across multiple LM families.
This ï¬nding is alsoconsistent with the â€œstages of inference hypothesisâ€ (18)stating that the function of early layers is to perform deto-kenization, middle layers do feature engineering, and lateAnswer for # StepContext Query 1
2 3 4 5 6The coï¬€ee is in Box Z, the stone is in Box M,the map is in Box H, the coat is in Box L,the string is in Box T, the watch is in Box E,the meat is in Box F.Box Zcontainsthestone map map string watch meatThe letter is in Box Q, the boot is in Box C,the fan is in Box N, the crown is in Box R,the guitar is in Box E, the bag is in Box D,the watch is in Box K.Box Qcontainstheboot fan crown guitar watch watchThe cross is in Box Z, the ice is in Box D,the ring is in Box F, the plane is in Box Q,the clock is in Box X, the paper is in Box I,the engine is in Box K.Box Zcontainstheice ring ring clock paper engineTable 1: Attributes inferred by Llama2-7B as a result of di-rected activation patching along OI-PC in the OI subspaceon the dataset of â€œr: is_inâ€, where color denotes the BI.layers map the representations from the middle layers intothe output embedding space for next-token prediction.
Ac-cording to the hypothesis, we would expect to ï¬nd theordering feature most prominently represented in middlelayers, which is exactly what the visualization shows.
Wecall this dimension that represents OI as OI Principle Com-ponent (OI-PC).
In the following section, we apply causalintervention on the OI-PC to analyze how OI-PC aï¬€ect themodel output.


3 Causal Interventions on OI-PC


In order to test if OIs are not only encoded in the OIsubspace, but that these representations can be steered soas to swap the binding and change LMâ€™s output, in thissection, we perform interventions to analyze the causality.
That is, we want to ï¬nd out if making interventions alongOI-PC leads to a change in LMâ€™s binding computation.
Activation Patching (AP)(19) has been recently pro-posed to causally intervene computational graph of a LMso as to inter pret the function of a target computationalnode (or edge).
Diï¬€erent with the common AP setup, werealize AP by directly editing activations along a particu-lar direction (i.e., along OI-PC), similar to the activationediting method of (20; 21; 22).


3.1 Setting

Dataset To explore the internal representation that en-ables binding, we adopt the entity tracking dataset (8; 9).The dataset consists of English sentence describing a setof objects (here called attributes) located in a set of boxeswith diï¬€erence labels (here called entities), and the task isto infer what is contained by a given box.
For instance,when a LM is presented with â€œThe coï¬€ee is in Box Z, the0 2 4 6âˆ’500 2 4 6âˆ’500 2 4 6âˆ’500 2 4 6âˆ’500 2 4 6âˆ’5050 2 4 6âˆ’505Intervention Step Intervention Step Intervention StepAverage Logit DifferenceAverage Logit DifferenceBI_1 BI_2
BI_3BI_4 BI_5
BI_6Figure 3: Logit Diï¬€erence (LD) for OI-PC based interven-tion across datasets on Llama2-7B, where x axis denotesthe number of intervention steps on ğ‘’0, y axis does the LD,BI_i represents each target attribute and the light yellowbottom line indicates the LD of original attribute (i.e., ğ‘0).Here, ğ‘™ â€œ 8, ğ‘£ â€œ 2.5, and ğ›¼ â€œ 3.0.0 1 2 3 4 5 600.20.40.60.81BI_6 BI_5 BI_4 BI_3 BI_2
BI_1 BI_0Intervention StepLabel ProportionFigure 4: Logit ï¬‚ip for OI-PC based intervention acrossdatasets on Llama2-7B, where x axis denotes the numberof intervention steps on ğ‘’0, y axis does the proportion ofeach inferred attribute in model output.stone is in Box M, the map is in Box H, ...
Box Z containstheâ€, the LM should infer the next token as â€œcoï¬€eeâ€.
Eachsample involves 7 EA pairs.
Metrics We apply two evaluation metrics: logit diï¬€er-ence (23) and logit ï¬‚ip (24).
The logit diï¬€erence metriccalculates diï¬€erence in logits of a target token betweenoriginal and intervened setting.
The "logit ï¬‚ip" accuracymetric represents the proportion of candidate tokens inmodel output after a causal intervention.


3.2 Results: Direct Editing OI Subspace

We intervene via the Equation 1, where x0,ğ‘™is the origi-nal activation of ğ‘’0(i.e., the leftmost entity) in layer ğ‘™, xËš0,ğ‘™isthe intervened activation, ğµğ‘Ÿis the OI subspace projectionmatrix mentioned in Section (Â§2), ğ›¼ is a hyper-parameter toscale the eï¬€ect of intervention and ğ›½ (0 Ä ğ›½ Ä 6) denotesthe number of steps.xËš0,ğ‘™â€œ x0,ğ‘™` ğ›¼ğµğ‘‡ğ‘Ÿpğµğ‘Ÿx0,ğ‘™` ğ›½ğ‘£q (1)Table 1 lists several examples under the OI subspaceintervention on the entity tracking dataset (8; 9).
We cansee that when adding 1 step along OI-PC, the model se-lects â€œstoneâ€ for entity â€œZâ€ instead of its original attributeâ€œcoï¬€eeâ€.
Similarly, when the step is doubled, the modelwill select attribute â€œmapâ€ for the entity, and so on.
Thisindicates that changing the value along OI-PC can inducethe swap of attribute.
Besides the qualitative analysis, we also conduct quan-titative analysis for the causality between the OI subspacebased AP and the binding behaviour of LMs.
We plotmean-aggregated eï¬€ect of the OI-PC based AP in Figure 3.Figure 3 indicates how the Logit Diï¬€erence (LD) of eachattribute changes as the step increases.
We can observethat as the number of steps increases, LD of the origi-nal attribute decreases.
In contrast, LD of other attributesgradually increase until a certain point and then graduallydecrease.
Given a candidate attribute, its LD peak roughlycorresponds to the number of steps that is equal to its BI.For instance, when adding 3 steps, the points of BI_3 (i.e.,attributes of BIâ€œ 3) achieve the highest LD score.
Thisindicates that by adjusting the value along the OI-PC, wecan adjust BI information and thus increase the logit scoreof the corresponding attribute.
Similarly, Figure 4 illustrates the relation between thenumber of steps and the logit ï¬‚ip, which gauges the per-centage of the predicted attributes under an intervention.
Figure 4 shows that as the step increases, the proportionbar becomes darker, it means that the model promotes theproportion of the corresponding attribute in its inference.
For instance, when adding 3 step on the subspace, the ğ‘3(i.e., BI_3) becomes the major of the answers.
This provesthat the OI-PC based interventions can causally aï¬€ect BIinformation as well as the computation of Binding in a LM.OI Subspace and Other Information: the indepen-dence of OI subspace from positional information (i.e.,ğ‘ğ‘œğ‘ ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘–ğ‘‘ğ‘ ) is studied in Appendix (Â§A.1), and its rela-tionship with the existence of a binding relation (e.g., â€œisinâ€) is analyzed in Appendix (Â§A.2).Figure 5: PLS components and R2 scoreInput (1-to-1)â€œA0is in E0, A1is in E1, A2is in E2, A3is in E3,A4is in E4, A5is in E5, A6is in E6.â€Input (n-to-1)â€œA0is in E0, A1is in E1, A2is in E0, A3is in E2,A4is in E3, A5is in E0, A6is in E4.â€Table 2: A n-to-1 sample where entity â€œE0â€ has 3 attributes

3.3 OI Subspace for n-to-1 Setting

Section (Â§2) reveals that LMs encode OI into OI-PCunder 1-to-1 setting, that is, one entity only has one at-tribute.
In this session, we analyze how a LM encodes OIunder n-to-1 setting, where one entity possesses multipleattributes.
To do so, we create an alternative dataset asshown in Table 2, and analyze it via Partial Least Squares(PLS)( 25).
PLS aims to learn low-dimensional represen-tation of the activation of an entity (e.g., E0) that keeps (orpredicts) the OIs of its corresponding multiple attributes(e.g., OIâ€œ 0, OIâ€œ 2 and OIâ€œ 5).
Figure 5 (left) showsthat compared to 1-to-1, in the n-to-1 setting, the LM hasa high R2 score when PLS components are more than 20,indicating that a LM encodes OI via relatively high-ranksubspace while dealing with multiple attributes.
In addi-tion, Figure5 (right) shows the R2 score for each attribute,indicating that the encoding capacity of OI varies with theorder of each bound attribute in the n-to-1 setting.


4 Conclusion and Future Work

In this work, we study the in-context binding, a fun-damental skill underlying many complex reasoning andnatural language understanding tasks.
We provide a novelview of the Binding ID mechanism (6) that there exists asubspace in the activation of LMs that primarily encodesthe ordering information and which is used as the proto-type of BIs to causally determine binding.
Our future workincludes: 1.
the analysis of OI subspace in a more realisticsetting; 2. OI subspace based mechanistic analysis.



Acknowledgements

This work was supported by JST CREST Grant Num-ber JPMJCR20D2 and JSPS KAKENHI Grant Number21K17814.

References


[1]Lauri Karttunen. Discourse referents. In Notes fromthe linguistic underground, pp. 363â€“385. Brill, 1976.
[2]Irene Heim. File change semantics and the familiaritytheory of deï¬niteness. Semantics Critical Concepts inLinguistics, pp. 108â€“135, 1983.
[3]Mante S Nieuwland and Jos JA Van Berkum. Whenpeanuts fall in love: N400 evidence for the power ofdiscourse.Journal of cognitive neuroscience, Vol. 18,No. 7, pp. 1098â€“1111, 2006.
[4]Regina Barzilay and Mirella Lapata. Modeling localcoherence: An entity-based approach. ComputationalLinguistics, Vol. 34, No. 1, pp. 1â€“34, 2008.
[5]Hans Kamp, Josef Van Genabith, and Uwe Reyle.Discourse representation theory. In Handbook ofPhilosophical Logic: Volume 15, pp. 125â€“394.Springer, 2010.
[6]Jiahai Feng and Jacob Steinhardt. How do languagemodels bind entities in context? arXiv preprintarXiv:2310.17191, 2023.
[7]Anne Treisman. The binding problem. Current opinionin neurobiology, Vol. 6, No. 2, pp. 171â€“178, 1996.
[8]Najoung Kim and Sebastian Schuster. Entity trackingin language models. arXiv preprint arXiv:2305.02363,2023.
[9]Nikhil Prakash, Tamar Rott Shaham, Tal Haklay,Yonatan Belinkov, and David Bau. Fine-tuning en-hances existing mechanisms: A case study on entitytracking. arXiv preprint arXiv:2402.14811, 2024.
[10]Hugo Touvron et al. Llama 2: Open foundation andï¬ne-tuned chat models, 2023.
[11]AI@Meta. Llama 3 model card. 2024.
[12]Jinze Bai, Bai, et al. Qwen technical report. arXivpreprint arXiv:2309.16609, 2023.
[13]Biderman et al. Pythia: A suite for analyzing largelanguage models across training and scaling. InInternational Conference on Machine Learning, pp.2397â€“2430. PMLR, 2023.
[14]Qin Dai, Benjamin Heinzerling, and Kentaro Inui.Representational analysis of binding in language mod-els. arXiv preprint arXiv:2409.05448, 2024.
[15]Ziyi Yang, Yinfei Yang, Daniel Cer, and Eric Darve.A simple and eï¬€ective method to eliminate the selflanguage bias in multilingual representations. arXivpreprint arXiv:2109.04727, 2021.
[16]Samuel Marks and Max Tegmark. The geometry oftruth: Emergent linear structure in large languagemodel representations of true/false datasets. arXivpreprint arXiv:2310.06824, 2023.
[17]Curt Tigges, Oskar John Hollinsworth, Atticus Geiger,and Neel Nanda. Linear representations of sen-timent in large language models. arXiv preprintarXiv:2310.15154, 2023.
[18]Vedang Lad, Wes Gurnee, and Max Tegmark. Theremarkable robustness of llms: Stages of inference?arXiv preprint arXiv:2406.19384, 2024.
[19]Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,Sharon Qian, Daniel Nevo, Yaron Singer, and StuartShieber. Investigating gender bias in language modelsusing causal mediation analysis. Advances in neuralinformation processing systems, Vol. 33, pp. 12388â€“12401, 2020.
[20]Yuta Matsumoto, Benjamin Heinzerling, MasashiYoshikawa, and Kentaro Inui. Tracing and manip-ulating intermediate values in neural math problemsolvers. arXiv preprint arXiv:2301.06758, 2023.
[21]Benjamin Heinzerling and Kentaro Inui. Monotonicrepresentation of numeric properties in language mod-els. arXiv preprint arXiv:2403.10381, 2024.
[22]Joshua Engels, Isaac Liao, Eric J. Michaud, WesGurnee, and Max Tegmark. Not all language modelfeatures are linear, 2024.
[23]Kevin Wang, Alexandre Variengien, Arthur Conmy,Buck Shlegeris, and Jacob Steinhardt. Interpretabilityin the wild: a circuit for indirect object identiï¬cation ingpt-2 small. arXiv preprint arXiv:2211.00593, 2022.
[24]Atticus Geiger, Wu, et al. Inducing causal structurefor interpretable neural networks. In InternationalConference on Machine Learning, pp. 7324â€“7338.PMLR, 2022.
[25]Paul Geladi and Bruce R Kowalski. Partial least-squares regression: a tutorial. Analytica chimica acta,Vol. 185, pp. 1â€“17, 1986.



A Appendix



A.1 OI Subspace and Position

To prove the independence between OI subspace and Po-sitional Information (PI), which is namely the ğ‘ğ‘œğ‘ ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘–ğ‘‘ğ‘ of input tokens, we create the following alternative dataset.
The dataset is created by adding Filler Words (FW) withvarious length, such as â€œOKâ€, â€œI see thatâ€ and â€œThereis no particular reasonâ€, in front of the entity trackingdataset (8; 9), as shown in Table 3.
Since the length (i.e.,PC1 PC2 PC3âˆ’0.200.20.40.60.81OIPICorrelationFigure 6: Spearmanâ€™s rank correlation between OI-PC andPI (or OI), where â€œPCğ‘–â€ denotes the ğ‘–-th PC of the OIsubspace and â€œPIâ€ is the length of FW.the number of tokens) of FW directly changes the PI ofits following entities and attributes without aï¬€ecting theirOIs, we take the length as the measure of intervention onPI and apply Spearmanâ€™s rank correlation ğœŒ to calculatethe correlation between the length (denoted as PI) and theOI-PC value.
Figure 6 shows ğœŒ between PI and OI-PC aswell as between OI and OI-PC.
We can observe that OI-PChas high ğœŒ with OI but almost zero ğœŒ with PI, indicatingthat the discovered OI-PC is highly correlated with OI in-formation but independent with PI.
Therefore, the OI-PCdoes not simply encode absolute token position.
Input (original)â€œThe apple is in Box E, the bell is in Box F, ...â€Input (with ï¬ller words)â€œI will ï¬nd out that the apple is in Box E, the bell isin Box F, ...â€œTable 3: An example of the dataset with ï¬ller words â€œI willï¬nd out thatâ€.
Input (original)â€œThe apple is in Box E, the bell is in Box F, ...â€Input (Non-related)â€œI see apple, somewhere else there is Box E,the bell and Box F are scattered around, ...â€Table 4: An example of the dataset with non-related ex-pression.
PC1 PC2 PC300.20.40.60.81RelatedYesNoCorrelationFigure 7: OI-PC based correlation between attributes andtheir corresponding entities, where â€œPCğ‘–â€ denotes the ğ‘–-th PC of the OI subspace, â€œYesâ€ and â€œNoâ€ represent therelated (i.e., original) and non-related dataset respectively.


A.2 OI Subspace and Relatedness


In order to uncover the relationship between OI-PC andthe relatedness, which namely means the existence of abinding relation, we create an alternative dataset by con-verting relational expression into non-related one, as shownin Table 4.
We can observe that non-related expressioncould make a target EA pair (e.g., â€œBox Eî¢Ÿî¢Ÿ and â€œap-pleâ€) semantically unrelated but retain their OI (e.g., theOI of â€œappleî¢Ÿî¢Ÿ and â€œbellâ€ are still 0 and 1 respectively).We select Spearmanâ€™s rank correlation ğœŒ as the correlationmetric and compare the ğœŒ of the non-related dataset withthe related one in the Figure 7.We can observe that ğœŒ of non-related dataset is slightlylower than the related (i.e., original) one, indicating thatthe OI-PC might contain limited relational information sothat removing it can marginally decrease the ğœŒ. However,there is still strong correlation between the non-related (ornon-bound) entity attribute pair, indicating that the OI-PCprimarily encodes the OI information but not the related-ness.