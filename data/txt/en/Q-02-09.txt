An in-depth human study of the mathematical reasoningabilities in Large Language Models

Carolina Dias-Alexiou, Edison Marrese-Taylor, Yutaka Matsuo



Graduate School of Engineering, The University of Tokyo



{carolina.dias,emarrese,matsuo}@weblab.t.u-tokyo.ac.jp



Abstract

We study the generalization capabilities of Large Lan-guage Models through the lens of mathematical reasoning,asking if these models can recognize that two structuresare the same even when they do not share the same nomen-clature.
We propose a human study to evaluate if LLMsreproduce proofs that they have most likely seen dur ingtraining, but when the symbols do not match the ones seen.
To test this in a controlled scenario, we look at proofs inpropositional calculus, foundational for other logic sys-tems, semantically complete and widely discussed online.
We replace the implication operator (→) with an unrelated,arbitrary symbol (♠) and ask experts to evaluate how theoutput of a selection of LLMs changes in terms of com-pliance, correctness, extensiveness and coherence.
Our re-sults show that nearly all our tested models produce lowerquality proofs in this test, in particular open-weights mod-els, suggesting the abilities of these LLMs to reason in thiscontext have important limitations.


1 Introduction

Mathematical reasoning is a key aspect of human intel-ligence that encompasses pattern recognition and logicalentailment.
The development of artiﬁcial intelligence sys-tems capable of tasks such as solving applied and theoreti-cal mathematical problems has been a long-standing focusof research in the ﬁelds of machine learning and naturallanguage processing, dating back to the 1960s
[1, 2].Our interest in this topic rises from recent attempts touse language models for theorem proving by means ofITPs’ programming languages and databases of theoremswith their proofs.
Deep learning models can be trainedin one of these many programming languages, and thenused to generate mathematical proofs.
Data sources forneural theorem proving in ITPs include interactive learningenvironments that interface with ITPs, and datasets derivedfrom proofs in ITP libraries.
When it comes to mathematical reasoning, and in par-ticular to the ability of models to understand logical state-ments, we note that despite the abundance of studies, pre-vious works generally assume that only “variables” are theones not constant throughout problems.
However, we seethat in mathematics diﬀerent nomenclatures are used in dif-ferent areas, as well as in diﬀerent time periods, to expressthe same ideas.
Examples of this fact include the use of ⊃instead of → in logic, particularly in texts written beforecomputers; Leibniz’sd 𝑓d𝑥, Lagrange’s 𝑓0, and Newton’s¤𝑓notation in diﬀerential calculus; the diﬀerent notation forthe inner product for physicists h·|·i and for mathematiciansh·,·i; preﬁx 𝑓 (𝑥) and postﬁx (𝑥) 𝑓 notations for functions;and diﬀerent notations for the von Neumann generated al-gebra, such as: 𝑊∗(·,·) and · ∨ ·, to name a few.
As we attempt to use LLMs to tackle proof generationtasks, for example using the “informal theorem proving”approach (please see §2 for more details on this), we thinkresearchers and practitioners need to take this fact intoconsideration.
Furthermore, and in contrast to all thesemodels, current state-of-the-art models in NLP are trainedon large datasets of text extracted from the web.
In this case,we have limited or no control on the kinds of expressionsand/or operators that the models are exposed to duringtraining.
We can, however, still assume the models havebeen exposed mostly to the standard nomenclature.
Given this scenario, we ask: can these models recognizethat two structures are the same even if they do not share thesame nomenclature?
For example, can models reproduceproofs that they have most likely seen during training, butwhen the symbols do not match the ones seen?
If so, towhat extent are these proofs plausible and correct?
In orderto answer these questions, we perform an in-depth humanevaluation to assess the quality of the output generated byLLMs when prompted to generate proofs similar to theones seen during training but with expressions that use adiﬀerent notation.


2 Related Work

Automated Theorem Proving The ﬁrst proofs us-ing computers started appearing as early as the 1950s, soonafter electronic computers became available.
This playeda big role in the development of the ﬁeld of automated rea-soning, which itself led to the development of AI.
Most ofthe early work on computer-assisted proof was devoted toautomated theorem proving (ATP)[3], in which machineswere expected to prove assertions fully automatically.
Theincreased availability of interactive time-sharing computeroperating systems in the 1960s allowed the developmentof interactive theorem provers (ITPs) in which the ma-chine and the user work together to produce a formal proof.
While ATPs include proof-search algorithms to generatewhole proofs, ITPs usually check the validity of humaninput statements, although they may also include reducedautomated tools.
More recently, work on “informal” theorem proving haspresented an alternative medium for theorem proving, inwhich statements and proofs are written in a mixture ofnatural language and symbols used in “standard” math-ematics (e.g., in LATEX), and are checked for correctnessby humans.
Here we ﬁnd the work of [4] who developedNaturalProofs, a large-scale dataset of 32k informal math-ematical theorems, deﬁnitions, and proofs, and provided abenchmark for premise selection via retrieval and gener-ation tasks.
Most of the data is taken from websites likeproofwiki.com, and though this enables more ﬂexibil-ity when proving, the task is approached in a way similarto ITPs.
Mathematical Reasoning in LLMs Early worksattempting to study the ability of models to recognize pat-terns in mathematical expressions focused on the manipu-lation of simple expressions using standard notation.
Forexample, [5] trained models on datasets in which pairs ofexamples contain Boolean logic and arithmetic expressionswhich are known to be equivalent.
For example, expres-sions like 𝑐2and (𝑐 · 𝑐) + (𝑏 − 𝑏) are equivalent.
However,expressions with the same structure, but diﬀerent variables,such as 𝑐 · (𝑎 ·𝑎 + 𝑏) and 𝑓 · (𝑑 · 𝑑 + 𝑒), are not.
They showedthat such models were capable of relating non-paired ex-pressions, like 𝑎 − (𝑏 − 𝑐) and 𝑏 − (𝑎 + 𝑐), as negations ofeach other.[6] studied the ability of neural networks to understandlogical entailment via training models on synthetic datasetsof logical statements and their evaluations (True/False).Concretely, they generated datasets of triples of the form( 𝐴, 𝐵, 𝐴 ⊨ 𝐵), where 𝐴 and 𝐵 are formulas of propositionallogic, and 𝐴 ⊨ 𝐵 is 1 if 𝐴 entails 𝐵, and 0 otherwise.
Theyconcluded that, from the models available at the time, thosewith a tree structure seemed to be better for domains withunambiguous syntax.
In these works, expressions are generated automaticallystarting from a set of simple rules plus a set of arbitrarycombinations.
This allows to scale and control the typesof expressions that are shown to the model during train-ing/inference.
Later works, like [7] and the more recent [8]shift to a question-answer format using natural language,with the release of the GSM8K and GPQA datasets, re-spectively, while also extending the class of questions toother areas of mathematics, like calculus and probability,where the ability to control for the type of expressions isreduced.


3 Proposed Approach

To study the posed research questions under a controlledscenario, we look at proofs in propositional calculus, abranch of formal logic that deals with propositions, whichcan be true or false, and relations between propositions,including the construction of arguments based on them[9].
Propositional calculus, also known as zeroth-orderlogic, does not deal with quantiﬁers over non-logical ob-jects (unlike ﬁrst-order or higher-order logic).
There areseveral reasons that we think make this the ideal scenariofor our study: (1) All the machinery of propositional logicis included in ﬁrst-order logic, higher-order logic, and allmathematics.
In this sense, propositional logic is the foun-dation of other logic systems; (2) Propositional calculus issemantically complete, i.e. any tautology (true formulas)can be proved with the formal axioms and the rules of in-ference of the system, and (3) Being the subject of commonundergraduate courses, demonstrations in this context havebeen widely discussed online so we can reasonably assumethat LLMs have been exposed to these type of proofs.
Propositional calculus is typically studied with a formalsystem, which contains a formal language and a deduc-tive system.
The language is composed of a set of well-formed formulas, which are strings of symbols from analphabet (composed of propositional variables and propo-sitional connectives) formed by a formal grammar (forma-tion rules).
The deductive system, in turn, contains therules of inference, a function which takes premises and re-turns conclusions.
To assess how models generalize in thisscenario, we compared proofs generated by these modelsusing ’usual’ and ’unusual’ symbols for connectives.
We use a standard proof system usually referred to asa Hilbert system.
It is a deductive system that generatestheorems from axioms (a tautology taken as starting pointfor further reasoning) and modus ponens.
Modus ponenscan be summarized as: If P implies Q and P is knownto be true, one can conclude that Q must also be true.
Itis generally expressed as {𝑃 → 𝑄, 𝑃} ` 𝑄, where theturnstile symbol (`) denotes derivability, i.e. there is aformal derivation of a theorem from the axioms.
As forconnectives, we limit it to the logical and (∧), logical or(∨), the negation operator (¬), and the implication operator(→).
For the axioms, we use a common set of 14 axiomsused in undergraduate courses, shown in Figure 1 in oursupplementary mater ial (§A).For our study we propose to replace the implication oper-ator (→) with an unrelated, arbitrary symbol (♠).
In orderto produce a signiﬁcant perturbation in the input token dis-tribution, we speciﬁcally select the unicode representationof the symbol (U+2660) for the replacement.
Alternativereplacements are left for future work.
We select two com-mon theorems from propositional calculus extracted from[10], shown in Figure 2 and Figure 3, in §A, and test modelsin two diﬀerent scenarios, as follows.
Full Context (FC)
Our ﬁrst evaluation scheme isintended to simulate a noisy retrieval step, pr ior to the proofgeneration.
Concretely, we oﬀer the model the completeset of axioms together with the selected rule of inference,modus ponens.
Selected Context (SC)
We assume that the rele-vant axioms for the requested proof have already been se-lected by an oracle, and we oﬀer only these axioms andrule of inference to the model input.
For each question, wemanually select the axioms required (Axioms 6, 7, 10 forQuestion 1; Axioms 7, 8 for Question 2).A key point of our study is to ensure that the gener-ated proofs are checked by mathematicians.
Previous workhas stressed the need to rely on experts for evaluation oftheorem proving systems, including [11], who carry onan in-depth annotation where an expert annotator is pre-sented with the theorem, proof-so-far, and a generatednext-step.
[12] also highlight that human evaluation ofadvanced mathematics that approaches research level isexpensive and requires experts.
The evaluation of the out-put of the language model for their work was performed bythe authors, who are all mathematicians.
To perform the evaluation, we concretely rely on onevolunteer (one of the authors of this paper) who has adegree in mathematics.
We design an annotation interfacewhere for each case, we show the annotators the exact inputfed to the model, as well as the output generated.
Below,we list the tasks that we require our annotators to perform.• First, we ask the annotators if the output of the modelcontains a proof or not.•
We present the steps of the correct proof and ask theannotators to judge if each step appears in the outputof the model.
Additionally, if the step invokes anaxiom or rule of inference, we ask them to do thefollowing.
– If the step invokes an axiom, to check if variableswere substituted correctly.
– If the step uses a rule of inference, we ask themto check if the rule is properly invoked.
– To judge whether the step contributes to the proofin the sense that it stirs the overall ﬂow of theproof in the right direction towards conclusion.• With respect to the coherence of the overall text, weask the annotators to rate the output in a scale of 0 to4 via the following labels: “Very Incoherent, Incoher-ent, Neither Coherent nor Incoherent, Coherent, Ver yCoherent”.


4 Results

For this study, we consider the following models:(1) API-based LLMs, including ChatGPT (gpt-3.5-turbo-0125)[13], Claude 3 Opus (claude-3-opus-20240229)[14], (2) Open-weights models, including Llama 3 (Meta-Llama-3-8B-Instruct)[15, 16], Llama 3.1 (Llama-3.1-8B-Instruct)[17] and Gemma 2 (gemma-2-9b-it)[18].
TheTable 1
Summary of the results of our human evaluation study, where Ctx. is short for context.
Bold numbers indicate the best scorefor each pair of (→, ♠) prompts for a given model, and we highlight the best score of all across the FC and SC scenarios for each model.
Model Ctx.
Compliance Extensiveness Correctness Flow Coherence→ ♠
→ ♠ → ♠ → ♠ → ♠ChatGPTSC 100% 100% 62.38% 49.52% 24.29% 5.71% 24.76% 17.14% 2.67 2.33FC 100% 100% 36.19% 30.48% 7.14% 4.76% 20.95% 12.86% 3.00 2.60Claude 3 OpusSC 100% 100% 95.24% 93.14% 76.19% 60.57% 80.95% 70.29% 3.83 3.40FC 100% 100% 85.71% 75.71% 60.95% 27.62% 71.43% 41.43% 3.67 3.00Gemma 2 (9B)SC 33.33% 0% 11.90% - 0% - 2.38% - 4.00 -FC 50.00% 0% 16.67% - 0% - 2.38% - 4.00 -Llama 3 (8B)SC 100% 83.33% 49.52% 41.43% 2.38% 2.38% 11.90% 4.76% 1.83 1.80FC 100% 83.33% 35.71% 21.90% 9.05% 2.38% 11.43% 0% 2.17 1.60Llama 3.1 (8B)SC 100% 100% 42.38% 40.95% 0% 2.38% 4.76% 5.71% 1.17 1.17FC 100% 100% 32.38% 24.29% 3.33% 0% 5.71% 4.76% 2.00 1.50latter models are obtained from HuggingFace, and quan-tized to 4-bits [19] to ﬁt our GPU memory.
For each input,we obtain 3 outputs from each model using a diﬀerent ran-dom seed.
We compute the following metrics to summarizemodel behavior.• Percentage of times the model generated output thatcontains a proof, which we consider a measure ofmodel compliance to our instruction (Compliance).• Percentage of steps from the actual proof that appearin the generated proof, or to which extent it used theneeded axioms and modus ponens (Extensiveness).• Percentage of steps that appear in the generated proofand were correctly applied.
In other words, the stepsthat are correct (Correctness).• Percentage of steps that appear in the output and pro-vide a expression which is a step towards ﬁnalizingthe proof, even if it was not correctly deduced (Flow).• Average coherence score reported for the overall textoutput from a given model (Coherence).Table 1 summarizes the results of our evaluation eﬀorts.
We can clearly see that most models (with the exceptionof Gemma 2) show decreased performance across all met-rics when prompted with (♠).
Furthermore, most models(with the exception of Llama 3.1) showed decreased per-formance in the Full Context compared to the SelectedContext.
Most interestingly, the score for Flow was gener-ally much higher than the Correctness, indicating that themodel ’remembers’ the right answer, even if it does not’remember’ how to deduce it.
The model with the best performance across all measureswas Claude 3 Opus.
Like for most models, its Extensive-ness measure was subject to a decrease when one comparesthe easiest scenario (SC with →) to the hardest scenario(FC with ♠), but its lowest value (75.71%) was still higherthan the highest Extensiveness measure of any other model.
However, it did experience a sharp decrease in both Cor-rectness (from 76.19% to 27.62%) and Flow (from 80.95%to 41.43%), the largest diﬀerent performance of any of themeasures.
As table shows, there is also a big diﬀerence inperformance between the API-based LLMs and the Open-weights models.
The Gemma 2 model refused to providea proof in most case, as it can be seeing by its Compliancemeasure, making it impossible to draw conclusions aboutits abilities.
We think this suggests open-weights modelsare signiﬁcantly behind APIs, which is well-aligned withperformance measured in popular automatic benchmarks.


5 Conclusions

This paper studies the generalization capabilities of LLMthrough the lens of mathematical reasoning.
We performan in-depth human evaluation of the output of LLMs whenthey are prompted to produce basic proofs in propositionalcalculus, comparing their answers when we replace theimplication operator (→) with an unrelated, arbitrary sym-bol (♠).
Our results show that nearly all our tested modelsproduce lower quality proofs in this test, in particular open-weights models, suggesting the abilities of these LLMs toreason in this context have important limitations.
For fu-ture work we would like to extend this study to incorporatemore proofs, models, and multiple annotators.
We wouldalso like to analyze how models react to other input per-turbations, for example using other replacement symbols,and/or alternative representations for them.



References


[1] Edward A Feigenbaum, et al. Computers and thought. Mc-Graw‐Hill, 1963.
[2] Daniel G Bobrow. Natural language input for a computer problemsolving system. AI Technical Reports, 1964.
[3] John Harrison, Josef Urban, and Freek Wiedijk. History of Interac-tive Theorem Proving. In Handbook of the History of Logic,Vol. 9, pp. 135–214. Elsevier, 2014.
[4] Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi,Yejin Choi, and Kyunghyun Cho. Naturalproofs: Mathematicaltheorem proving in natural language. In Thirty-ﬁfth Conferenceon Neural Information Processing Systems (NeurIPS)Datasets and Benchmarks Track, 2021.
[5] Miltiadis Allamanis, Pankajan Chanthirasegaran, Pushmeet Kohli,and Charles Sutton. Learning Continuous Semantic Representa-tions of Symbolic Expressions. In Proceedings of the 34thInternational Conference on Machine Learning, pp. 80–88. PMLR, July 2017.
[6] Richard Evans, David Saxton, David Amos, Pushmeet Kohli, andEdward Grefenstette. Can Neural Networks Understand LogicalEntailment? In International Conference on Learning Rep-resentations, February 2018.
[7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton,Reiichiro Nakano, Christopher Hesse, and John Schulman. Train-ing veriﬁers to solve math word problems. arXiv preprintarXiv:2110.14168, 2021.
[8] David Rein, Betty Li Hou, Asa Cooper Stickland, JacksonPetty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, andSamuel R. Bowman. GPQA: A Graduate-Level Google-ProofQ&A Benchmark, November 2023.
[9] Propositional Logic | Internet Encyclopedia of Philosophy.
[10] Dino Rossegger. Introduction to Mathematical Logic. 2019.
[11] Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi,and Yejin Choi. Naturalprover: Grounded mathematical proofgeneration with language models. In Advances in Neural In-formation Processing Systems (NeurIPS), 2022.
[12] Simon Frieder, Martin Trimmel, Rashid Alawadhi, and Klaus Gy.LLM vs ITP. In The 3rd Workshop on Mathematical Rea-soning and AI at NeurIPS’23, October 2023.
[13] Greg Brockman, Atty Eleti, Elie Georges, Joanne Jang, LoganKilpatrick, Rachel Lim, Luke Miller, and Michelle Pokrass. Intro-ducing chatgpt and whisper apis. OpenAI Blog, 2023.
[14] Anthropic. Introducing claude. Anthropic, 2023.
[15] Aaron Grattaﬁori and the Llama 3 Team. The llama 3 herd ofmodels, 2024.
[16] AI@Meta. Llama 3 model card. 2024.
[17] Introducing Llama 3.1: Our most capable models to date.https://ai.meta.com/blog/meta-llama-3-1/.
[18] Gemma Team. Gemma 2: Improving open language models at apractical size, 2024.
[19] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettle-moyer. Qlora: Eﬃcient ﬁnetuning of quantized llms. In A. Oh,T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,editors, Advances in Neural Information Processing Sys-tems, Vol. 36, pp. 10088–10115. Curran Associates, Inc., 2023.



A Appendix

Prompt (portion)(Ax 1)(( 𝜙 ∧ 𝜓) → 𝜙)(Ax 2)(( 𝜙 ∧ 𝜓) → 𝜓)(Ax 3)( 𝜙 → ( 𝜓 → ( 𝜙 ∧ 𝜓 )))(Ax 4)( 𝜙 → ( 𝜙 ∨ 𝜓 ))(Ax 5)( 𝜙 → ( 𝜓 ∨ 𝜙))(Ax 6)(( 𝜙 → 𝜒) → (( 𝜓 → 𝜒) → (( 𝜙 ∨ 𝜓) → 𝜒)))(Ax 7)( 𝜙 → ( 𝜓 → 𝜙))(Ax 8)(( 𝜙 → ( 𝜓 → 𝜒))
→ (( 𝜙 → 𝜓) → ( 𝜙 → 𝜒)))(Ax 9)(( 𝜙 → 𝜓) → (( 𝜙 → ¬𝜓) → ¬𝜙))(Ax 10)(¬𝜙 → ( 𝜙 → 𝜓 ))
(Ax 11)( 𝜙 ∨ ¬𝜙)(Ax 12)(( 𝜙 ∧ ¬𝜙) → 𝜓)(Ax 13)(( 𝜙 → ( 𝜓 ∧ ¬𝜓))
→ ¬ 𝜙 )(Ax 14)(¬¬𝜙 → 𝜙)(Modus Ponens){ 𝑃 → 𝑄, 𝑃} ` 𝑄Figure 1 Portion of the prompt provided to the LLMs showingthe content of the full context provided, namely, the axioms andrules of inference we allow the models to use.
QuestionQuestion: Prove that ` ((¬𝑃 ∨ 𝑄) → ( 𝑃 → 𝑄)).Answer:((¬𝑃 → ( 𝑃 → 𝑄))
→ ((𝑄 → ( 𝑃 → 𝑄))
→((¬𝑃 ∨ 𝑄) → ( 𝑃 → 𝑄))))(Ax 6)(¬𝑃 → ( 𝑃 → 𝑄))(Ax 10)((𝑄 → ( 𝑃 → 𝑄)) → ((¬𝑃 ∨ 𝑄) → ( 𝑃 → 𝑄)) )
1, 2 MP(𝑄 → ( 𝑃 → 𝑄))(Ax 7)((¬𝑃 ∨ 𝑄) → ( 𝑃 → 𝑄))
3, 4 MPFigure 2 Details of the ﬁrst question (Question 1) utilized forour study, also showing the actual proof which we allow ourannotators to see.
QuestionQuestion: Prove that { ( 𝑃 → 𝑄), (𝑄 → 𝑅) } ` (𝑃 → 𝑅).Answer:(𝑃 → 𝑄)(hyp)(𝑄 → 𝑅)(hyp)((𝑄 → 𝑅) → (𝑃 → (𝑄 → 𝑅)))(Ax 7)(𝑃 → (𝑄 → 𝑅)) 2, 3 MP((𝑃 → (𝑄 → 𝑅)) → (( 𝑃 → 𝑄) → ( 𝑃 → 𝑅)) )
(Ax 8)((𝑃 → 𝑄) → ( 𝑃 → 𝑅)) 4, 5 MP(𝑃 → 𝑅) 1, 6 MPFigure 3 Details of the second question (Question 2) utilizedfor our study, also showing the actual proof which we allow ourannotators to see.