Evaluating Large Language Models in Mongolian

Dorjnyam Tumur-Ochir



 



Fei Cheng



 



Yugo Murawaki



 



Chenhui Chu



Kyoto University



dorjnyam@nlp.ist.i.kyoto-u.ac.jp



{feicheng, murawaki, chu}@i.kyoto-u.ac.jp



Abstract

This paper presents a comprehensive evaluation for as-sessing large language model (LLM) capabilities in theMongolian language, addressing a critical gap in multilin-gual LLM evaluation.
We introduce MonMLU, a novelbenchmark derived from native-level university entranceexams, alongside Mongolian adaptations of establishedbenchmarks including Vicuna, MT-Bench, MGSM, andXCOPA.
Our evaluation of leading commercial and openlyavailable models reveals that while GPT-4o-mini achievesthe highest performance (8.86 on Vicuna, 8.10 on MT-Bench), openly available models signiﬁcantly underper-form.
These ﬁndings highlight future opportunities forimproving LLM performance in Mongolian and other low-resource languages.


1 Introduction

The advancements in large language models (LLMs)have been especially signiﬁcant in high-resource languagessuch as English and Chinese [1, 2].
These modelshave demonstrated exceptional capabilities in text genera-tion, classiﬁcation, understanding, and reasoning tasks
[3].While many state-of-the-art models are multilingual, theireﬀectiveness in low-resource languages often lags due tolimited training data and benchmarks.
Multilingual bench-marks such as MMMLU and MGSM have been developedto address these gaps by extending the original Englishbenchmarks to various languages [4, 5].The Mongolian language, with over 6 million speakersacross Mongolia and Inner Mongolia (China), represents asigniﬁcant yet underserved language
[6].
Digital resourcesin Mongolian comprise less than 0.02% of Common Crawldata, highlighting the substantial disparity in representationcompared to high-resource languages.
This underrepre-sentation necessitates dedicated evaluation frameworks toassess and advance LLM capabilities in Mongolian.
To address the resource gap in Mongolian, we developand release MonMLU,1）a multi-subject multiple-choicelanguage understanding benchmark based on native en-trance examination materials in Mongolian.
We adapt thewidely used English benchmarks such as Vicuna and MT-Bench [7] to evaluate Mongolian rapidly.
We show thatcurrent models can generate intelligible responses and fol-low simple instructions in Mongolian.
In addition, to mea-sure reasoning and understanding abilities, MGSM [5] andXCOPA[8] are adapted.
The main contributions of our work include:• MonMLU, a comprehensive benchmark based on na-tive Mongolian university entrance exams, testing cul-tural and linguistic knowledge across eight subjects.• Mongolian-speciﬁc versions of established bench-marks (Vicuna, MT-Bench, MGSM, XCOPA)through translation and localization.•
We evaluate leading commercial and open-sourceLLMs, providing a quantitative assessment of theirMongolian language capabilities.


2 Related Work



2.1 Monolingual benchmarks

Monolingual benchmarks play a crucial role in assessingthe performance of large language models (LLMs) withina speciﬁc language.
Benchmarks such as MMLU, MT-Bench, and GSM8k are widely used to evaluate LLMsfor reasoning, conversational ability, and mathematicalproblem-solving tasks.
These benchmarks are designed totest models on diverse, challenging tasks and have signiﬁ-cantly advanced the capabilities of LLMs for high-resourcelanguages like English.1）
https://huggingface.co/ku-nlp/monmluFor instance, MMLU assesses knowledge across 57 sub-jects, including humanities, sciences, and general knowl-edge.
MT-Bench evaluates models multi-turn conver-sation abilities, ensuring context and coherence across in-teractions are maintained.
Similarly, GSM8k focuses ongrade school math word problems, testing models math-ematical reasoning and problem-solving skills.
However, such comprehensive benchmarks have notbeen developed for low-resource languages like Mongo-lian.
The lack of Mongolian benchmarks limits the abilityto evaluate models comprehensively and hinders progressin improving LLMs for the language.
Developing tai-lored benchmarks for the Mongolian language is criticalfor understanding and enhancing model performance inthis context.


2.2 Multilingual benchmarks

Eﬀorts to extend evaluations beyond high-resource lan-guages have led to the development of multilingual bench-marks.
These benchmarks aim to assess LLM performanceacross a wide range of languages, providing insights intomultilingual and cross-lingual capabilities.
Key examplesinclude:• MMMLU (Multilingual MMLU):
An extension ofMMLU, MMMLU provides evaluations in 14 lan-guages by translating the original English dataset.• XCOPA (Cross-lingual Choice of Plausible Alter-natives): Derived from the original English COPAbenchmark, XCOPA evaluates commonsense reason-ing in multiple languages by asking models to selectthe most plausible outcome or cause of a given sce-nario.• MGSM (Multilingual Grade School Math): This is amultilingual adaptation of the GSM8k dataset, con-taining grade school-level math problems translatedinto various languages.
MGSM assesses the ability tosolve basic mathematical problems within a multilin-gual context.
These multilingual benchmarks have signiﬁcantly broad-ened the scope of LLM evaluations.
Nevertheless, Mon-golian is often underrepresented, or even absent, in manyexisting multilingual benchmarks.
This underscores theneed for targeted evaluation datasets that reﬂect the lin-guistic and cultural speciﬁcs of the language.


2.3 Low-resource language bench-



marks

Low-resource languages are frequently incorporated intomassively multilingual benchmarks such as SIB-200 [9]and Taxi-200
[10].
However, massively multilingualbenchmarks often prioritize breadth of language coverageover data quality, particularly for low-resource languages.
Furthermore, they are often limited to classiﬁcation tasks,neglecting other important and diverse tasks such as rea-soning and generation.


3 Mongolian Language

Mongolian, a member of the Mongolic language fam-ily, presents a signiﬁcant yet underrepresented languagein natural language processing.
It is primarily spoken innot just Mongolia, but also in some parts of China (In-ner Mongolia) and Russia, with approximately 5-6 millionspeakers.
The language employs diverse writing systems,including the traditional Mongolian and Cyrillic scripts.
A Romanized transliteration of the Cyrillic script is alsocommon in informal online communication.
This studyfocuses on Mongolian written in Cyrillic, the most preva-lent form.
Mongolian presents unique challenges due to itscomplex linguistic features such as agglutinative morphol-ogy and vowel harmony.
These linguistic characteristics,coupled with limited digital resources, make Mongolian acompelling case study for enhancing LLM performance inlow-resource language settings.


4 Benchmarks

To overcome the scarcity of evaluation resources forMongolian, we employ a two-pronged approach: adapt-ing existing multilingual benchmarks and creating novelbenchmarks tailored speciﬁcally for the language.
Thesebenchmarks comprehensively evaluate Natural LanguageUnderstanding (NLU) and Natural Language Generation(NLG) tasks.
This section introduces and discusses thefollowing benchmarks: Mongolian Vicuna and MT-Bench,MonGSM and MonCOPA, and MonMLU.
See AppendixA.2 for samples.


4.1 Adapting existing benchmarks

To adapt Vicuna, MT-Bench, MGSM, and XCOPAfor Mongolian, we followed a machine translation, post-editing, and localization approach:Table 1: Summary of benchmarks.
Benchmark Description Task type Size Construction methodMongolian Vicuna Single-turn dialogue NLG 80 TranslationMongolian MT-Bench Multi-turn dialogue NLG 80 TranslationMonGSM Math word problems NLU (Reasoning) 250 TranslationMonCOPA Commonsense reasoning NLU (Reasoning) 500 TranslationMonMLU General knowledge NLU (Knowledge) 400 Entrance Examination Collection1.
Machine translation: We used GPT-4o to translatethe original English benchmarks into Mongolian.2.
Manual post-editing: A native Mongolian speakermanually edited the translations to ensure linguisticaccuracy and cultural appropr iateness.3.
Localization: Nouns and named entities such as per-sons and places were replaced by Mongolian namesto make their context more native.
For example, robeto дээл, Hawaii to Х ө всг ө л.
Mongolian Vicuna and MT-Bench To quicklyevaluate models in the Mongolian language, we adaptedthe widely used Vicuna and MT-Bench, which are widelyused benchmarks for evaluating conversational abilities inLLMs.
These benchmarks assess the capacity to generatecoherent, contextually relevant, and logically consistent re-sponses in single-turn and multi-turn dialogues.
MonGSM and MonCOPA MonGSM is a Mongo-lian adaptation of MGSM dataset, which consists of gradeschool-level math word problems.
These problems test ba-sic arithmetic, logical reasoning, and problem-solving abil-ities.
XCOPA is a benchmark for evaluating models’ abilityto transfer commonsense reasoning across languages.
These benchmarks highlight the reasoning capabilitiesof models in Mongolian and oﬀer a comparative perspec-tive on their performance relative to other languages.


4.2 Constructing native MonMLU

MonMLU is a benchmark speciﬁcally designed to eval-uate general knowledge and reasoning abilities in the Mon-golian language.
We obtained test questions from theMongolian General Entrance Examination and receivedpermission from the relevant institution,2）making it a re-liable source for testing models on diverse subjects.
Itcovers eight subjects: Mongolian language, Mongolianhistory, Biology, Chemistry, Physics, and Social Science.
Each subject contains 50 multiple-choice questions with2）
https://eec.mn/ﬁve choices.
We obtained exam questions and answers, publicly avail-able on 2）, in PDF format.
We manually extract the textfrom the exams, excluding images and tables to focus onunimodal models.
MonMLU evaluates models on the accuracy of re-sponses, the depth of knowledge across various subjects,and the ability to understand complex queries and providecontextually correct answers.
Table1 shows the summary of all benchmarks.
Thesebenchmarks collectively provide a comprehensive evalua-tion suite for the Mongolian language, covering conversa-tional skills, reasoning, and general knowledge.


5 Evaluation



5.1 Settings

Models For our evaluation, we selected the leadingcommercial and openly available models.
For commer-cial models, we use a family of OpenAI models, namelyGPT-3.5-Turbo, GPT-4-Turbo, and GPT-4o-mini.3）Openlyavailable models are Gemma-2-9B and 27B,4）Llama-3.18B,5）Mistral 7B,6）EMMA-500,7）and
Qwen2.5 7B.8）Wealso include versions of Gemma-2-9B and Llama-3.1-8B,Gemma-2-9B-Alpaca-MN and Llama-3.1-8B-Alpaca-MNrespectively, ﬁne-tuned on the original Alpaca instructiondataset that is machine-translated to Mongolian.9）Evaluation method We implemented a consistentevaluation protocol across all benchmarks.
For reasoning3） https://platform.openai.com/docs/models4） https://huggingface.co/google/gemma-2-9b-it5） https://huggingface.co/meta-llama/Llama-3.1-8B6）
https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.37）
https://huggingface.co/MaLA-LM/emma-500-llama2-7b8） https://huggingface.co/Qwen/Qwen2.5-7B-Instruct9）
https://huggingface.co/datasets/saillab/alpaca-mongolian-cleanedTable 2: Performance of various models across NLG and NLU tasks.
NLG NLUVicuna
MT-Bench MonGSM MonMLU MonCOPAGPT-4o-mini 8.86 8.10 68.0 52.3 61.0GPT-4-Turbo 8.49 7.89 74.4 51.1 59.8GPT-3.5-Turbo 2.95 3.22 17.6 18.0 15.3Gemma-2-27B-IT 7.01 6.50 59.2 43.7 49.6Gemma-2-9B-IT 4.22 3.96 46.4 32.2 38.1Gemma-2-9B-Alpaca-MN 5.95 4.76 14.0 21.6 23.4Llama-3.1-8B-Instruct 1.98 2.40 34.0 21.1 23.5Llama-3.1-8B-Alpaca-MN 5.47 3.99 4.0 17.5 16.7Mistral-7B-Instruct-v0.3 1.38 1.10 4.0 0.0 10.3EMMA-500-llama2-7b 3.96 2.54 0.0 4.0 0.0Qwen-2.5-7B-Instruct 2.50 2.64 12.4 23.1 15.6tasks such as MonGSM, we employed a zero-shot chain-of-thought setting to assess natural problem-solving ca-pabilities.
Conversational tasks, Vicuna and MT-Bench,were evaluated using GPT-4o as an independent judge asproposed by [11].
Multiple-choice questions were scoredusing exact match criteria.


5.2 Results

As shown in Table 2, our evaluation revealed signiﬁcantperformance variations across models and tasks.
Amongcommercial models, GPT-4o-mini demonstrated superiorperformance in both NLG and NLU tasks, achieving scoresof 8.86 on Vicuna and 52.3 on MonMLU.
GPT-4-Turbofollowed closely with strong results of 8.49 on Vicunaand 51.1 on MonMLU.
However, GPT-3.5-Turbo showednotably lower per formance, scoring only 2.95 on Vicunaand 18.0 on MonMLU.WritingRoleplayReasoningMathCodingExtractionSTEMHumanities0 1 2 3 4 5 6 7 8 9modelQwen2.5-7B-InstructEMMA-500-llama2-7bGemma-2-9b-Alpaca-MLlama-3.1-8B-Alpaca-MGemma-2-9B-ITLlama3.1-8B-InstructGPT-4o-miniFigure 1: Category-wise scores of various models on Mon-golian Vicuna.
In the openly available category, Gemma-2-27B-ITachieved the strongest results among non-commercial mod-els, scoring 7.01 on Vicuna and 6.50 on MT-Bench.
The9B variant also shows reasonable performance.
Smallermodels demonstrated limited eﬀectiveness, as evidencedby Mistral-7B and Llama-3.1-8B-Instruct.
As illustrated by Figures 1 and 2, ﬁne-tuned models onAlpaca, most notably Llama-3.1-8B, show considerableperformance improvement in generation tasks in categoriessuch as writing and roleplay.
At the same time, they losetheir performance in categories such as extraction, coding,and math.
However, we can see a huge performance dropin NLU tasks.
This may be due to a lack of task-speciﬁctraining data as Alpaca contains mostly generic instr uc-tions.


6 Conclusion

In this work, we introduced a set of benchmarks forthe Mongolian language.
Four benchmarks were adaptedfrom widely used English and multilingual benchmarks in-cluding MT-Bench and MGSM.
MonMLU, a multi-subjectmultiple-choice benchmark based on native examinationmaterial, was constructed.
We evaluated various types ofmodels on these newly constructed benchmarks.
Openlyavailable models lag behind commercial models consid-erably.
Also, simply ﬁne-tuning models with additionalMongolian instruction data, while improving text coher-ence in generated output, does not enhance understandingand reasoning in the Mongolian language.
In future work,we plan to explore methods aimed at simultaneously im-proving both generative and understanding capabilities.



Acknowledgment

This work was supported by JSPS KAKENHI GrantNumber JP23K28144.

References


[1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timothée Lacroix, Bap-tiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar,Aurelien Rodriguez, Armand Joulin, Edouard Grave, andGuillaume Lample. Llama: Open and eﬃcient foundationlanguage models, 2023.
[2] Qwen, :, An Yang, Baosong Yang, Beichen Zhang,Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Day-iheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang,Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang,Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Ke-qin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, PeiZhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, TingyuXia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su,Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, ZhenruZhang, and Zihan Qiu. Qwen2.5 technical report, 2024.
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-lakantan, Pranav Shyam, Girish Sastry, Amanda Askell,Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,Tom Henighan, Rewon Child, Aditya Ramesh, DanielZiegler, Jeﬀrey Wu, Clemens Winter, Chris Hesse, MarkChen, Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam McCandlish,Alec Radford, Ilya Sutskever, and Dario Amodei. Lan-guage models are few-shot learners. In H. Larochelle,M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors,Advances in Neural Information Processing Sys-tems, Vol. 33, pp. 1877–1901. Curran Associates, Inc.,2020.
[4] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Mea-suring massive multitask language understanding, 2020.
[5] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,Suraj Srivats, Soroush Vosoughi, Hyung Won Chung,Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, andJason Wei. Language models are multilingual chain-of-thought reasoners, 2022.
[6] Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali,and Monojit Choudhury. The state and fate of linguisticdiversity and inclusion in the NLP world. In Dan Ju-rafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault,editors, Proceedings of the 58th Annual Meetingof the Association for Computational Linguistics,pp. 6282–6293, Online, July 2020. Association for Com-putational Linguistics.
[7] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuo-han Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E.Gonzalez, and Ion Stoica. Judging llm-as-a-judge withmt-bench and chatbot arena, 2023.
[8] Edoardo Maria Ponti, Goran Glavaš, Olga Majewska,Qianchu Liu, Ivan Vulić, and Anna Korhonen. XCOPA:A multilingual dataset for causal commonsense reason-ing. In Bonnie Webber, Trevor Cohn, Yulan He, and YangLiu, editors, Proceedings of the 2020 Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP), pp. 2362–2376, Online, November2020. Association for Computational Linguistics.
[9] David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, NikitaVassilyev, Jesujoba O. Alabi, Yanke Mao, Haonan Gao,and Annie En-Shiun Lee. Sib-200: A simple, inclusive,and big evaluation dataset for topic classiﬁcation in 200+languages and dialects, 2024.
[10] Chunlan Ma, Ayyoob ImaniGooghari, Haotian Ye, RenhaoPei, Ehsaneddin Asgari, and Hinrich Schütze. Taxi1500:A multilingual dataset for text classiﬁcation in 1500 lan-guages, 2024.
[11] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuo-han Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E.Gonzalez, and Ion Stoica. Judging llm-as-a-judge withmt-bench and chatbot arena, 2023.



A Appendix



A.1 MT-Bench

WritingRoleplayReasoningMathCodingExtractionSTEMHumanities0 1 2 3 4 5 6 7 8modelQwen2.5-7B-InstructEMMA-500-llama2-7bGemma-2-9b-Alpaca-MLlama-3.1-8B-Alpaca-MGemma-2-9B-ITLlama-3.1-8B-InstructGPT-4o-miniFigure 2: Category-wise scores of various models on Mon-golian MT-Bench.


A.2 Samples

Figure 3: A sample instruction and response pair of GPT-4o-mini on Mongolian Vicuna.
Figure 4: A sample from MT-Bench.
Words in red arelocalized words.
Figure 5: A sample from MonGSM.
Words in red arelocalized words.
Figure 6: A sample from MonMLU.
The correct answer isin bold.