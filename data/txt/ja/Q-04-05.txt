疎なパラメータを用いて大規模言語モデルを効率よく Fine-Tune する手法の提案

 原田慎太朗  山崎智弘  吉田尚水



  株式会社東芝 研究開発センター



 {shintaro2.harada,tomohiro2.yamasaki,takami.yoshida}@toshiba.co.jp



概要

本稿では、疎なパラメータを用いて大規模言語モデルの部分的更新を行なうことで効率よく Fine-Tuneする手法を提案する。
大規模なコーパスで学習された事前学習済みの言語モデルを特定のタスクやドメインに対して Fine-Tune するには、事前学習済みモデルのパラメータを固定し、少量の追加パラメータのみを学習する手法(Parameter-Eﬃcient Fine-Tuning,PEFT)が主流である。
しかし、通常の PEFT は学習時の計算リソースを抑える一方でパラメータをすべて更新するため、知識保持および編集が重要なタスクやドメインの性能が劣化したりモデル解釈性が失われたりする問題がある。
そこで本稿では、疎なパラメータを用いることで計算リソースを抑えつつ、新たにモデル解釈性も備えた新しい PEFT を提案する。
事前学習済みモデルとして RoBERTa とLlama3-8B に適用することで、言語理解と算術推論ベンチマークで性能が向上すること、およびモデル解釈が可能であることを示す。
1 はじめに深層学習を用いた言語モデルは大規模化が進んでいる。
そのため、特定のタスクやドメインに対してモデルのパラメータをすべて更新する場合、大規模な計算リソースが必要である。
例えば、更新するパラメータ数を 𝜙 としたとき、モデルの状態を 16bit、Optimizer の状態を 32bit で保持しようとすると、16𝜙byte の計算リソースが必要である1）。
8B サイズのLlama3[1]の場合、16 × 8𝐵 = 128 Gbyte 必要である。
この問題に対しては、更新するパラメータ数 𝜙を減らして効率よく Fine-Tune することができるPEFT(Parameter-Eﬃcient Fine-Tuning)を用いることが主流である。
PEFT は大きく Soft-Prompt 型[2, 3, 4, 5]1） https://huggingface.co/blog/Isayoften/optimization-rushFrozen Parameters×Frozen Parameters×××LoRAOurs: Frozen: Trainable ×××図 1 提案手法の概要図。
左に示す先行研究ではすべてのパラメータを上書きしてしまうが、右に示す提案手法では特定のパラメータ（ピンクの部分）のみを更新できる。
右の図は、パラメータを行で 𝑛 = 4 個、列で 𝑚 = 4 個の計 16 個のブロックに切り分け、𝑊𝑝∈ ℝ𝑛×𝑚の中で重要な 𝑘 = 4 個のブロック位置を特定し、そこにブロックパラメータ 𝑊𝑣∈ ℝ𝑘×𝑑/𝑛×𝑑/𝑚を挿入して、疎なパラメータを構築した状態を表している。
と
Adapter 型[6, 7, 8, 9, 10, 11, 12, 13]に分けられる。
本稿で注目する Adapter 型では、ベースモデルのパラメータを固定し、少数の追加パラメータのみを更新することでパラメータ数 𝜙 を減らしている。
その中には、行列分解[6, 7, 11]や特殊な操作[12, 13]によりパラメータ数 𝜙 を減らすものがある。
これらは追加したパラメータを学習後に元のパラメータに統合できるという特徴があり、追加パラメータの計算による推論速度の低下を避けることができる。
一方これらの PEFT は、対象とするモジュールのパラメータをすべて更新してしまう。
先行研究[14]でも言及されているとおり、特定のタスクおよびドメインに寄与するパラメータは限定的であり、パラメータをすべて更新してしまうと、保持したい事前知識が失われたり、特定の事前知識の編集が困難になったりする。
すなわち、(Q1)事前学習による優れた性能と(Q2)モデル解釈性が失われると考えられる。
これらの課題に対して、先行研究[14]では重要

疎なパラメータを用いて大規模言語モデルを効率よく Fine-Tune する手法の提案

 原田慎太朗  山崎智弘  吉田尚水



  株式会社東芝 研究開発センター



 {shintaro2.harada,tomohiro2.yamasaki,takami.yoshida}@toshiba.co.jp



概要

本稿では、疎なパラメータを用いて大規模言語モデルの部分的更新を行なうことで効率よく Fine-Tuneする手法を提案する。
大規模なコーパスで学習された事前学習済みの言語モデルを特定のタスクやドメインに対して Fine-Tune するには、事前学習済みモデルのパラメータを固定し、少量の追加パラメータのみを学習する手法(Parameter-Eﬃcient Fine-Tuning,PEFT)が主流である。
しかし、通常の PEFT は学習時の計算リソースを抑える一方でパラメータをすべて更新するため、知識保持および編集が重要なタスクやドメインの性能が劣化したりモデル解釈性が失われたりする問題がある。
そこで本稿では、疎なパラメータを用いることで計算リソースを抑えつつ、新たにモデル解釈性も備えた新しい PEFT を提案する。
事前学習済みモデルとして RoBERTa とLlama3-8B に適用することで、言語理解と算術推論ベンチマークで性能が向上すること、およびモデル解釈が可能であることを示す。
1 はじめに深層学習を用いた言語モデルは大規模化が進んでいる。
そのため、特定のタスクやドメインに対してモデルのパラメータをすべて更新する場合、大規模な計算リソースが必要である。
例えば、更新するパラメータ数を 𝜙 としたとき、モデルの状態を 16bit、Optimizer の状態を 32bit で保持しようとすると、16𝜙byte の計算リソースが必要である1）。
8B サイズのLlama3[1]の場合、16 × 8𝐵 = 128 Gbyte 必要である。
この問題に対しては、更新するパラメータ数 𝜙を減らして効率よく Fine-Tune することができるPEFT(Parameter-Eﬃcient Fine-Tuning)を用いることが主流である。
PEFT は大きく Soft-Prompt 型[2, 3, 4, 5]1） https://huggingface.co/blog/Isayoften/optimization-rushFrozen Parameters×Frozen Parameters×××LoRAOurs: Frozen: Trainable ×××図 1 提案手法の概要図。
左に示す先行研究ではすべてのパラメータを上書きしてしまうが、右に示す提案手法では特定のパラメータ（ピンクの部分）のみを更新できる。
右の図は、パラメータを行で 𝑛 = 4 個、列で 𝑚 = 4 個の計 16 個のブロックに切り分け、𝑊𝑝∈ ℝ𝑛×𝑚の中で重要な 𝑘 = 4 個のブロック位置を特定し、そこにブロックパラメータ 𝑊𝑣∈ ℝ𝑘×𝑑/𝑛×𝑑/𝑚を挿入して、疎なパラメータを構築した状態を表している。
と
Adapter 型[6, 7, 8, 9, 10, 11, 12, 13]に分けられる。
本稿で注目する Adapter 型では、ベースモデルのパラメータを固定し、少数の追加パラメータのみを更新することでパラメータ数 𝜙 を減らしている。
その中には、行列分解[6, 7, 11]や特殊な操作[12, 13]によりパラメータ数 𝜙 を減らすものがある。
これらは追加したパラメータを学習後に元のパラメータに統合できるという特徴があり、追加パラメータの計算による推論速度の低下を避けることができる。
一方これらの PEFT は、対象とするモジュールのパラメータをすべて更新してしまう。
先行研究[14]でも言及されているとおり、特定のタスクおよびドメインに寄与するパラメータは限定的であり、パラメータをすべて更新してしまうと、保持したい事前知識が失われたり、特定の事前知識の編集が困難になったりする。
すなわち、(Q1)事前学習による優れた性能と(Q2)モデル解釈性が失われると考えられる。
これらの課題に対して、先行研究[14]では重要度に基づいて部分的にパラメータを更新する PEFTを提案しているが、強い制約を導入するために複雑な操作を必要としている。
また更新されたパラメータの解釈に関しては無視されている。
本稿では、図 1 に示すような疎のパラメータを用いることで、より単純かつモデル解釈可能な PEFTを提案する。
実験の結果、(A1)モデルサイズに関わらず部分的なパラメータの更新でも性能が向上すること、(A2)学習ドメインに対して重要なパラメータを示せることを確認した。
2 PEFT本節では、大規模モデルを少ない計算リソースでFine-Tune するための PEFT について説明する。
前節で述べたように、更新するモジュールのパラメータ数を 𝜙 とする場合、すべてのパラメータを更新するためには計算リソースが 16𝜙 byte 必要となる。
そこで、Adapter 型の PEFT ではℎ = 𝑊′𝑥 = (𝑊𝑜+ Δ𝑊)𝑥 (1)のように元のモジュールのパラメータ 𝑊𝑜を固定し、少量の追加パラメータ Δ𝑊 のみを更新することで、パラメータ数 𝜙 を減らしている。
ここで、𝑥 は入力ベクトル、ℎ は 𝑥 に対する潜在ベクトル、𝑊𝑜∈ ℝ𝑑×𝑑は対象モジュールのパラメータ、Δ𝑊 ∈ ℝ𝑑×𝑑は追加した学習可能パラメータである。
Δ𝑊 の作り方にはいくつか種類があり、行列分解に基づいて Δ𝑊 = 𝐵 𝐴 とするもの[6]や、特殊な操作に基づいて Δ𝑊 = Op( 𝐴)とするもの[12]などがある。
いずれも推論時は、𝑊′= 𝑊𝑜+ Δ𝑊 とパラメータをあらかじめ統合しておくことで、追加の行列計算による推論速度の低下を避けることができる。
しかし Δ𝑊 は密なパラメータであるため、元のパラメータ 𝑊𝑜が持つ事前知識を上書きしてしまう問題がある。
例えば、ベースの言語モデルを指示調整した後にドメイン学習を行うと、事前および指示調整で獲得した知識を上書きし続けてしまい、知識忘却につながる。
また、図 1 のように、Δ𝑊 が密なパラメータということは、どのパラメータがどのタスクおよびドメインに対する性能に寄与するのか解釈が難しいという問題がある。
3 疎なパラメータを用いた PEFT前述したとおり、密なパラメータを追加して更新する PEFT では、保持したい事前知識が失われたり特定の箇所だけの編集が困難な場合がある。
また、どのパラメータがどのドメインやタスクに対する性能向上に寄与するのか解釈が難しい。
そこで本節では、図 1 に示すとおり、Δ𝑊 = Sparse(p, v𝑊𝑣)(2)のように疎なパラメータを用いることで、必要な計算リソースを抑えつつ特定のパラメータのみを選択して更新できる新しい PEFT を提案する。
後述するように、更新されるパラメータを特定できるのでモデル解釈性も得ることができる。
ここで、Sparse(·)は疎な行列 Δ𝑊 を構築する関数であり、引数として、位置を表す p と値を表す v𝑊𝑣がある。
疎な行列を構築するために必要な p、v、𝑊 は、次の順で導出する。
まず、重要なパラメータの大まかな位置を特定するために、元のパラメータ 𝑊𝑜∈ R𝑑×𝑑を行と列でそれぞれ 𝑛 個と 𝑚 個のブロックに分ける。
次に、𝑛 × 𝑚 個のブロックに対する重要度合いを表す更新可能なパラメータ 𝑊𝑝∈ R𝑛×𝑚を用意し、上位 𝑘 個のブロックの位置とスコアをp, v = TopK(𝑊𝑝)(3)のように取得する。
ここで、TopK(·)は入力 𝑊𝑝において値が上位 𝑘 番目までの位置 p と値 v を返す関数である。
得られた 𝑘 個の位置とスコアに対し、𝑘 個の更新可能なブロックパラメータ 𝑊𝑣∈ ℝ𝑘×𝑑/𝑛×𝑑/𝑚を用意して、v𝑊𝑣= [𝑣1𝑊𝑣1, 𝑣2𝑊𝑣2, ..., 𝑣𝑘𝑊𝑣𝑘](4)のように 𝑘 個のスコアでそれぞれのブロックパラメータ𝑊𝑣𝑖を重みづける。
ここで、𝑣𝑖は正の実数値、𝑊𝑣𝑖∈ ℝ𝑑/𝑛×𝑑/𝑚は選択された 𝑘 個の位置に対する学習可能なブロックパラメータ、v は各ブロックに対するスコアである。
また、ハイパーパラメータ 𝛼 を用いてスケールする[6]。
このように疎なパラメータを用いると、保持すべきパラメータは 𝑊𝑝と 𝑊𝑣だけであり、パラメータ数は 𝜙 = 𝑛 × 𝑚 + 𝑘 × 𝑑/𝑛 × 𝑑/𝑚)と表現できる。
特に、𝑛𝑚 = 𝑑 とすると 𝜙 = (1 + 𝑘)𝑑 となり、元のパラメータ数は 𝑑 の 2 乗であったものが、先行研究の PEFTと同じく 𝑑 の 1 乗に抑えることができる。
さらに、元のパラメータをブロックに分けて、一部のブロックのみ更新できることから、事前知識の上書きを防いで性能の向上することができる。
また、選択した

疎なパラメータを用いて大規模言語モデルを効率よく Fine-Tune する手法の提案

 原田慎太朗  山崎智弘  吉田尚水



  株式会社東芝 研究開発センター



 {shintaro2.harada,tomohiro2.yamasaki,takami.yoshida}@toshiba.co.jp



概要

本稿では、疎なパラメータを用いて大規模言語モデルの部分的更新を行なうことで効率よく Fine-Tuneする手法を提案する。
大規模なコーパスで学習された事前学習済みの言語モデルを特定のタスクやドメインに対して Fine-Tune するには、事前学習済みモデルのパラメータを固定し、少量の追加パラメータのみを学習する手法(Parameter-Eﬃcient Fine-Tuning,PEFT)が主流である。
しかし、通常の PEFT は学習時の計算リソースを抑える一方でパラメータをすべて更新するため、知識保持および編集が重要なタスクやドメインの性能が劣化したりモデル解釈性が失われたりする問題がある。
そこで本稿では、疎なパラメータを用いることで計算リソースを抑えつつ、新たにモデル解釈性も備えた新しい PEFT を提案する。
事前学習済みモデルとして RoBERTa とLlama3-8B に適用することで、言語理解と算術推論ベンチマークで性能が向上すること、およびモデル解釈が可能であることを示す。
1 はじめに深層学習を用いた言語モデルは大規模化が進んでいる。
そのため、特定のタスクやドメインに対してモデルのパラメータをすべて更新する場合、大規模な計算リソースが必要である。
例えば、更新するパラメータ数を 𝜙 としたとき、モデルの状態を 16bit、Optimizer の状態を 32bit で保持しようとすると、16𝜙byte の計算リソースが必要である1）。
8B サイズのLlama3[1]の場合、16 × 8𝐵 = 128 Gbyte 必要である。
この問題に対しては、更新するパラメータ数 𝜙を減らして効率よく Fine-Tune することができるPEFT(Parameter-Eﬃcient Fine-Tuning)を用いることが主流である。
PEFT は大きく Soft-Prompt 型[2, 3, 4, 5]1） https://huggingface.co/blog/Isayoften/optimization-rushFrozen Parameters×Frozen Parameters×××LoRAOurs: Frozen: Trainable ×××図 1 提案手法の概要図。
左に示す先行研究ではすべてのパラメータを上書きしてしまうが、右に示す提案手法では特定のパラメータ（ピンクの部分）のみを更新できる。
右の図は、パラメータを行で 𝑛 = 4 個、列で 𝑚 = 4 個の計 16 個のブロックに切り分け、𝑊𝑝∈ ℝ𝑛×𝑚の中で重要な 𝑘 = 4 個のブロック位置を特定し、そこにブロックパラメータ 𝑊𝑣∈ ℝ𝑘×𝑑/𝑛×𝑑/𝑚を挿入して、疎なパラメータを構築した状態を表している。
と
Adapter 型[6, 7, 8, 9, 10, 11, 12, 13]に分けられる。
本稿で注目する Adapter 型では、ベースモデルのパラメータを固定し、少数の追加パラメータのみを更新することでパラメータ数 𝜙 を減らしている。
その中には、行列分解[6, 7, 11]や特殊な操作[12, 13]によりパラメータ数 𝜙 を減らすものがある。
これらは追加したパラメータを学習後に元のパラメータに統合できるという特徴があり、追加パラメータの計算による推論速度の低下を避けることができる。
一方これらの PEFT は、対象とするモジュールのパラメータをすべて更新してしまう。
先行研究[14]でも言及されているとおり、特定のタスクおよびドメインに寄与するパラメータは限定的であり、パラメータをすべて更新してしまうと、保持したい事前知識が失われたり、特定の事前知識の編集が困難になったりする。
すなわち、(Q1)事前学習による優れた性能と(Q2)モデル解釈性が失われると考えられる。
これらの課題に対して、先行研究[14]では重要度に基づいて部分的にパラメータを更新する PEFTを提案しているが、強い制約を導入するために複雑な操作を必要としている。
また更新されたパラメータの解釈に関しては無視されている。
本稿では、図 1 に示すような疎のパラメータを用いることで、より単純かつモデル解釈可能な PEFTを提案する。
実験の結果、(A1)モデルサイズに関わらず部分的なパラメータの更新でも性能が向上すること、(A2)学習ドメインに対して重要なパラメータを示せることを確認した。
2 PEFT本節では、大規模モデルを少ない計算リソースでFine-Tune するための PEFT について説明する。
前節で述べたように、更新するモジュールのパラメータ数を 𝜙 とする場合、すべてのパラメータを更新するためには計算リソースが 16𝜙 byte 必要となる。
そこで、Adapter 型の PEFT ではℎ = 𝑊′𝑥 = (𝑊𝑜+ Δ𝑊)𝑥 (1)のように元のモジュールのパラメータ 𝑊𝑜を固定し、少量の追加パラメータ Δ𝑊 のみを更新することで、パラメータ数 𝜙 を減らしている。
ここで、𝑥 は入力ベクトル、ℎ は 𝑥 に対する潜在ベクトル、𝑊𝑜∈ ℝ𝑑×𝑑は対象モジュールのパラメータ、Δ𝑊 ∈ ℝ𝑑×𝑑は追加した学習可能パラメータである。
Δ𝑊 の作り方にはいくつか種類があり、行列分解に基づいて Δ𝑊 = 𝐵 𝐴 とするもの[6]や、特殊な操作に基づいて Δ𝑊 = Op( 𝐴)とするもの[12]などがある。
いずれも推論時は、𝑊′= 𝑊𝑜+ Δ𝑊 とパラメータをあらかじめ統合しておくことで、追加の行列計算による推論速度の低下を避けることができる。
しかし Δ𝑊 は密なパラメータであるため、元のパラメータ 𝑊𝑜が持つ事前知識を上書きしてしまう問題がある。
例えば、ベースの言語モデルを指示調整した後にドメイン学習を行うと、事前および指示調整で獲得した知識を上書きし続けてしまい、知識忘却につながる。
また、図 1 のように、Δ𝑊 が密なパラメータということは、どのパラメータがどのタスクおよびドメインに対する性能に寄与するのか解釈が難しいという問題がある。
3 疎なパラメータを用いた PEFT前述したとおり、密なパラメータを追加して更新する PEFT では、保持したい事前知識が失われたり特定の箇所だけの編集が困難な場合がある。
また、どのパラメータがどのドメインやタスクに対する性能向上に寄与するのか解釈が難しい。
そこで本節では、図 1 に示すとおり、Δ𝑊 = Sparse(p, v𝑊𝑣)(2)のように疎なパラメータを用いることで、必要な計算リソースを抑えつつ特定のパラメータのみを選択して更新できる新しい PEFT を提案する。
後述するように、更新されるパラメータを特定できるのでモデル解釈性も得ることができる。
ここで、Sparse(·)は疎な行列 Δ𝑊 を構築する関数であり、引数として、位置を表す p と値を表す v𝑊𝑣がある。
疎な行列を構築するために必要な p、v、𝑊 は、次の順で導出する。
まず、重要なパラメータの大まかな位置を特定するために、元のパラメータ 𝑊𝑜∈ R𝑑×𝑑を行と列でそれぞれ 𝑛 個と 𝑚 個のブロックに分ける。
次に、𝑛 × 𝑚 個のブロックに対する重要度合いを表す更新可能なパラメータ 𝑊𝑝∈ R𝑛×𝑚を用意し、上位 𝑘 個のブロックの位置とスコアをp, v = TopK(𝑊𝑝)(3)のように取得する。
ここで、TopK(·)は入力 𝑊𝑝において値が上位 𝑘 番目までの位置 p と値 v を返す関数である。
得られた 𝑘 個の位置とスコアに対し、𝑘 個の更新可能なブロックパラメータ 𝑊𝑣∈ ℝ𝑘×𝑑/𝑛×𝑑/𝑚を用意して、v𝑊𝑣= [𝑣1𝑊𝑣1, 𝑣2𝑊𝑣2, ..., 𝑣𝑘𝑊𝑣𝑘](4)のように 𝑘 個のスコアでそれぞれのブロックパラメータ𝑊𝑣𝑖を重みづける。
ここで、𝑣𝑖は正の実数値、𝑊𝑣𝑖∈ ℝ𝑑/𝑛×𝑑/𝑚は選択された 𝑘 個の位置に対する学習可能なブロックパラメータ、v は各ブロックに対するスコアである。
また、ハイパーパラメータ 𝛼 を用いてスケールする[6]。
このように疎なパラメータを用いると、保持すべきパラメータは 𝑊𝑝と 𝑊𝑣だけであり、パラメータ数は 𝜙 = 𝑛 × 𝑚 + 𝑘 × 𝑑/𝑛 × 𝑑/𝑚)と表現できる。
特に、𝑛𝑚 = 𝑑 とすると 𝜙 = (1 + 𝑘)𝑑 となり、元のパラメータ数は 𝑑 の 2 乗であったものが、先行研究の PEFTと同じく 𝑑 の 1 乗に抑えることができる。
さらに、元のパラメータをブロックに分けて、一部のブロックのみ更新できることから、事前知識の上書きを防いで性能の向上することができる。
また、選択した表 1 言語理解ベンチマーク(GLEU)における定量評価†：先行研究[6]から引用学習可能パラメータ数 MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B AvgFull† 125.0M 87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2 86.4LoRA 0.9M 87.4 94.4 88.7 60.8 92.8 90.8 79.1 90.8 85.7Ours 0.9M 83.2 94.6 90.0 55.2 92.7 86.5 79 .1 89.9 83.9表 2 算術推論ベンチマークにおける定量評価学習可能パラメータ数 MultiArith GSM8K AddSub AQuA SingleEq SVAMP AvgLoRA 56.6M 68.1 50.7 83.3 35.8 87.2 71.2 66.1Ours 56.3M 67.5 50.1 84.5 36.6 87.9 72.8 66.6ブロックのスコアを観察することでモデル解釈性が高まることが期待される。
先行研究[6, 15]でも述べられている通り、学習の初めは Δ𝑊 が影響を及ぼさないようにすることが重要であるため、𝑊𝑝は正規分布、𝑊𝑣はゼロで初期化する。
4 実験本節では、まず、モデルサイズに関わらず、疎なパラメータ更新でも性能を向上できることを検証する。
その後に、学習データに応じて選択されるパラメータにパターンについて観察する。
4.1 設定疎なパラメータ更新でも性能を向上できることを検証するために、幅広く利用されている密なパラメータ更新の LoRA (Low-Rank Adaptation)[6]と比較する。
ベースのモデルとしては、小さいものとして RoBERTa-125M2）[16]、大きいものとしてLlama3-8B3）[1]を用いた。
ベースラインを先行研究[6]に設定して、正しく比較できるよう学習パラメータ 𝜙 は同じになるように設定した。
RoBERTa-125Mは言語理解ベンチマークの GLEU[17]でそれぞれ学習および評価する。
学習設定は先行研究[6]に従い、独自のハイパーパラメータは 𝑛 = 24、𝑚 = 32、𝑘 = 15、𝛼 = 1.0 に設定した。
Llama3-8B は 8bit で量子化して Math10K で学習し、算術推論のベンチマークの MultiArith、GSM8K、AddSub、AQuA、SingleEq、SVAMP で評価する。
Math10K には GSM8K と AQuAの学習データのみ含まれる。
学習設定は先行研究[18]に従い、独自のハイパーパラメータは、𝑛 = 64、𝑚 = 64、𝑘 = 50、𝛼 = 2.0 に設定した。
また、ブロックパラメータの重要度を示す 𝑊𝑝∈ R𝑛×𝑚に対しては、2） https://huggingface.co/FacebookAI/roberta-base3） https://huggingface.co/meta-llama/Meta-Llama-3-8B学習データのドメインやタスクに応じてパターンが現れるか人手で確認する。
すべての実験で NVIDIAH100 を用い、Llama3-8B には FlashAttention2 [19]を用いた。
4.2 定量評価はじめに表 1 に言語理解ベンチマークでの定量評価の結果を示す。
表 1 からわかるように、密なパラメータを更新を行う先行研究と比較して、疎なパラメータを更新でも SST-2、MRPC、QNLI、RTE のデータセットでより同等以上の性能を示した。
しかし、密なパラメータを更新と比較すると MNLI、CoLA、QQP での性能向上は低く、平均が 2 ポイントほど下回った。
続いて表 2 に算術推論ベンチマークでの定量評価の結果を示す。
表 2 からわかるように、密なパラメータを更新を行う先行研究と比較して、疎なパラメータを更新でも MultiArith と GSM8K 以外のデータセットで同等以上の性能を示した。
平均は 0.5 ポイントほど上回った。
4.3 モデル解釈について提案手法では、学習した後に獲得された 𝑊𝑝∈R𝑛×𝑚に基づいて、学習データに対して選択されたパラメータの位置とスコアを観察できる。
図2 と図 3 にそれぞれ MRPC と CoLA で学習したRoBERTa-125M[16]の self-attention 層の query パラメータに対するブロックスコアを可視化したものを示す。
明るいほどスコアが高く、暗いほどスコアが低いことを示している。
図 2 からわかるように、MRPC では最初の 0 層目に明るいブロックは少なく、学習される 𝑘 = 15 個のブロックで足りている。
最後の 11 層目にも明るいブロックは少なく、学習される 𝑘 = 15 個のブロック

疎なパラメータを用いて大規模言語モデルを効率よく Fine-Tune する手法の提案

 原田慎太朗  山崎智弘  吉田尚水



  株式会社東芝 研究開発センター



 {shintaro2.harada,tomohiro2.yamasaki,takami.yoshida}@toshiba.co.jp



概要

本稿では、疎なパラメータを用いて大規模言語モデルの部分的更新を行なうことで効率よく Fine-Tuneする手法を提案する。
大規模なコーパスで学習された事前学習済みの言語モデルを特定のタスクやドメインに対して Fine-Tune するには、事前学習済みモデルのパラメータを固定し、少量の追加パラメータのみを学習する手法(Parameter-Eﬃcient Fine-Tuning,PEFT)が主流である。
しかし、通常の PEFT は学習時の計算リソースを抑える一方でパラメータをすべて更新するため、知識保持および編集が重要なタスクやドメインの性能が劣化したりモデル解釈性が失われたりする問題がある。
そこで本稿では、疎なパラメータを用いることで計算リソースを抑えつつ、新たにモデル解釈性も備えた新しい PEFT を提案する。
事前学習済みモデルとして RoBERTa とLlama3-8B に適用することで、言語理解と算術推論ベンチマークで性能が向上すること、およびモデル解釈が可能であることを示す。
1 はじめに深層学習を用いた言語モデルは大規模化が進んでいる。
そのため、特定のタスクやドメインに対してモデルのパラメータをすべて更新する場合、大規模な計算リソースが必要である。
例えば、更新するパラメータ数を 𝜙 としたとき、モデルの状態を 16bit、Optimizer の状態を 32bit で保持しようとすると、16𝜙byte の計算リソースが必要である1）。
8B サイズのLlama3[1]の場合、16 × 8𝐵 = 128 Gbyte 必要である。
この問題に対しては、更新するパラメータ数 𝜙を減らして効率よく Fine-Tune することができるPEFT(Parameter-Eﬃcient Fine-Tuning)を用いることが主流である。
PEFT は大きく Soft-Prompt 型[2, 3, 4, 5]1） https://huggingface.co/blog/Isayoften/optimization-rushFrozen Parameters×Frozen Parameters×××LoRAOurs: Frozen: Trainable ×××図 1 提案手法の概要図。
左に示す先行研究ではすべてのパラメータを上書きしてしまうが、右に示す提案手法では特定のパラメータ（ピンクの部分）のみを更新できる。
右の図は、パラメータを行で 𝑛 = 4 個、列で 𝑚 = 4 個の計 16 個のブロックに切り分け、𝑊𝑝∈ ℝ𝑛×𝑚の中で重要な 𝑘 = 4 個のブロック位置を特定し、そこにブロックパラメータ 𝑊𝑣∈ ℝ𝑘×𝑑/𝑛×𝑑/𝑚を挿入して、疎なパラメータを構築した状態を表している。
と
Adapter 型[6, 7, 8, 9, 10, 11, 12, 13]に分けられる。
本稿で注目する Adapter 型では、ベースモデルのパラメータを固定し、少数の追加パラメータのみを更新することでパラメータ数 𝜙 を減らしている。
その中には、行列分解[6, 7, 11]や特殊な操作[12, 13]によりパラメータ数 𝜙 を減らすものがある。
これらは追加したパラメータを学習後に元のパラメータに統合できるという特徴があり、追加パラメータの計算による推論速度の低下を避けることができる。
一方これらの PEFT は、対象とするモジュールのパラメータをすべて更新してしまう。
先行研究[14]でも言及されているとおり、特定のタスクおよびドメインに寄与するパラメータは限定的であり、パラメータをすべて更新してしまうと、保持したい事前知識が失われたり、特定の事前知識の編集が困難になったりする。
すなわち、(Q1)事前学習による優れた性能と(Q2)モデル解釈性が失われると考えられる。
これらの課題に対して、先行研究[14]では重要度に基づいて部分的にパラメータを更新する PEFTを提案しているが、強い制約を導入するために複雑な操作を必要としている。
また更新されたパラメータの解釈に関しては無視されている。
本稿では、図 1 に示すような疎のパラメータを用いることで、より単純かつモデル解釈可能な PEFTを提案する。
実験の結果、(A1)モデルサイズに関わらず部分的なパラメータの更新でも性能が向上すること、(A2)学習ドメインに対して重要なパラメータを示せることを確認した。
2 PEFT本節では、大規模モデルを少ない計算リソースでFine-Tune するための PEFT について説明する。
前節で述べたように、更新するモジュールのパラメータ数を 𝜙 とする場合、すべてのパラメータを更新するためには計算リソースが 16𝜙 byte 必要となる。
そこで、Adapter 型の PEFT ではℎ = 𝑊′𝑥 = (𝑊𝑜+ Δ𝑊)𝑥 (1)のように元のモジュールのパラメータ 𝑊𝑜を固定し、少量の追加パラメータ Δ𝑊 のみを更新することで、パラメータ数 𝜙 を減らしている。
ここで、𝑥 は入力ベクトル、ℎ は 𝑥 に対する潜在ベクトル、𝑊𝑜∈ ℝ𝑑×𝑑は対象モジュールのパラメータ、Δ𝑊 ∈ ℝ𝑑×𝑑は追加した学習可能パラメータである。
Δ𝑊 の作り方にはいくつか種類があり、行列分解に基づいて Δ𝑊 = 𝐵 𝐴 とするもの[6]や、特殊な操作に基づいて Δ𝑊 = Op( 𝐴)とするもの[12]などがある。
いずれも推論時は、𝑊′= 𝑊𝑜+ Δ𝑊 とパラメータをあらかじめ統合しておくことで、追加の行列計算による推論速度の低下を避けることができる。
しかし Δ𝑊 は密なパラメータであるため、元のパラメータ 𝑊𝑜が持つ事前知識を上書きしてしまう問題がある。
例えば、ベースの言語モデルを指示調整した後にドメイン学習を行うと、事前および指示調整で獲得した知識を上書きし続けてしまい、知識忘却につながる。
また、図 1 のように、Δ𝑊 が密なパラメータということは、どのパラメータがどのタスクおよびドメインに対する性能に寄与するのか解釈が難しいという問題がある。
3 疎なパラメータを用いた PEFT前述したとおり、密なパラメータを追加して更新する PEFT では、保持したい事前知識が失われたり特定の箇所だけの編集が困難な場合がある。
また、どのパラメータがどのドメインやタスクに対する性能向上に寄与するのか解釈が難しい。
そこで本節では、図 1 に示すとおり、Δ𝑊 = Sparse(p, v𝑊𝑣)(2)のように疎なパラメータを用いることで、必要な計算リソースを抑えつつ特定のパラメータのみを選択して更新できる新しい PEFT を提案する。
後述するように、更新されるパラメータを特定できるのでモデル解釈性も得ることができる。
ここで、Sparse(·)は疎な行列 Δ𝑊 を構築する関数であり、引数として、位置を表す p と値を表す v𝑊𝑣がある。
疎な行列を構築するために必要な p、v、𝑊 は、次の順で導出する。
まず、重要なパラメータの大まかな位置を特定するために、元のパラメータ 𝑊𝑜∈ R𝑑×𝑑を行と列でそれぞれ 𝑛 個と 𝑚 個のブロックに分ける。
次に、𝑛 × 𝑚 個のブロックに対する重要度合いを表す更新可能なパラメータ 𝑊𝑝∈ R𝑛×𝑚を用意し、上位 𝑘 個のブロックの位置とスコアをp, v = TopK(𝑊𝑝)(3)のように取得する。
ここで、TopK(·)は入力 𝑊𝑝において値が上位 𝑘 番目までの位置 p と値 v を返す関数である。
得られた 𝑘 個の位置とスコアに対し、𝑘 個の更新可能なブロックパラメータ 𝑊𝑣∈ ℝ𝑘×𝑑/𝑛×𝑑/𝑚を用意して、v𝑊𝑣= [𝑣1𝑊𝑣1, 𝑣2𝑊𝑣2, ..., 𝑣𝑘𝑊𝑣𝑘](4)のように 𝑘 個のスコアでそれぞれのブロックパラメータ𝑊𝑣𝑖を重みづける。
ここで、𝑣𝑖は正の実数値、𝑊𝑣𝑖∈ ℝ𝑑/𝑛×𝑑/𝑚は選択された 𝑘 個の位置に対する学習可能なブロックパラメータ、v は各ブロックに対するスコアである。
また、ハイパーパラメータ 𝛼 を用いてスケールする[6]。
このように疎なパラメータを用いると、保持すべきパラメータは 𝑊𝑝と 𝑊𝑣だけであり、パラメータ数は 𝜙 = 𝑛 × 𝑚 + 𝑘 × 𝑑/𝑛 × 𝑑/𝑚)と表現できる。
特に、𝑛𝑚 = 𝑑 とすると 𝜙 = (1 + 𝑘)𝑑 となり、元のパラメータ数は 𝑑 の 2 乗であったものが、先行研究の PEFTと同じく 𝑑 の 1 乗に抑えることができる。
さらに、元のパラメータをブロックに分けて、一部のブロックのみ更新できることから、事前知識の上書きを防いで性能の向上することができる。
また、選択した表 1 言語理解ベンチマーク(GLEU)における定量評価†：先行研究[6]から引用学習可能パラメータ数 MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B AvgFull† 125.0M 87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2 86.4LoRA 0.9M 87.4 94.4 88.7 60.8 92.8 90.8 79.1 90.8 85.7Ours 0.9M 83.2 94.6 90.0 55.2 92.7 86.5 79 .1 89.9 83.9表 2 算術推論ベンチマークにおける定量評価学習可能パラメータ数 MultiArith GSM8K AddSub AQuA SingleEq SVAMP AvgLoRA 56.6M 68.1 50.7 83.3 35.8 87.2 71.2 66.1Ours 56.3M 67.5 50.1 84.5 36.6 87.9 72.8 66.6ブロックのスコアを観察することでモデル解釈性が高まることが期待される。
先行研究[6, 15]でも述べられている通り、学習の初めは Δ𝑊 が影響を及ぼさないようにすることが重要であるため、𝑊𝑝は正規分布、𝑊𝑣はゼロで初期化する。
4 実験本節では、まず、モデルサイズに関わらず、疎なパラメータ更新でも性能を向上できることを検証する。
その後に、学習データに応じて選択されるパラメータにパターンについて観察する。
4.1 設定疎なパラメータ更新でも性能を向上できることを検証するために、幅広く利用されている密なパラメータ更新の LoRA (Low-Rank Adaptation)[6]と比較する。
ベースのモデルとしては、小さいものとして RoBERTa-125M2）[16]、大きいものとしてLlama3-8B3）[1]を用いた。
ベースラインを先行研究[6]に設定して、正しく比較できるよう学習パラメータ 𝜙 は同じになるように設定した。
RoBERTa-125Mは言語理解ベンチマークの GLEU[17]でそれぞれ学習および評価する。
学習設定は先行研究[6]に従い、独自のハイパーパラメータは 𝑛 = 24、𝑚 = 32、𝑘 = 15、𝛼 = 1.0 に設定した。
Llama3-8B は 8bit で量子化して Math10K で学習し、算術推論のベンチマークの MultiArith、GSM8K、AddSub、AQuA、SingleEq、SVAMP で評価する。
Math10K には GSM8K と AQuAの学習データのみ含まれる。
学習設定は先行研究[18]に従い、独自のハイパーパラメータは、𝑛 = 64、𝑚 = 64、𝑘 = 50、𝛼 = 2.0 に設定した。
また、ブロックパラメータの重要度を示す 𝑊𝑝∈ R𝑛×𝑚に対しては、2） https://huggingface.co/FacebookAI/roberta-base3） https://huggingface.co/meta-llama/Meta-Llama-3-8B学習データのドメインやタスクに応じてパターンが現れるか人手で確認する。
すべての実験で NVIDIAH100 を用い、Llama3-8B には FlashAttention2 [19]を用いた。
4.2 定量評価はじめに表 1 に言語理解ベンチマークでの定量評価の結果を示す。
表 1 からわかるように、密なパラメータを更新を行う先行研究と比較して、疎なパラメータを更新でも SST-2、MRPC、QNLI、RTE のデータセットでより同等以上の性能を示した。
しかし、密なパラメータを更新と比較すると MNLI、CoLA、QQP での性能向上は低く、平均が 2 ポイントほど下回った。
続いて表 2 に算術推論ベンチマークでの定量評価の結果を示す。
表 2 からわかるように、密なパラメータを更新を行う先行研究と比較して、疎なパラメータを更新でも MultiArith と GSM8K 以外のデータセットで同等以上の性能を示した。
平均は 0.5 ポイントほど上回った。
4.3 モデル解釈について提案手法では、学習した後に獲得された 𝑊𝑝∈R𝑛×𝑚に基づいて、学習データに対して選択されたパラメータの位置とスコアを観察できる。
図2 と図 3 にそれぞれ MRPC と CoLA で学習したRoBERTa-125M[16]の self-attention 層の query パラメータに対するブロックスコアを可視化したものを示す。
明るいほどスコアが高く、暗いほどスコアが低いことを示している。
図 2 からわかるように、MRPC では最初の 0 層目に明るいブロックは少なく、学習される 𝑘 = 15 個のブロックで足りている。
最後の 11 層目にも明るいブロックは少なく、学習される 𝑘 = 15 個のブロック0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 161718192021222324 25 26 27 28 29 30 310246810121416182022layer.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 161718192021222324 25 26 27 28 29 30 310246810121416182022layer.11図 2 MRPC で学習した後に獲得された 𝑊𝑝∈ ℝ𝑛×𝑚のブロックスコアを可視化したもの。
スコアが高い明るいブロックは 0 層目にも 11 層目にもほとんど出現しない。
で
足りていることが分かる。
同じく図 3 からわかるように、CoLA でも最初の 0 層目に明るいブロックは少なく、学習される 𝑘 = 15 個のブロックで足りている。
しかし最後の 11 層目に明るいブロックが多くなり、学習される 𝑘 = 15 個のブロックでは足りていないことが分かる。
この結果は、LoRA において、入力に近い層にはランクを小さく、出力に近い層はランクを大きくした方が性能が良くなることを示した先行研究[7]と似ている。
これが、CoLA における性能向上の頭打ちの原因だと考えられる。
また、𝑊𝑝の初期値にも強く影響を受けることが考えられるが、この分析については今後の課題とする。
以上のことは、RoBERTa-125M においては学習ドメインやタスクに応じて重要なパラメータが存在していることを示唆しており、学習した後に獲得された 𝑊𝑝∈ R𝑛×𝑚がモデル解釈に利用できることを示している。
また CoLA 以外でも最初の 0 層目に明るいブロックは少なく、最後の 11 層目に明るいブロックが多くなる傾向があることから、先行研究[7]同様、入力に近い層は 𝑘 を小さくし、出力に近い層は 𝑘 を大きくするとさらなる性能向上が見込まれることが分かる。
加えて疎であるため学習ドメインやタスクごとに更新した Δ𝑊 は重複しづらく、統合の際に知識の上書きがされにくいことも示唆している。
これらは、先行研究[6]には無いユニークな特徴である。
Llama3-8B でのブロックスコアの分析については今後の課題とする。
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 161718192021222324 25 26 27 28 29 30 310246810121416182022layer.00 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 161718192021222324 25 26 27 28 29 30 310246810121416182022layer.11図 3 CoLA で学習された後に獲得された 𝑊𝑝∈ ℝ𝑛×𝑚のブロックスコアを可視化したもの。
スコアが高い明るいブロックは 0 層目にはほとんど出現しないが、11 層目には 𝑘 = 15 個を超えて出現する。
5 おわりに本稿では、先行研究[6]における密なパラメータの更新では元のモデルが持つ事前知識の保持および編集が複雑になり、性能向上とモデル解釈性が失われることを指摘した。
そこで、疎なパラメータを用いた新たな PEFT を提案した。
先行研究と同様に計算リソースを抑えつつ、新たにモデルパラメータの解釈性を持たせられる特徴がある。
結果として、部分的なパラメータ更新でも性能が向上することを示すとともに、どのパラメータが性能に寄与するのか解釈できることも示した。
今後の展望としては、先行研究における密なパラメータの更新では知識忘却につながるが、提案手法ではそれが抑えられることを検証することが考えられる。
例えば、指示調整した後にドメイン調整すると、事前学習および指示調整で獲得した知識を上書きしてしまい性能が劣化する可能性があるが、提案手法では解消できる可能性がある。
また、複数のパラメータ Δ𝑊 で複数のドメインやタスクを Fine-Tuneすることを想定すると、互いのパラメータ Δ𝑊 が干渉しないようにうまく制御できるようにすることも考えられる。



参考文献


[1] Llama-Team. The llama 3 herd of models.
[2] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizingcontinuous prompts for generation. In Chengqing Zong,Fei Xia, Wenjie Li, and Roberto Navigli, editors, Pro-ceedings of the 59th Annual Meeting of the Asso-ciation for Computational Linguistics and the 11thInternational Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers), pp. 4582–4597,Online, August 2021. Association for Computational Lin-guistics.
[3] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, YujieQian, Zhilin Yang, and Jie Tang. Gpt understands, too,2023.
[4] Zhen Wang, Rameswar Panda, Leonid Karlinsky,Rog´erio Schmidt Feris, Huan Sun, and Yoon Kim. Mul-titask prompt tuning enables parameter-eﬃcient transferlearning. ArXiv, Vol. abs/2303.02861, , 2023.
[5] Tsachi Blau, Moshe Kimhi, Yonatan Belinkov, Alexan-der Bronstein, and Chaim Baskin. Context-aware prompttuning: Advancing in-context learning with adversarialmethods. arXiv preprint arXiv:2410.17222, 2024.
[6] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. LoRA: Low-rank adaptation of large language mod-els. In International Conference on Learning Repre-sentations, 2022.
[7] Qingru Zhang, Minshuo Chen, Alexander Bukharin,Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.Adaptive budget allocation for parameter-eﬃcient ﬁne-tuning. In The Eleventh International Conference onLearning Representations, 2023.
[8] Eric L. Buehler and Markus J. Buehler. X-lora: Mixtureof low-rank adapter experts, a ﬂexible framework for largelanguage models with applications in protein mechanicsand molecular design, 2024.
[9] Nam Hyeon-Woo, Moon Ye-Bin, and Tae-Hyun Oh. Fed-para: Low-rank hadamard product for communication-eﬃcient federated learning, 2023.
[10] Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, YuxuanXue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo,Songyou Peng, Yandong Wen, Michael J. Black, AdrianWeller, and Bernhard Sch¨olkopf. Parameter-eﬃcient or-thogonal ﬁnetuning via butterﬂy factorization. In ICLR,2024.
[11] Wenxuan Tan, Nicholas Roberts, Tzu-Heng Huang, JitianZhao, John Cooper, Samuel Guo, Chengyu Duan, andFrederic Sala. More ﬁne-tuning with 10x fewer parameters,2024.
[12] Ziqi Gao, Qichao Wang, Aochuan Chen, Zijing Liu,Bingzhe Wu, Liang Chen, and Jia Li. Parameter-eﬃcientﬁne-tuning with discrete fourier transform. In Forty-ﬁrst International Conference on Machine Learning,ICML 2024, Vienna, Austria, July 21-27, 2024, 2024.
[13] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang,Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun,Qi Zhang, Deqing Wang, and Fuzhen Zhuang. Mora:High-rank updating for parameter-eﬃcient ﬁne-tuning.ArXiv, Vol. abs/2405.12130, , 2024.
[14] Haoyu Wang, Tianci Liu, Ruirui Li, Monica Xiao Cheng,Tuo Zhao, and Jing Gao. RoseLoRA: Row and column-wise sparse low-rank adaptation of pre-trained languagemodel for knowledge editing and ﬁne-tuning. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Pro-ceedings of the 2024 Conference on Empirical Meth-ods in Natural Language Processing, pp. 996–1008,Miami, Florida, USA, November 2024. Association forComputational Linguistics.
[15] Baohao Liao, Shaomu Tan, and Christof Monz. Make yourpre-trained model reversible: From parameter to memoryeﬃcient ﬁne-tuning, 2023.
[16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, LukeZettlemoyer, and Veselin Stoyanov. Roberta: A ro-bustly optimized bert pretraining approach. ArXiv, Vol.abs/1907.11692, , 2019.
[17] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural languageunderstanding, 2019.
[18] Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-PengLim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria.Llm-adapters: An adapter family for parameter-eﬃcientﬁne-tuning of large language models. arXiv preprintarXiv:2304.01933, 2023.
[19] Tri Dao. FlashAttention-2: Faster attention with betterparallelism and work partitioning. In International Con-ference on Learning Representations (ICLR), 2024.