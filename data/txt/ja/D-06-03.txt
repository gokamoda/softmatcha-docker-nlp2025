Vector Quantization に基づく離散系列の発話による分散型深層モデルの提案

三好遼

1

栗田修平

2,31

フリー

2

国立情報学研究所

3

NII LLMC



miyoshi2020robotcs@gmail.com skurita@nii.ac.jp



概要

近年、記号創発ロボティクスでは、ベイズ脳仮説を始めとした統合情報理論に基づいたマルチモーダル処理や環境との相互作用による概念獲得などの様々な研究が進んでいる。
中でも言語創発は、近年注目の研究であり、分散的ベイズ学習によるコミュニケーションモデルが提案されている。
そこで本提案手法では、深層モデルへの応用を検討するため、ベクトル量子化を拡張した VQCom-VAE を提案し、実験にてコミュニケーションによる相手の発話から予測画像の生成が可能であることを示した。


1 はじめに

近年、記号創発ロボティクスでは、ベイズ脳仮説[1]に基づいた分散的ベイズ学習による言語創発の研究が行われてきた[2, 3, 4, 5]。
文献[2]では、メトロポリス・ヘイスティングス法を拡張したメトロポリス・ヘイスティングス名付けゲームによる分散的ベイズ学習を提案している。
具体的には、ベイズモデルである Gaussian MixtureModel（GMM）と深層ベイズモデルである VariationalAuto-Encoder（VAE）を Serket [6]を使用して統合する。
統合したモデルを Fruits-360 データセット[7]（果物の異なる視点画像）を使用して GMM でカテゴリを学習する。
カテゴリを発話とし、学習済みモデル（エージェント A）と未学習モデル（エージェント B）をメトロポリス・ヘイスティングス名付けゲームでコミュニケーション学習する。
その結果、異なる視点の入力画像に対しても発話の一致率が約8 割の性能となり、メトロポリス・ヘイスティングス名付けゲームによる記号創発現象を示した。
さらに、文献[5]では、マルチモーダル情報をMultimodal Latent Dirichlet Allocation（MLDA） [8]で統合し、潜在変数から Gaussian Process Hidden Semi-Markov Model（GP-HSMM） [9]による単語パターンの推論と連続記号の生成をメトロポリス・ヘイスティングス名付けゲームで学習することで、連続記号から分節化に基づいた言語創発を可能にした。
しかし、これらの文献では、MCMC 法による学習が一般的であり、勾配降下法を使用する深層モデルへの応用は、困難である。
そこで本稿では、Vector Quantized Variational Auto-Encoder（VQ-VAE） [10]を分散型深層モデルに拡張した Vector Quantized Communication Variational Auto-Encoder（VQCom-VAE）を提案する。
本提案手法では、エージェント 𝐴 のエンコーダの出力 𝑧 から量子化した離散系列の発話 𝑚𝐴を生成し、エージェント𝐵 に送信する。
エージェント 𝐵 は受信した発話 𝑚𝐴を埋め込み空間 𝐸 を通して、デコーダから予測画像 ˆ𝑜𝐵を生成することで、コミュニケーションに基づいた画像生成が可能である。
また、コミュニケーションは、エージェント 𝐴 から 𝐵、エージェント 𝐵から 𝐴 の双方向による予測も可能である。
学習では、ミニバッチ 𝑡 におけるエージェント 𝐴 の発話𝑚𝐴を含めてエージェント 𝐵 の埋め込み空間 𝐸𝐵𝑡を更新することで予測画像 ˆ𝑜𝐵と発話 𝑚𝐴との関連付け学習が可能である。
さらに、重み係数 𝛼∗を導入し、エージェント間の発話における学習の影響度合いを表現する。
実験では、MNIST データセット[11]を使用し、2者エージェントによる発話 𝑚𝐴, 𝑚𝐵に基づいた数字画像の予測が可能であることを検証した。
その結果、提案手法によるエージェント 𝐴 の発話 𝑚𝐴に基づいた予測画像 ˆ𝑜𝐵をエージェント 𝐵 で生成可能であることを示した。
さらに、重み係数 𝛼 を変化させたコミュニケーションでは、相手の発話に対する重み係数を低くすることで、コミュニケーションが通じなくなることを示した。

Agent A Agent BInput: 𝒐𝑨𝒎𝑨SenderOutput: ෝ𝒐𝑩Encoder𝒒(𝒛𝑨|𝒐𝑨)Decoder𝒑(ෝ𝒐𝑩|𝒛𝒒𝑩)Embedding 𝑬𝑩ReceiverVector Quantized Communication Observation 𝒐𝑨Inference ෝ𝒐𝑩Vector Quantized𝒐𝑨ෝ𝒐𝑩図 1 提案手法の概要

2 提案手法：VQCom-VAE

本提案手法の概要図を図 1 に示した。
本提案手法では、VQ 層を拡張し、双方向のコミュニケーションを可能とした分散型深層モデルである。
節 2.1 では，VQCom-VAE におけるエンコーダとデコーダの学習について述べる。
節 2.2 と節 2.3 では、VQ 層を拡張した手法による発話 𝑚 の生成および埋め込み空間 𝐸 の更新について述べる。



2.1 エンコーダとデコーダの学習

まず、入力画像 𝑜 をガウス分布を仮定したエンコーダ𝑞(𝑧|𝑜)に入力し、潜在変数𝑧∈R𝐷×𝑊 ×𝐻を出力する。
𝑧 ∼ 𝑞(𝑧|𝑜)(1)次元数 𝐷 は、潜在変数の次元数であり、次元数 𝑊 ，𝐻 は Convolutional Neural Network(CNN)層で入力画像を畳み込んだ特徴マップにおける縦横のピクセル数である。
エンコーダ 𝑞(𝑧|𝑜)の出力 𝑧 とカテゴリ数を 𝐾 とする埋め込み空間 𝐸 の重みベクトル 𝑒 ∈R𝐾 ×𝐷を用いて量子化する。
この量子化した発話 𝑚 ∈N𝑊 ×𝐻から特徴 𝑧𝑞∈R𝐷×𝑊 ×𝐻を出力する。
特徴 𝑧𝑞をガウス分布を仮定したデコーダ 𝑝( ˆ𝑜|𝑧𝑞)から予測画像 ˆ𝑜 を生成する。
ˆ𝑜 ∼ 𝑝( ˆ𝑜|𝑧𝑞)(2)式(3)の損失関数Lを最小化するようなパラメータ Θ を学習する。
第一項は、再構成誤差と呼ばれる項であり、入力画像 𝑜 と予測画像 ˆ𝑜 との誤差を小さくすることで入力画像 𝑜 に近い予測画像 ˆ𝑜 を生成するように学習する。
第二項は、コミットメント損失と呼ばれる項であり、埋め込み空間 𝐸 の出力 𝑧𝑞を潜在変数 𝑧 に近づけるための項である。
第三項は、ベクトル量子化損失と呼ばれる項であり、潜在変数𝑧 を埋め込み空間 𝐸 の出力 𝑧𝑞に近づけるための項である。
第二項、第三項における sg[·]は、勾配停止(stop gradient)と呼ばれる操作であり、埋め込み空間𝐸 の直接的な勾配伝達の停止を意味する。
また、第二項における 𝛽 は定数であり、一般的に 𝛽 = 0.25 と設計されることが多く、適切な値に設定することで損失関数を調整する。
LΘ= k𝑜 − ˆ𝑜k22+ 𝛽ksg[𝑧] − 𝑧𝑞k22+ k𝑧𝐴− sg[𝑧𝑞]k22(3)

2.2 各エージェントの発話 𝑚 の送信

まず、Gumbel-softmax sampling [12]に基づき、ガンベル分布に従う乱数 𝑔𝑖∈R𝐾 ×𝑊 ×𝐻を生成する。
節 2.1 のエンコーダ 𝑞(𝑧|𝑜)の出力 𝑧𝑖と乱数 𝑔𝑖、埋め込み空間 𝐸 の重みベクトル 𝑒𝑖を式(4)に代入し、カテゴリ数 𝐾 のカテゴリカル分布 𝑝(𝜋|𝑧𝑖)を計算する。
𝑝(𝜋|𝑧𝑖) =exp(−k𝑧𝑖− 𝑒𝑖k22+ 𝑔𝑖)/𝜏𝐷−1𝑑=0exp(−k𝑧𝑑− 𝑒𝑑k22+ 𝑔𝑑)/𝜏(4)定数 𝜏 は温度定数であり、値が高いほど確率が均等な分布に近づき、低いほど特定のカテゴリが強調された分布になる。
カテゴリ 𝑗 ∈ {0, ..., 𝐾 − 1} とするカゴリカル分布𝑝(𝜋|𝑧𝑖)から arg max𝑗𝑝(𝜋|𝑧𝑖)を計算し、エンコーダ𝑞(𝑧|𝑜)の出力 𝑧𝑖と埋め込み空間 𝐸 の重みベクトル𝑒𝑖が最も近いカテゴリ 𝑗 を発話 𝑚𝑤,ℎ∈Nとする(𝑤 ∈ 𝑊, ℎ ∈ 𝐻)。
また、発話 𝑚 は、𝑊 × 𝐻 の離散系列(𝑚 ∈N𝑊 ×𝐻)であり、入力画像 𝑜 を畳み込んだ特

徴 𝑧 のピクセルに対応したカテゴリ 𝑗 をエージェント間で送受信すると解釈できる。
𝑚𝑤,ℎ= arg max𝑗𝑝(𝜋𝑗|𝑧𝑖)(5)

2.3 発話 𝑚 から埋め込み空間 𝐸

𝑡

の更新

まず、ミニバッチ 1 からミニバッチ 𝑡 までの埋め込み空間 𝐸1:𝑡は、エンコーダ 𝑞 (𝑧1:𝑡|𝑜1:𝑡)の出力 𝑧1:𝑡との条件付き確率 𝑝(𝑒1:𝑡|𝑧1:𝑡)として表すことができ、ベイズの定理から関係式(6)を得る。
𝑝(𝑒1:𝑡|𝑧1:𝑡) ∝ 𝑝 (𝑒𝑡 −1) 𝑝(𝑧𝑡|𝑒𝑡)(6)確率分布 𝑝 (𝑒𝑡 −1)は、ミニバッチ 𝑡 − 1 における埋め込み空間 𝐸𝑡 −1のカテゴリ 𝑗 ∈ {0, ..., 𝐾 − 1} に対応する重みベクトル 𝑒𝑡 −1, 𝑗の確率分布である。
また、確率分布 𝑝 (𝑧𝑡|𝑒𝑡)は、ミニバッチ 𝑡 におけるエンコーダ 𝑞(𝑧1:𝑡|𝑜1:𝑡)の出力 𝑧1:𝑡と埋め込み空間 𝐸𝑡の重みベクトル 𝑒𝑡との条件付き確率である。
ここで、エージェント数 𝑁 を 𝑁 = 2 とし、ミニバッチ 𝑡 における自身の発話と相手の発話をそれぞれ 𝑚𝐴𝑡, 𝑚𝐵𝑡とする。
さらに、それぞれの発話 𝑚𝐴𝑡, 𝑚𝐵𝑡に対する重み係数 𝛼𝐴, 𝛼𝐵を導入する。
𝑝(𝑧𝑡|𝑒𝑡, 𝑗) =𝑚𝐴𝑡,𝑚𝐵𝑡𝑝(𝑧𝑡, 𝑚𝐴𝑡, 𝑚𝐵𝑡|𝑒𝑡, 𝑗)= 𝛼𝐴𝑅−1𝑟=0𝑝(𝑚𝐴𝑡|𝑒𝑡, 𝑗) 𝑝(𝑧𝑡,𝑟|𝑒𝑡, 𝑗)+ 𝛼𝐵𝑅−1𝑟=0𝑝(𝑚𝐵𝑡|𝑒𝑡, 𝑗) 𝑝(𝑧𝑡,𝑟|𝑒𝑡, 𝑗)(7)式(7)における変数 𝑅 は 𝑅 = 𝐷 × 𝑊 × 𝐻 であり、総和𝑅−1𝑟=0は、カテゴリ数 𝐾 以外の次元数の総和を意味する。
式(7)を埋め込み空間 𝐸𝑡の重みベクトル 𝑒𝑡, 𝑗に関して、最尤推定することで更新式(9)を得る。
𝑐𝑡, 𝑗← 𝜆𝑐𝑡 −1, 𝑗+ (1 − 𝜆)𝛼𝐴𝑅−1𝑟=0®1[𝑚𝐴𝑡= 𝑒𝑡 −1, 𝑗]+𝛼𝐵𝑅−1𝑟=0®1[𝑚𝐵𝑡= 𝑒𝑡 −1, 𝑗](8)𝑒𝑡, 𝑗← 𝜆𝑒𝑡 −1, 𝑗+ (1 − 𝜆)𝛼𝐴𝑅−1𝑟=0®1[𝑚𝐴𝑡= 𝑒𝑡 −1, 𝑗]𝑧𝑡,𝑟𝑐𝑡, 𝑗+𝛼𝐵𝑅−1𝑟=0®1[𝑚𝐵𝑡= 𝑒𝑡 −1, 𝑗]𝑧𝑡,𝑟𝑐𝑡, 𝑗(9)定数 𝜆 はミニバッチ 𝑡 における更新の減衰率であり、値を小さくすると過去の情報が強く反映され、(b) A to B Communication(c) B to A Communication(a) Input Image𝒐𝑨𝒐𝑩ෝ𝒐𝑨ෝ𝒐𝑩ෝ𝒐𝑨ෝ𝒐𝑩図 2 (a)は、エージェント 𝐴 から 𝐵 における入力画像𝑜𝐴。
エージェント 𝐵 から 𝐴 における入力画像 𝑜𝐵(b)は、提案手法によるエージェント 𝐴 から 𝐵 の予測画像 ˆ𝑜𝐴，ˆ𝑜𝐵．(c)は、提案手法によるエージェント 𝐵 から 𝐴 の予測画像 ˆ𝑜𝐴， ˆ𝑜𝐵．値を大きくすると、最新の情報が強く反映する。
同様に発話 𝑚𝑛の重み係数 𝛼𝑛も大きくすると、エージェント 𝑛 の情報が強く反映するように更新される。
ただし、重み係数 𝛼𝑛は，𝑁𝑛∈ 𝐴,𝐵𝛼𝑛= 1 である。


3 実験

本実験では、提案手法によるエージェント間のコミュニケーションが可能なことを検証した。
想定する実験は、エージェント数 𝑁 = 2 とし、エージェント 𝐴、エージェント 𝐵 で異なる視点の入力画像𝑜𝐴, 𝑜𝐵を学習する。
エージェント 𝐴 の入力画像 𝑜𝐴には、0 ∼ 9 の数字が描かれた画像 60, 000 枚を含むMNIST データセット[11]を使用し、エージェント𝐵 の入力画像 𝑜𝐵には、エージェント 𝐴 の入力画像𝑜𝐴を 45◦回転した画像を入力する。
また、式(3)の損失関数LΘを最小化するように学習すると同時に、式(5)から量子化したエージェント 𝐴, 𝐵 の発話𝑚𝐴, 𝑚𝐵を送信する。
受信した発話 𝑚𝐴, 𝑚𝐵からエージェント別に埋め込み空間 𝐸𝐴, 𝐸𝐵を式(9)で更新する。
このようなコミュニケーションを 30 回繰り返し学習する。
パラメータ本実験におけるパラメータの設定値を以下の項目に記載した。
• エージェント数：2• 入力チャンネル数 𝑐𝑖𝑛：1• 潜在変数 𝑧𝐴の次元数 𝐷：128• カテゴリ数 𝐾：512• 学習係数 𝑙𝑟：0.001• パラメータ 𝛽：0.25

表 1 入力画像 𝑜∗と予測画像 ˆ𝑜∗との予測誤差(有効数字 3 桁)。
太字は、各実験における予測誤差の最小値である。
重み係数 𝛼 Mean Squared Error(𝑜∗, ˆ𝑜∗)𝐴 𝐵 𝐴 𝐵場面 𝛼𝐴𝐴𝛼𝐴𝐵𝛼𝐵𝐴𝛼𝐵𝐵学習データテストデータ学習データテストデータ提案手法 A to B 0.5 0.5 0.5 0.5 0.0251 0.0246 0.0536 0.0531(VQCom-VAE) 0.7 0.3 0.3 0.7 0.0231 0.0227 0.0540 0.05380.9 0.1 0.1 0.9 0.0215 0.0212 0.0741 0.07461.0 0.0 0.0 1.0 0.0224 0.0221 0.0825 0.0829B to A 0.5 0.5 0.5 0.5 0.0506 0.0503 0.0271 0.02660.7 0.3 0.3 0.7 0.0546 0.0540 0.0245 0.02410.9 0.1 0.1 0.9 0.0697 0.0696 0.0238 0.02351.0 0.0 0.0 1.0 0.0815 0.0821 0.0234 0.0230• 減衰率 𝜆: 0.99• 温度定数 𝜏: 0.5• 学習回数 epoch：30• ミニバッチ数：64検証では、テストデータ 10, 000 枚を使用し、エージェントの発話 𝑚∗に基づく予測画像 ˆ𝑜∗と入力画像𝑜∗との予測誤差を Mean Squared Error(MSE)で評価した。
ただし、送信者と受信者で異なる視点画像 𝑜∗を学習するため、単純な評価は困難である。
そのため、受信者の評価では、受信者の視点に直した画像を使用して計算した。
また、エージェント 𝑛 の発話𝑚𝑛の重み係数 𝛼∗𝑛の変化による予測画像 𝑜∗の影響についても検証した。


4 実験結果

表 1 にエージェント間のコミュニケーションによる入力画像 𝑜∗と予測画像 ˆ𝑜∗との誤差の評価結果を示した。
まず、エージェント 𝐴 から 𝐵 のコミュニケーションで、最もエージェント 𝐵 の予測誤差が小さいのは、重み係数 𝛼𝐴𝐴= 0.5, 𝛼𝐴𝐵= 0.5, 𝛼𝐵𝐴= 0.5, 𝛼𝐵𝐵=0.5 の条件であった。
一方で、エージェント 𝐴 から 𝐵において、最もエージェント 𝐴 の予測誤差が小さいのは、重み係数 𝛼𝐴𝐴= 0.9, 𝛼𝐴𝐵= 0.1, 𝛼𝐵𝐴= 0.1, 𝛼𝐵𝐵= 0.9の条件であった。
図 2 では、重み係数 𝛼𝐴𝐴= 0.5, 𝛼𝐴𝐵=0.5, 𝛼𝐵𝐴= 0 .5, 𝛼𝐵𝐵= 0 .5 の条件時におけるエージェント 𝐴 の発話 𝑚𝐴からそれぞれデコーダで出力した予測画像 ˆ𝑜∗を示した。
予測画像 ˆ𝑜∗には、ぼやけている画像もあるが、相手の発話 𝑚∗から予測画像 ˆ𝑜∗の生成を確認できた。
また、エージェント 𝐵 の発話𝑚𝐴に対する重み係数 𝛼𝐵𝐴を 𝛼𝐵𝐴= 0.5, 0.3, 0.1, 0.0 と減少させ、エージェント 𝐵 の発話 𝑚𝐵に対する重み係数 𝛼𝐵𝐵を 𝛼𝐵𝐵= 0.5, 0.7, 0.9, 1.0 と増加させて、条件ごとに学習した。
図 3 は、エージェント 𝐵 の重み係数𝛼𝐵𝐴の変化による発話𝑚𝐴からの予測画像ˆ𝑜𝐵を示した。
この結果からエージェント 𝐴 の発話 𝑚𝐴に対𝜶𝑨𝑩Images Predicted ෝ𝒐𝑩from Message 𝒎𝑨𝟎. 𝟓𝟎. 𝟑𝟎. 𝟏𝟎. 𝟎図 3 エージェント 𝐴 から 𝐵 のコミュニケーション時の重み係数 𝛼𝐵𝐴の変化による予測画像 𝑜𝐵の影響する 𝐵 の重み係数 𝛼𝐵𝐴が低くなるほど、𝐴 の発話 𝑚𝐴から生成した予測画像 ˆ𝑜𝐵がぼやけていき、相手の発話に対する重み係数 𝛼 を低くすることで、コミュニケーションが通じなくなることを示した。



5 まとめ

本稿では、VQ-VAE を分散型深層モデルに拡張した VQCom-VAE を提案した。
提案手法では、エンコーダ、デコーダの学習と同時に式(9)を用いて埋め込み空間 𝐸 を更新することで、エージェント間の発話 𝑚∗に基づいた予測画像 𝑜∗を生成することが可能である。
実験では、MNIST データセットを使用し、発話 𝑚∗から予測画像 ˆ𝑜∗が生成可能かを検証した。
評価では、入力画像 𝑜∗と予測画像 ˆ𝑜∗との誤差を計算した。
また、生成した予測画像 ˆ𝑜∗を確認した結果、相手の発話 𝑚∗から予測画像 ˆ𝑜∗の生成が可能なことを示した。
さらに相手の発話 𝑚∗の重み係数 𝛼 を低くすることで、相手の発話 𝑚∗から予測画像 ˆ𝑜∗がぼやけていき、コミュニケーションが通じなくなることを示した。
しかし、本提案手法では概念獲得や二重分節性などを考慮した言語創発モデルではない。
今後は埋め込み空間 𝐸 を階層化し、概念獲得や二重分節性を考慮した手法に発展させる予定である。
さらに，Action Chunking Transformer(ACT)[13]に応用し、ロボットによる協調制御にも発展させたい。



参考文献


[1] Kenji Doya. Bayesian brain: Probabilistic approachesto neural coding. MIT press, 2007.
[2] Tadahiro Taniguchi, Yuto Yoshida, Akira Taniguchi, andYoshinobu Hagiwara. Emergent communication throughmetropolis-hastings naming game with deep generativemodels, 2023.
[3] Hiroto Ebara, Tomoaki Nakamura, Akira Taniguchi, andTadahiro Taniguchi. Multi-agent reinforcement learn-ing with emergent communication using discrete and in-diﬀerentiable message. In 2023 15th InternationalCongress on Advanced Applied Informatics Winter(IIAI-AAI-Winter), pp. 366–371. IEEE, 2023.
[4] Ziwoo You, Hiroto Ebara, Tomoaki Nakamura, AkiraTaniguchi, and Tadahiro Taniguchi. Multimodal contin-uous symbol emergence using a probabilistic generativemodel based on gaussian processes. 2024 IEEE Inter-national Conference on Development and Learning(ICDL), pp. 1–6, 2024.
[5] Issei Saito, Tomoaki Nakamura, Akira Taniguchi, TadahiroTaniguchi, Yohei Hayamizu, and Shiqi Zhang. Emergenceof continuous signals as shared symbols through emergentcommunication. 2024 IEEE International Conferenceon Development and Learning (ICDL), pp. 1–6, 2024.
[6] Tomoaki Nakamura, Takayuki Nagai, and TadahiroTaniguchi. Serket: An architecture for connecting stochas-tic models to realize a large-scale cognitive model, 2017.
[7] Fruits-360 dataset. Fruits-360 dataset, 2024. https://www.kaggle.com/datasets/moltean/fruits.
[8] Tomoaki Nakamura, Takaya Araki, Takayuki Nagai, andNaoto Iwahashi. Grounding of word meanings in la-tent dirichlet allocation-based multimodal concepts. Ad-vanced Robotics, Vol. 25, No. 17, pp. 2189–2206, 2011.
[9] Tomoaki Nakamura, Takayuki Nagai, Daichi Mochihashi,Ichiro Kobayashi, Hideki Asoh, and Masahide Kaneko.Segmenting continuous motions with hidden semi-markovmodels and gaussian processes. Frontiers in neuro-robotics, Vol. 11, p. 67, 2017.
[10] Aaron van den Oord, Oriol Vinyals, and KorayKavukcuoglu. Neural discrete representation learning,2018.
[11] Yann LeCun, L´eon Bottou, Yoshua Bengio, and PatrickHaﬀner. Gradient-based learning applied to documentrecognition. Proceedings of the IEEE, Vol. 86, No. 11,pp. 2278–2324, 1998.
[12] Eric Jang, Shixiang Gu, and Ben Poole. Categorical repa-rameterization with gumbel-softmax, 2017.
[13] Tony Z. Zhao, Vikash Kumar, Sergey Levine, and ChelseaFinn. Learning ﬁne-grained bimanual manipulation withlow-cost hardware, 2023.