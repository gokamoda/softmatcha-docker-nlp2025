æ–‡ã®åŸ‹ã‚è¾¼ã¿ã«åŠ¹æœçš„ãªé™çš„å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®ç²å¾—

å’Œç”°å´‡å² â€ƒå¹³å·å„ªä¼ â€ƒæ¸…æ°´è‰¯å¤ªéƒ â€ƒå·å³¶è²´å¤§ â€ƒæ–è—¤ä¾‘è¼



ZOZO Research



{takashi.wada,yuki.hirakawa,ryotaro.shimizu}@zozo.com



{takahiro.kawashima,yuki.saito}@zozo.com



æ¦‚è¦

æœ¬ç ”ç©¶ã¯ã€ æ–‡ã®åŸ‹ã‚è¾¼ã¿ã«æœ‰åŠ¹ãªé™çš„å˜èªãƒ™ã‚¯ãƒˆãƒ«(Static Word Embedding)ã‚’ææ¡ˆã™ã‚‹ã€‚
æ–‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‡ºåŠ›ã™ã‚‹ã‚ˆã†ã«äº‹å‰å­¦ç¿’ã•ã‚ŒãŸ Sentence Transformerã‹ã‚‰å˜èªãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡ºã—ã€ ã•ã‚‰ã«ä¸»æˆåˆ†åˆ†æã¨çŸ¥è­˜è’¸ç•™ã‚’è¡Œã†ã“ã¨ã§æ–‡è¡¨ç¾ã«é©ã—ãŸå˜èªãƒ™ã‚¯ãƒˆãƒ«ã‚’ç²å¾—ã™ã‚‹ã€‚
è¨ˆ 56 å€‹ã®è‹±èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰æˆã‚‹Massive Text Embedding Benchmark (MTEB)ã§å®Ÿé¨“ã‚’è¡Œã„ã€ æ—¢å­˜æ‰‹æ³•ã®ç²¾åº¦ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚


1 ã¯ã˜ã‚ã«

è¿‘å¹´ã€ æ§˜ã€…ãªã‚¿ã‚¹ã‚¯(æ¤œç´¢ãƒ»åˆ†é¡ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç­‰)ã«å¿œç”¨å¯èƒ½ãªæ–‡åŸ‹ã‚è¾¼ã¿ã®ãƒ¢ãƒ‡ãƒ«ã®ç ”ç©¶ãŒç››ã‚“ã«è¡Œã‚ã‚Œã¦ã„ã‚‹ã€‚
ç¾åœ¨ä¸»æµã®æ‰‹æ³•ã¯ã€ BERT[1]ã‚„æœ€è¿‘ã§ã¯ LLaMA 3[2]ã®ã‚ˆã†ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(LLM)ã‚’æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã§ ï¬ne-tune ã—ã€å…¥åŠ›æ–‡ã®æ„å‘³ã‚’æ‰ãˆãŸå›ºå®šé•·ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‡ºåŠ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹[3, 4]. ã—ã‹ã—ã€ ã“ã†ã—ãŸæ‰‹æ³•ã¯å¤šãã®è¨ˆç®—è³‡æºã‚’å¿…è¦ã¨ã™ã‚‹ãŸã‚ã€ å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½ã‚³ã‚¹ãƒˆã§é«˜é€Ÿã«å‡¦ç†ã™ã‚‹ã“ã¨ã‚„ã€ ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã®ã‚ˆã†ãªè¨ˆç®—è³‡æºãŒé™ã‚‰ã‚Œã‚‹æ©Ÿå™¨ã§ãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ã™ã“ã¨ãŒå›°é›£ã¨ãªã£ã¦ã„ã‚‹ã€‚
ãã“ã§æœ¬ç ”ç©¶ã¯ã€ æ–‡åŸ‹ã‚è¾¼ã¿ã«æœ‰åŠ¹ãªé™çš„å˜èªãƒ™ã‚¯ãƒˆãƒ«(Static Word Embedding)ã‚’ææ¡ˆã™ã‚‹ã€‚
æ–‡åŸ‹ã‚è¾¼ã¿ã®ã‚¿ã‚¹ã‚¯ã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸ Transformer[5]ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«(Sentence Transformer)[3]ã‹ã‚‰å˜èªãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡ºã—ã€ æ›´ã«ãã‚Œã‚’ä¸»æˆåˆ†åˆ†æã‚„çŸ¥è­˜è’¸ç•™ã§æ”¹å–„ã™ã‚‹ã€‚
æ¨è«–æ™‚ã¯ã€ ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®å¹³å‡ã§è¡¨ã™æ‰€è¬‚ bag-of-words ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ãŸã‚ã€ CPU ä¸Šã§ã‚‚é«˜é€Ÿã«å‹•ã‹ã™ã“ã¨ãŒå‡ºæ¥ã‚‹ã€‚
ç²¾åº¦è©•ä¾¡ã‚’è¨ˆ 56 å€‹ã®è‹±èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰æˆã‚‹ Massive Text EmbeddingBenchmark (MTEB)[6]ã§è¡Œã„ã€ æ—¢å­˜ã®é™çš„å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®ç²¾åº¦ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚
ã¾ãŸã€SimCSE[7]ã®ã‚ˆã†ãªåŸºç¤çš„ãª Sentence Transformer ã®ç²¾åº¦ã‚‚ä¸Šå›ã‚Šã€ å®Ÿå¿œç”¨ä¸Šã«ãŠã‘ã‚‹æœ‰åŠ¹æ€§ã‚‚ç¤ºã—ãŸã€‚



2 ææ¡ˆæ‰‹æ³•



2.1 é™çš„å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®æŠ½å‡º

ã¾ãšã€ æ–‡åŸ‹ã‚è¾¼ã¿ã«æœ€é©åŒ–ã•ã‚ŒãŸ Transformer ãƒ¢ãƒ‡ãƒ«(Sentence Transformer, ä»¥ä¸‹ ST)ã‹ã‚‰é™çš„å˜èªãƒ™ã‚¯ãƒˆãƒ«(Static Word Embedding, ä»¥ä¸‹ SWE)ã‚’æŠ½å‡ºã™ã‚‹ã€‚
ä»»æ„ã®èªå½™é›†åˆ ğ‘‰ ã«å«ã¾ã‚Œã‚‹å˜èª ğ‘¤ âˆˆ ğ‘‰ ã®ãƒ™ã‚¯ãƒˆãƒ«SWE(ğ‘¤) âˆˆ â„ğ‘‘ã‚’å¾—ã‚‹ãŸã‚ã«ã€ ã¾ãšå¤§è¦æ¨¡ãª Web ã‚³ãƒ¼ãƒ‘ã‚¹(CC-100[8, 9])ã‹ã‚‰ ğ‘¤ ã‚’å«ã‚“ã æ–‡ã‚’ ğ‘ (= 100)æ–‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ å„æ–‡ ğ‘§ğ‘–(ğ‘– = 1, 2, ...ğ‘)ã‚’ ST ã§åŸ‹ã‚è¾¼ã‚€ã€‚
ãã—ã¦ã€ Transformer ã®æœ€çµ‚å±¤ã‹ã‚‰å‡ºåŠ›ã•ã‚Œã‚‹|ğ‘§ğ‘–| å€‹ã®ãƒ™ã‚¯ãƒˆãƒ«(|ğ‘§ğ‘–| ã¯ ğ‘§ğ‘–ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°)ã®ã†ã¡ã€ ğ‘¤ãŒå…¥åŠ›ã•ã‚ŒãŸä½ç½®ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡ºã™ã‚‹ã€‚
ã“ã‚Œã‚’ ğ‘æ–‡ã§ãã‚Œãã‚Œè¨ˆç®—ã—ã€ ãã‚Œã‚‰ã®å¹³å‡ã‚’å–ã‚‹ã“ã¨ã§éæ–‡è„ˆåŒ–ã•ã‚ŒãŸ ğ‘¤ ã® SWE ã‚’å¾—ã‚‹ã€‚
ãªãŠã€ æœ¬ç ”ç©¶ã§ã¯èªå½™é›†åˆ ğ‘‰ ã‚’(å°æ–‡å­—åŒ–ã—ãŸ)Web ã‚³ãƒ¼ãƒ‘ã‚¹ã§æœ€ã‚‚ä½¿ç”¨é »åº¦ãŒé«˜ã„ 15 ä¸‡å˜èªã¨è¨­å®šã—ãŸãŸã‚ã€ ä½é »åº¦èªã¯ST ã® tokenizer ã§ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã«åˆ†å‰²ã•ã‚Œã‚‹ã€‚
ãã®å ´åˆã€ Transformer ã®æœ€çµ‚å±¤ã§ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®å¹³å‡ã‚’å–ã‚Šã€ãã‚Œã‚’ ğ‘ æ–‡ã§å¹³å‡ã—ãŸã‚‚ã®ã‚’ SWE ã¨ã¿ãªã™ã€‚



2.2 æ–‡ãƒ™ã‚¯ãƒˆãƒ«ã®ä¸»æˆåˆ†åˆ†æ

å‰ç¯€ã§å¾—ã‚‰ã‚ŒãŸ SWE ã«ä¸»æˆåˆ†åˆ†æ(PCA)ã‚’é©ç”¨ã—ã€ æ–‡è¡¨ç¾ã«ã‚ˆã‚Šé©ã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’ç²å¾—ã™ã‚‹ã€‚
ã¾ãšã€å‰ç¯€ã§ç”¨ã„ãŸæ–‡é›†åˆã‹ã‚‰ ğ‘€ (= 100k)æ–‡ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡ºã—ã€ å„æ–‡ã‚’ SWE ã®å¹³å‡ã§è¡¨ã™ã€‚
ãã—ã¦å¾—ã‚‰ã‚ŒãŸæ–‡ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ— ğ‘‹ âˆˆ â„ğ‘€ Ã—ğ‘‘ã« PCA ã‚’é©ç”¨ã—ã€ ä¸­å¿ƒåŒ–ã•ã‚ŒãŸè¡Œåˆ— ğ‘‹ âˆ’Â¯ğ‘‹ ã‚’æ–°ã—ã„è»¸ã«å†™åƒã™ã‚‹è¡Œåˆ—ğ‘Š âˆˆ â„ğ‘‘Ã—ğ‘‘ã‚’å¾—ã‚‹ã€‚
å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã‚’ ğ‘Š ã§å†™åƒã™ã‚‹ã“ã¨ã§æ–‡ã®åˆ†æ•£ãŒæ‰ãˆã‚„ã™ããªã‚Šã€ æ–‡åŸ‹ã‚è¾¼ã¿ã®ã‚¿ã‚¹ã‚¯ã§ç²¾åº¦ãŒæ”¹å–„ã—ãŸã€‚
ãªãŠã€ æ–‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ SWEã®å¹³å‡ã§è¡¨ã—ã¦ã„ã‚‹ãŸã‚ã€ äº‹å‰ã«å„å˜èª ğ‘¤ âˆˆ ğ‘‰ ã®SWE(ğ‘¤)ã‚’(Â¯ğ‘‹ ã‚’å¼•ã„ã¦ã‹ã‚‰)ğ‘Š ã§å†™åƒã™ã‚‹ã“ã¨ãŒå¯èƒ½ã¨ãªã‚‹(âˆµ ğ‘ŠâŠ¤(1ğ‘î›ğ‘¥ğ‘¥ âˆ’ ğ‘) =1ğ‘î›ğ‘¥ğ‘ŠâŠ¤(ğ‘¥ âˆ’ ğ‘)).ã¾ãŸã€ PCA ã¯æ¬¡å…ƒå‰Šæ¸›ã®æ‰‹æ³•ã¨ã—ã¦ã‚‚æ´»ç”¨ã§ãã‚‹ã€‚

ãã®å ´åˆã€ ğ‘Š ã®ç¬¬ 1 ä¸»æˆåˆ†ã‹ã‚‰ç¬¬ ğ‘‘â€²ä¸»æˆåˆ†ã¾ã§ã®å€¤ã‚’ç”¨ã„ã¦ ğ‘‘â€²(< ğ‘‘)æ¬¡å…ƒã«å‰Šæ¸›ã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã‚ã‚‹ãŒã€ æœ¬ç ”ç©¶ã§ã¯ç¬¬ ğ‘˜ ä¸»æˆåˆ†ã‹ã‚‰ç¬¬ ğ‘˜ + ğ‘‘â€²ä¸»æˆåˆ†ã®å€¤ã‚’ç”¨ã„ã¦æ¬¡å…ƒå‰Šæ¸›ã‚’è¡Œã†ã€‚
ã“ã‚Œã¯ã€ æœ€åˆã®ç¬¬ ğ‘˜ ä¸»æˆåˆ†ã‚’å–ã‚Šé™¤ãã“ã¨ã«ã‚ˆã‚Š SWE ã®ç²¾åº¦ãŒè‰¯ããªã‚‹ã“ã¨ãŒæ–‡çŒ®[10]ã§ç¤ºã•ã‚Œã¦ã„ã‚‹ãŸã‚ã§ã€ å®Ÿéš›ã«æœ¬ç ”ç©¶ã§ã‚‚åŒæ§˜ã®çµæœã¨ãªã£ãŸã€‚
æ–‡çŒ®[10]ã«å€£ã„ã€ ğ‘˜ = âŒŠğ‘‘100âŒ‹ ã¨ã—ãŸã€‚
ã¨ã“ã‚ã§ã€ SWE ã®æ¬¡å…ƒå‰Šæ¸›ã¯ SWE ã®èªå½™è¡Œåˆ—ğ‘Œ âˆˆ â„|ğ‘‰ | Ã—ğ‘‘ã« PCA ã‚’é©ç”¨ã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã‚ã‚‹ãŒã€ä¸Šè¨˜ã®æ‰‹æ³•ã‚ˆã‚Šã‚‚æ–‡åŸ‹ã‚è¾¼ã¿ã®ç²¾åº¦ãŒæ‚ªåŒ–ã—ãŸã€‚
ã“ã‚Œã¯ã€ æ–‡åŸ‹ã‚è¾¼ã¿ã§ã¯å˜èªã‚ˆã‚Šæ–‡ã®æ„å‘³ã®åˆ†æ•£ã‚’æ‰ãˆã‚‹æ–¹ãŒé‡è¦ã§ã‚ã‚‹ã“ã¨ã«èµ·å› ã™ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚


2.3 çŸ¥è­˜è’¸ç•™

æœ€å¾Œã«,æ¬¡å…ƒå‰Šæ¸›ã—ãŸSWEã‚’çŸ¥è­˜è’¸ç•™(KnowledgeDistillation)[11]ã§ã•ã‚‰ã«æ”¹å–„ã™ã‚‹ã€‚
çŸ¥è­˜è’¸ç•™ã®æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¯ç¯€ 2.1 ã§ SWE ã®æŠ½å‡ºã«ç”¨ã„ãŸ ST, ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã¯ SWE ã®å¹³å‡ã§æ–‡ã‚’åŸ‹ã‚è¾¼ã‚€ãƒ¢ãƒ‡ãƒ«ã¨ã™ã‚‹ã€‚
ä»»æ„ã® 2 ã¤ã®æ–‡ãŒä¸ãˆã‚‰ã‚ŒãŸæ™‚ã€ ãã‚Œã‚‰ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ã¾ãšæ•™å¸«ãƒ¢ãƒ‡ãƒ«ã§è¨ˆç®—ã—ã€ ãã®å€¤ã‚’ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã§å†ç¾ã§ãã‚‹ã‚ˆã†ã« SWE ã‚’ ï¬ne-tune ã™ã‚‹ã€‚
å…·ä½“çš„ã«ã¯ã€ ğ¾ (= 128)æ–‡ã‹ã‚‰æˆã‚‹ãƒŸãƒ‹ãƒãƒƒãƒã«å¯¾ã—ã¦ä»¥ä¸‹ã®loss ã‚’ä¸‹ã’ã‚‹ã‚ˆã†ã«å­¦ç¿’ã™ã‚‹:loss = âˆ’1ğ¾ğ¾î›•ğ‘–=1ğ¾î›•ğ‘–â‰  ğ‘—, ğ‘—=1q(ğ‘¡ğ‘–, ğ‘—)log(p(ğ‘ ğ‘–, ğ‘—)),p(ğ‘ ğ‘–, ğ‘—) =ğ‘’ğ‘ ğ‘–, ğ‘—ğœî›ğ¾ğ‘˜â‰ ğ‘–,ğ‘˜=1ğ‘’ğ‘ ğ‘–,ğ‘˜ğœ, q(ğ‘¡ğ‘–, ğ‘—) =ğ‘’ğ‘¡ğ‘–, ğ‘—ğœî›ğ¾ğ‘˜â‰ ğ‘–,ğ‘˜=1ğ‘’ğ‘¡ğ‘–,ğ‘˜ğœ.ã“ã“ã§ã€ ğ‘¡ğ‘–, ğ‘—ã¨ ğ‘ ğ‘–, ğ‘—ã¯ãã‚Œãã‚Œã€ æ•™å¸«ãƒ¢ãƒ‡ãƒ«åŠã³ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã§è¨ˆç®—ã—ãŸãƒŸãƒ‹ãƒãƒƒãƒå†…ã® ğ‘– ç•ªç›®ã¨ ğ‘— ç•ªç›®ã®æ–‡ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§ã‚ã‚‹ã€‚
ãªãŠã€ ãƒŸãƒ‹ãƒãƒƒãƒå†…ã®æ–‡ã¯ç¯€ 2.1 ã§ç”¨ã„ãŸæ–‡ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡ºã™ã‚‹ã€‚
ğœ ã¯é¡ä¼¼åº¦è¡Œåˆ—ã®å°–åº¦ã‚’ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€ æœ¬ç ”ç©¶ã§ã¯ ğœ = 0.05 ã¨ã—ãŸã€‚
æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å›ºå®šã—ã¦ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’ï¬ne-tune ã™ã‚‹ã“ã¨ã§ã€ SWE ã‚’ã•ã‚‰ã«æ–‡åŸ‹ã‚è¾¼ã¿ã«æœ€é©åŒ–ã™ã‚‹ã€‚
ãªãŠã€ æ–‡çŒ®[12]ã¯ã“ã‚Œã¨åŒæ§˜ã®æ‰‹æ³•ã‚’ç”¨ã„ã¦ ST ã‹ã‚‰ BERT ã®ã‚ˆã†ãªä»–ã® Transformer ãƒ¢ãƒ‡ãƒ«ã«çŸ¥è­˜è’¸ç•™ã‚’è¡Œã„ã€ ãã®æœ‰åŠ¹æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
ãªãŠã€ ç¯€ 2.1 ã® SWE ã®æŠ½å‡ºã¨æœ¬ç¯€ã®çŸ¥è­˜è’¸ç•™ã‚’è¡Œã†éš›ã€ ST ã«ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®å¤šã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ ç²¾åº¦ãŒæ‚ªåŒ–ã—ãŸã€‚
ã“ã‚Œã¯ã€ å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤šããªã‚‹ã«ã¤ã‚Œã¦(1) ST ã®æœ€çµ‚å±¤ã§å…¥åŠ›å˜èª ğ‘¤ ä»¥å¤–ã®æƒ…å ±ã‚’(self-attention ã«ã‚ˆã‚Š)å¤šãå«ã‚€ã‚ˆã†ã«ãªã‚Šã€ (2)æ•™å¸«ã¨ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®è€ƒæ…®ã§ãã‚‹æƒ…å ±(=èªé †)ã®å·®ãŒåºƒãŒã£ã¦çŸ¥è­˜è’¸ç•™ãŒå›°é›£ã«ãªã‚‹ã€ ã¨ã„ã†ã“ã¨ã«èµ·å› ã™ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚
æ–‡ã®æŠ½å‡ºã«ç”¨ã„ãŸ CC-100 ã¯ï¼‘è¡Œã«é•·ã„æ–‡ç« ã‚’å«ã‚€ã“ã¨ãŒå¤šã€…ã‚ã‚‹ãŸã‚ã€ ãã‚Œã‚‰ã‚’ã¾ãšæ–‡ã«åˆ†å‰²ã—ã€1ï¼‰ğ‘¤ âˆˆ ğ‘‰ ã‚’å«ã‚€æ–‡ã‚’æœ€å¤§ 2,000 æ–‡ãšã¤æŠ½å‡ºã—ã€ ãã®ã†ã¡ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒæœ€ã‚‚å°‘ãªã„ ğ‘ æ–‡ã‚’SWE ã®æŠ½å‡ºã€ ä¸»æˆåˆ†åˆ†æã€ çŸ¥è­˜è’¸ç•™ã«ç”¨ã„ãŸã€‚



2.4 å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«

æ©Ÿæ¢°å­¦ç¿’ã®æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€ è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã™ã‚‹ã“ã¨ã®æœ‰åŠ¹æ€§ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚
ãã“ã§ã€ æœ¬ç ”ç©¶ã¯ ğ¿ å€‹ã® SWE (SWE1, . . . , SWEğ¿)ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã—ã€ ãƒ†ã‚­ã‚¹ãƒˆ ğ‘§ ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«åŸ‹ã‚è¾¼ã‚€ã“ã¨ã‚’ææ¡ˆã™ã‚‹:E(ğ‘§) =1î™±î›ğ¿ğ‘–=1ğ‘2ğ‘–[ğ‘1f(E1(ğ‘§)), . . . , ğ‘ğ¿f(Eğ¿(ğ‘§))], (1)Eğ‘–(ğ‘§) =î›•ğ‘¤â€²âˆˆğ‘§SWEğ‘–(ğ‘¤â€²). (2)ã“ã“ã§ã€ f ã¯ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒãƒ«ãƒ ã‚’ 1 ã«æ­£è¦åŒ–ã™ã‚‹é–¢æ•°ã€ ğ‘ğ‘–> 0 ã¯å„ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã«å¯¾å¿œã™ã‚‹ã‚¹ã‚«ãƒ©ãƒ¼å€¤ã€ ãã—ã¦[ğ‘¥1, . . . , ğ‘¥ğ¿]ã¯ ğ‘¥1, . . . , ğ‘¥ğ¿ã‚’çµåˆã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’è¡¨ã™ã€‚
E(ğ‘§)ã§ 2 ã¤ã®æ–‡ã®å†…ç©ã‚’å–ã‚‹ã¨ã€SWE1, . . . , SWEğ¿ã§ãã‚Œãã‚Œè¨ˆç®—ã—ãŸã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ ğ‘2ğ‘–ã§é‡ã¿ä»˜ã‘ã—ãŸåŠ é‡å¹³å‡ã¨ãªã‚Šã€ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã¨ãªã‚‹ã€‚
æœ¬ç ”ç©¶ã§ã¯ç°¡å˜ã®ãŸã‚ ğ‘1= Â· Â· Â· = ğ‘ğ¿= 1ã§å®Ÿé¨“ã‚’è¡Œã†ã€‚
ãªãŠã€ ST ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã¨ã¯ç•°ãªã‚Šã€æœ¬æ‰‹æ³•ã¯æ–‡åŸ‹ã‚è¾¼ã¿ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’ã‚ã¾ã‚Šå¢—ã‚„ã•ãšã«é©ç”¨ã§ãã‚‹ã¨ã„ã†åˆ©ç‚¹ãŒã‚ã‚‹ã€‚
ãªãœãªã‚‰ã€ å„å˜èªğ‘¤ âˆˆ ğ‘‰ ã«ã¤ã„ã¦ SWE(ğ‘¤) = [SWE1(ğ‘¤), . . . , SWEğ¿(ğ‘¤)]ã¨äº‹å‰ã«çµåˆã—ã¦ãŠã‘ã°ã€ E(ğ‘§)ã¯ SWE(ğ‘¤)ã®å’Œã‚’å–ã£ãŸå¾Œã«å„ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒã§ãã‚Œãã‚Œæ­£è¦åŒ–ã™ã‚‹ã“ã¨ã§ã€ ç°¡å˜ã«æ±‚ã‚ã‚‰ã‚Œã‚‹ã‹ã‚‰ã§ã‚ã‚‹ã€‚
2ï¼‰

3 å®Ÿé¨“



3.1 ãƒ¢ãƒ‡ãƒ«

æœ¬ææ¡ˆæ‰‹æ³•ã®å­¦ç¿’ã«ç”¨ã„ã‚‹ ST ã¨ã—ã¦ GTE-base3ï¼‰[13]ã‚’é¸ã‚“ã ã€‚
ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯æ§˜ã€…ãªãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ BERT ã‚’ ï¬ne-tune ã—ãŸã‚‚ã®ã§ã€ ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ãŒ 768 ã¨æ¯”è¼ƒçš„è»½é‡ãªãŒã‚‰ MassiveText Embedding Benchmark (MTEB)[6]ã¨å‘¼ã°ã‚Œã‚‹æ–‡ãƒ™ã‚¯ãƒˆãƒ«è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã«ãŠã„ã¦é«˜ã„ç²¾åº¦ã‚’é”æˆã—ã¦ã„ã‚‹ã€‚
ã¾ãš GTE ã‹ã‚‰ SWE ã‚’ç¯€ 2.1 ã®æ‰‹æ³•ã§æŠ½å‡ºã—ã€ ç¯€1ï¼‰ https://github.com/microsoft/BlingFire ã‚’ç”¨ã„ãŸ2ï¼‰ ãŸã ã—ã€ é¡ä¼¼åº¦è¨ˆç®—ã®ã‚³ã‚¹ãƒˆã‚„ãƒ¡ãƒ¢ãƒªãƒ¼æ¶ˆè²»é‡ã¯å¢—åŠ ã™ã‚‹ã€‚
3ï¼‰ https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5

2.2 ã§ PCA ã‚’ç”¨ã„ã¦æ¬¡å…ƒæ•°ã‚’ ğ‘‘â€²= {256, 512} ã¾ã§å‰Šæ¸›ã—ãŸã€‚
ç¯€ 2.3 ã§ã¯å­¦ç¿’ç‡ 0.001 ã® Adam[14]ã‚’ç”¨ã„ã¦æœ€å¤§ 3 ä¸‡ã‚¹ãƒ†ãƒƒãƒ—å­¦ç¿’ã—ã€ æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã® loss ãŒä¸‹ãŒã‚‰ãªããªã‚Œã°å­¦ç¿’ã‚’æ­¢ã‚ãŸã€‚
å­¦ç¿’ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¯ç¯€ 2.1 ã§ç”¨ã„ãŸæ–‡ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡ºã—ãŸã€‚
ææ¡ˆæ‰‹æ³•ã¨ç›´æ¥æ¯”è¼ƒå¯èƒ½ãª SWE ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ GloVe [15]ã¨ Sent2Vec [16]ã‚’é¸ã‚“ã ã€‚
ä¸¡ãƒ¢ãƒ‡ãƒ«ã¨ã‚‚ãƒ©ãƒ™ãƒ«ãªã—ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ç”¨ã„ã¦å˜èªã®å…±èµ·æƒ…å ±ã‚’å…ƒã«å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ãŒã€ Sent2Vec ã¯åå‰ã®é€šã‚Šæ–‡åŸ‹ã‚è¾¼ã¿ã«æœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«ã¨ãªã£ã¦ã„ã‚‹ã€‚
ã¾ãŸã€ ææ¡ˆæ‰‹æ³•ã¨åŒæ§˜ã« ST ã‚’ç”¨ã„ã¦å­¦ç¿’ã—ãŸ SWEãŒæœ€è¿‘ 2 ã¤ã®ç•°ãªã‚‹ GitHub ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§å…¬é–‹ã•ã‚ŒãŸãŸã‚(WordLlama4ï¼‰[17], Model2Vec5ï¼‰[18]), ã“ã‚Œã‚‰ã‚‚ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«å«ã‚è©•ä¾¡ã—ãŸã€‚
ãŸã ã—ã€ ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã«ã¯è«–æ–‡ã‚„è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€å®Ÿé¨“è¨­å®šã‚„å­¦ç¿’ã®ç´°éƒ¨ã‚’æŠŠæ¡ã™ã‚‹ã®ãŒå›°é›£ã¨ãªã£ã¦ã„ã‚‹ã€‚
WordLlama ã¯ LLaMa 3[2]ã®å…¥åŠ›å±¤ã§ä½¿ã‚ã‚Œã‚‹ SWE ã‚’æŠ½å‡ºã—ã€ ãã‚Œã‚’æ§˜ã€…ãªãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦æ”¹å–„ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã€ Model2Vec ã¯ã€ BGE6ï¼‰[19]ã«ğ‘¤ âˆˆ ğ‘‰ ã‚’ï¼‘å˜èªãšã¤(æ–‡è„ˆãªã—ã§)å…¥åŠ›ã—ã¦å‡ºåŠ›ã•ã‚Œã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’ SWE(ğ‘¤)ã¨ã¿ãªã—ã€ æ›´ã«ã€ ä»»æ„ã®æ–‡ã‚’BGE ã«å…¥åŠ›ã—ã¦å‡ºåŠ›ã•ã‚Œã‚‹æ–‡ãƒ™ã‚¯ãƒˆãƒ«ã¨ SWE ã®å¹³å‡ã§å¾—ã‚‰ã‚Œã‚‹æ–‡ãƒ™ã‚¯ãƒˆãƒ«ã®äºŒä¹—èª¤å·®ã‚’æœ€å°åŒ–ã™ã‚‹å­¦ç¿’ã‚’è¡Œã„ SWE(ğ‘¤)ã‚’æ”¹å–„ã™ã‚‹ã€‚
Model2Vec ã¯ãã®å¾Œã« SWE ã®èªå½™è¡Œåˆ—ã«å¯¾ã—ã¦ PCA ã‚’é©ç”¨ã—ã¦æ¬¡å…ƒå‰Šæ¸›ã‚’è¡Œã„ã€ æœ€å¾Œã«é«˜(ä½)é »åº¦èªã®ãƒãƒ«ãƒ ã‚’å°ã•ã(å¤§ãã)ã™ã‚‹ã¨ã„ã†å¤å…¸çš„ãª heuristic ã‚’ç”¨ã„ã¦ã„ã‚‹ã€‚



3.2 ãƒ‡ãƒ¼ã‚¿ã¨è©•ä¾¡

æ–‡åŸ‹ã‚è¾¼ã¿ã®è©•ä¾¡ã«ã¯ã€ Massive Text EmbeddingBenchmark (MTEB)[6]ã‚’ç”¨ã„ãŸã€‚
ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ 7å€‹ã®ã‚¿ã‚¹ã‚¯(classiï¬cation, pair classiï¬cation, cluster-ing, reranking, retrieval, semantic textual similarity (STS),summarisation evaluation)ã‚’ç¶²ç¾…ã™ã‚‹ 56 å€‹ã®è‹±èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã€‚
è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã«å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã€ å¾—ã‚‰ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ†ã‚­ã‚¹ãƒˆé–“ã®é¡ä¼¼åº¦è¨ˆç®—ã‚„åˆ†é¡å™¨ã®ç‰¹å¾´é‡ã¨ã—ã¦ç”¨ã„ã‚‹ã€‚
åˆ†é¡å™¨ã®å­¦ç¿’ç­‰ã¯ MTEB ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ¼å†…éƒ¨ã§è‡ªå‹•ã§è¡Œã‚ã‚Œã‚‹ã€‚
ãã—ã¦ã€ ãã‚Œãã‚Œã®ã‚¿ã‚¹ã‚¯ãƒ»æŒ‡æ¨™ã§ç²¾åº¦ã‚’ç®—å‡ºã—ã€ å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å¹³å‡ç²¾åº¦(Avg)ã§ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’è¡Œã†ã€‚
æ›´ã«ã€ æœ¬ç ”ç©¶ã§ã¯å…¨ 56 ãƒ‡ãƒ¼ã‚¿ã®ã†ã¡ Sentence-to-Sentence (S2S)ã¨ã‚¿ã‚°ä»˜ã‘ã•ã‚ŒãŸã€ å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒæ¯”è¼ƒçš„çŸ­ã„ 33 å€‹ã®ãƒ‡ãƒ¼4ï¼‰ https://github.com/dleemiller/WordLlama5ï¼‰ https://github.com/MinishLab/model2vec6ï¼‰ https://huggingface.co/BAAI/bge-base-en-v1.5ã‚¿ã‚»ãƒƒãƒˆã§ã®å¹³å‡ç²¾åº¦(Avg-s2s)ã‚‚æ¯”è¼ƒã™ã‚‹ã€‚
ã“ã‚Œã¯ã€1,000 å˜èªã‚’è¶…ãˆã‚‹ã‚ˆã†ãªé•·ã„æ–‡ç« ã‚’ SWE ã§è¡¨ã™ã®ã¯éå¸¸ã«å›°é›£ã§ã€ ã‚ã¾ã‚Šç¾å®Ÿçš„ã§ãªã„ãŸã‚ã§ã‚ã‚‹ã€‚


3.3 çµæœ

å®Ÿé¨“çµæœã‚’è¡¨ 1 ã«ç¤ºã™ã€‚
æœ€åˆã® 3 ãƒ¢ãƒ‡ãƒ«ã¯ ST ã§ã€ãã‚Œä»¥é™ã¯å…¨ã¦ SWE ã§ã‚ã‚‹ã€‚
SimCSE7ï¼‰[7]ã¯ BERTãƒ™ãƒ¼ã‚¹ã®ä»£è¡¨çš„ãª ST, GTE-base ã¯ææ¡ˆæ‰‹æ³•ã®æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã€ ãã—ã¦ LLM2Vec[4]ã¯ LLaMa 3 (8B)ã‚’ ï¬ne-tuneã—ãŸ ST ã§ã‚ã‚‹ã€‚
è¡¨ãŒç¤ºã™ã‚ˆã†ã«ã€ SWE ã®ä¸­ã§ã¯ææ¡ˆæ‰‹æ³•(OURS)ãŒ Avg åŠã³ Avg-s2s ã«ãŠã„ã¦æœ€ã‚‚ç²¾åº¦ãŒé«˜ããªã£ãŸã€‚
åŠ ãˆã¦ã€ OURS ãŒ SimCSE ã®ç²¾åº¦ã‚’ä¸Šå›ã£ãŸã®ã¯é©šãã®çµæœã§ã‚ã‚‹ã€‚
ãŸã ã—ã€ ä»–ã®å¼·åŠ›ãª ST ã«å¯¾ã—ã¦ã¯ Avg ã§å¤§ããä¸‹å›ã£ã¦ãŠã‚Šã€ ç‰¹ã«Retrieval ã®ã‚¿ã‚¹ã‚¯ã§å¤§ããå·®ã‚’ã¤ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã€‚
ã“ã‚Œã¯ã€ Retrieval ã§ã¯å˜èªæ•°ã®å¤šã„æ–‡æ›¸ãŒå…¥åŠ›ã¨ãªã‚‹ã“ã¨ãŒå¤šãã€ èªé †ã‚’è€ƒæ…®ã§ããªã„ SWE ã«ã¨ã£ã¦ä¸åˆ©ãªæ¡ä»¶ã ã‹ã‚‰ã§ã‚ã‚‹ã€‚
äº‹å®Ÿã€ Avg-s2s ã§ã¯ ST ã¨ã®å·®ãŒå°ã•ããªã£ã¦ãŠã‚Šã€ å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã„å ´åˆã«ãŠã„ã¦ã¯ SWE ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ãŒç¢ºã‹ã‚ã‚‰ã‚ŒãŸã€‚
æœ€å¾Œã® 2 è¡Œã¯ã€ OURS (256)ã¨ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ç¯€ 2.4ã®æ‰‹æ³•ã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã—ãŸçµæœã§ã‚ã‚‹ã€‚
OURS å˜ä½“ã¨æ¯”è¼ƒã—ã¦ã€ ç‰¹ã« Reranking ã¨ Retrieval ã§ç²¾åº¦ãŒæ”¹å–„ã—ãŸã€‚
ã“ã‚Œã‚‰ã®ã‚¿ã‚¹ã‚¯ã§ã¯ WordLlama ãŒæœ€ã‚‚ç²¾åº¦ãŒé«˜ã„ SWE ãªãŸã‚ã€ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã‚ˆã‚Š OURS ã®å¼±ç‚¹ã‚’è£œã†ã“ã¨ãŒã§ããŸã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚
ä¸€æ–¹ã€ WordLlamaãŒè‹¦æ‰‹ãª Classiï¬cation ã‚„ STS ã§ã¯ç²¾åº¦ãŒã‚„ã‚„æ‚ªåŒ–ã—ãŸãŸã‚ã€ å¼ 1 ã®é‡ã¿ ğ‘ğ‘–ã‚’å„ãƒ¢ãƒ‡ãƒ«ã®å¼·ã¿ã«å¿œã˜ã¦èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€ æ›´ãªã‚‹ç²¾åº¦æ”¹å–„ãŒæœŸå¾…ã§ãã‚‹ã€‚
æ¬¡ã«ã€ STS ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® 1 ã¤ã§ã‚ã‚‹ STS15 ã§ã®ç²¾åº¦ã¨å®Ÿè¡Œé€Ÿåº¦ã‚’è¡¨ 2 ã§æ¯”è¼ƒã—ãŸã€‚
æœ€åˆã® 2 ã¤ã® STã¨æ¯”ã¹ã¦ææ¡ˆæ‰‹æ³•ã¯ç²¾åº¦ã§ã‚„ã‚„åŠ£ã‚‹ã‚‚ã®ã®ã€ CPU ä¸Šã§ã‚‚éå¸¸ã«é«˜é€Ÿã«æ–‡ã‚’åŸ‹ã‚è¾¼ã‚€ã“ã¨ãŒã§ãã‚‹
ã€‚


4 åˆ†æ

ç¯€ 2.1, 2.2, 2.3 ã§å¾—ã‚‰ã‚Œã‚‹ SWE ã®ç²¾åº¦(Avg)ã¯ãã‚Œãã‚Œ 44.1, 49.7, 51.9 ã§ã‚ã‚Š(ğ‘‘â€²= 256), PCA ã¨çŸ¥è­˜è’¸ç•™ã§ç²¾åº¦ãŒæ”¹å–„ã™ã‚‹çµæœã¨ãªã£ãŸã€‚
ã¾ãŸã€ ç¯€ 2.2 ã§è¿°ã¹ãŸé€šã‚Šã€ ç¬¬ 1 ã‹ã‚‰ç¬¬ ğ‘‘â€²ä¸»æˆåˆ†ã¾ã§ã‚’ç”¨ã„ã¦æ¬¡å…ƒå‰Šæ¸›ã™ã‚‹ã¨ç¯€ 2.2 ã§ã®ç²¾åº¦ãŒ 49.7 ã‹ã‚‰ 48.7 ã«æ‚ªåŒ–ã—ã€ PCA ã‚’èªå½™è¡Œåˆ—ã«é©ç”¨ã™ã‚‹ã¨ 48.1 ã«æ‚ªåŒ–ã—ãŸã€‚
å„æ®µéšã§ SWE ãŒã©ã†å¤‰åŒ–ã—ã¦ã„ã‚‹ã‹ã‚’ç¤ºã™ãŸã‚ã«ã€ å„å˜èªã®ãƒãƒ«ãƒ ã®å¤‰åŒ–ã«æ³¨ç›®ã—ã¦åˆ†æã‚’è¡Œã£ãŸã€‚
ä»¥ä¸‹ã€ ç¯€ 2.1, 2.2, 2.3 ã«ãŠã‘ã‚‹å˜èª ğ‘¤ ã®ãƒãƒ«ãƒ 7ï¼‰ princeton-nlp/sup-simcse-bert-base-uncased

Models (ğ‘‘)Class. Clust. PairClass. Rerank. Retr. STS Summ. Avg-s2s Avg# Data Sets 12 11 3 4 15 10 1 33 56TransformersSimCSE-supervised (768) 67.32 33.43 74.90 47.54 21.82 79.12 31.17 62.86 48.93GTE-base (768) 77.17 46.82 85.33 57.66 54.09 81.97 31.17 71.51 64.11LLM2Vec-LLaMa3 (4,096) 75.92 46.45 87.80 59.68 56.63 83.58 30.94 72.94 65.01Static Word EmbeddingsGloVe (300) 57.29 27.73 70.92 43.29 21.62 61.85 28.87 52.63 41.97Sent2vec (700) 61.16 31.75 71.88 47.31 27.79 65.80 29.51 56.33 46.29WordLlama (256)[WL] 60.06 35.87 73.59 52.68 32.41 72.25 30.12 59.99 49.74Model2Vec (256)[MV] 64.67 32.94 76.62 49.72 30.43 73.24 29.20 60.67 49.74OURS (256)[OURSğ‘ ] 67.21 36.76 78.72 50.86 31.03 75.88 30.04 63.76 51.87OURS (512) 68.07 36.17 79.14 50.90 31.50 75.65 29.82 63.82 52.04OURSğ‘ + WL (512) 66.23 37.25 79.14 52.61 33.36 75.61 30.22 63.82 52.48OURSğ‘ + WL + MV (768) 67.12 37.17 79.03 52.09 33.97 75.28 30.27 63.86 52.72è¡¨ 1 MTEB ã§ã®å®Ÿé¨“çµæœã€‚
æ‹¬å¼§å†…ã®æ•°å­—ã¯å„ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°(ğ‘‘)ã‚’è¡¨ã™ã€‚
Models (ğ‘‘) ğœŒå®Ÿè¡Œé€Ÿåº¦(ç§’)GPU CPULLM2Vec (4,096) 0.88 30.3 >10,000GTE-base (768) 0.87 3.4 52.7OURS (256) 0.83 â€“ 0.5è¡¨ 2 å„ãƒ¢ãƒ‡ãƒ«ã® STS15 ã§ã®ç²¾åº¦(ã‚¹ãƒ”ã‚¢ãƒãƒ³ã®ç›¸é–¢ä¿‚æ•°ğœŒ)åŠã³å®Ÿè¡Œé€Ÿåº¦.é€Ÿåº¦ã¯NVIDIA A100-SXM4-40GBã§GPU ã‚ã‚Š/ãªã—ã§ 3 å›ç‹¬ç«‹ã«å®Ÿè¡Œã—ã€ å¹³å‡ã‚’å–ã£ãŸã€‚
ã‚’
n1(ğ‘¤), n2(ğ‘¤), n3(ğ‘¤)ã¨è¨˜ã™ã€‚
ã¾ãŸã€ ğ‘¤ ã‚’å“è©æƒ…å ±ãŒã‚¿ã‚°ä»˜ã‘ã•ã‚ŒãŸ Brown Corpus[20]ã§ä½¿ç”¨é »åº¦ãŒæœ€ã‚‚é«˜ã„ 1 ä¸‡èªã«é™å®šã™ã‚‹ã€‚
ã¾ãšã€ n2(ğ‘¤)/n1(ğ‘¤)ã®ä¸‹ä½100 å˜èªã«å¤šãç¾ã‚Œã‚‹å“è©ã¯å‹•è©(23), å‰ç½®è©(19),äººç§°ä»£åè©(8), be å‹•è©(7)ã¨ç¶šãã€8ï¼‰ä¸‹ä½ 3 å˜èªã¯of, to, an ã§ã‚ã£ãŸã€‚
ä¸€æ–¹ã€ ä¸Šä½ 100 å˜èªã®å“è©ã¯åè©(71), å›ºæœ‰åè©(19), å‹•è©(5)ã®é †ã§ã€ ä¸Šä½ 3 å˜èªã¯railroads, orioles, churchyard ã ã£ãŸã€‚
ã¤ã¾ã‚Šã€ PCA ã«ã‚ˆã£ã¦å‰ç½®è©ã‚„å† è©ãªã©ã®æ–‡ã®æ„å‘³ã«ã‚ã¾ã‚Šå½±éŸ¿ã—ãªã„å˜èªã®ãƒãƒ«ãƒ ãŒå°ã•ããªã‚Šã€ åå¯¾ã«åè©ã®ãƒãƒ«ãƒ ãŒå¤§ãããªã‚‹ã“ã¨ã‚’è¡¨ã—ã¦ã„ã‚‹ã€‚
ãªãŠã€ æ–‡çŒ®[21]ã¯ST ã«ã‚‚åè©ã®æƒ…å ±ã‚’å¤šãåŸ‹ã‚è¾¼ã‚€ãƒã‚¤ã‚¢ã‚¹ãŒã‚ã‚‹ã“ã¨ã‚’å ±å‘Šã—ã¦ã„ã‚‹ã€‚
ã¾ãŸã€ n3(ğ‘¤)/n2(ğ‘¤)ã®ä¸‹ä½ 100å˜èªã®å“è©ã¯å‰¯è©(16), äººç§°ä»£åè©(14), åè©(13)ã®é †ã§ã€ ä¸‹ä½ 3 å˜èªã¯ they, we, it ã§ã‚ã£ãŸã€‚
çŸ¥è­˜è’¸ç•™ã§ã¯ã€ ãŸã¨ãˆåè©ã§ã‚ã£ã¦ã‚‚æƒ…å ±é‡ã‚’ã‚ã¾ã‚ŠæŒãŸãªã„å˜èªã®ãƒãƒ«ãƒ ãŒå°ã•ããªã‚‹ã‚ˆã†ã«èª¿æ•´ã•ã‚Œã¦ã„ã‚‹ã¨8ï¼‰ å“è©ãŒè¤‡æ•°ã‚ã‚‹å˜èªã¯ä¸€ç•ªé »åº¦ãŒé«˜ã„å“è©ã‚’å‚ç…§ã—ãŸã€‚
è€ƒãˆã‚‰ã‚Œã‚‹ã€‚
ä¸€æ–¹ã€ ä¸Šä½ 100 å˜èªã®å“è©ã¯é †ã«å‹•è©(25), åè©(24), å½¢å®¹è©(18)ã¨æ§˜ã€…ã§ã€ ä¸Šä½ 3 å˜èªã¯which, unable, provide ã§ã‚ã£ãŸã€‚
ğ‘¤â„ğ‘–ğ‘â„ ã‚„ ğ‘¢ğ‘›ğ‘ğ‘ğ‘™ğ‘’ ã¯é«˜é »åº¦èªã§ã‚ã‚‹ã‚‚ã®ã®ã€ å…¥åŠ›æ–‡ã«ç–‘å•ã‚„å¦å®šãŒå«ã¾ã‚Œã‚‹ã‹ã©ã†ã‹ã®æ‰‹ãŒã‹ã‚Šã«ãªã‚‹å ´åˆãŒã‚ã‚‹ã®ã§ã€ ãƒãƒ«ãƒ ãŒå¤§ããèª¿æ•´ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
æœ€å¾Œã«ã€å˜èªã®å‡ºç¾é »åº¦ã¨ n1(ğ‘¤), n2(ğ‘¤), n3(ğ‘¤)ã®ã‚¹ãƒ”ã‚¢ãƒãƒ³ã®é †ä½ç›¸é–¢ã‚’è¦‹ã‚‹ã¨é †ã« âˆ’0.34, âˆ’0 .36, âˆ’0.37 ã§ã€ é«˜é »åº¦èªã®ãƒãƒ«ãƒ ãŒå¾ã€…ã«å°ã•ããªã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸã€‚


5 é–¢é€£ç ”ç©¶

æ–‡çŒ®[22, 23, 24]ã¯ã€ ç¯€ 2.1 ã¨åŒæ§˜ã®æ‰‹æ³•ã‚’ç”¨ã„ã¦BERTç­‰ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰SWEã‚’æŠ½å‡ºã—,ãã‚Œã‚’å˜èªã‚¢ãƒŠãƒ­ã‚¸ãƒ¼(word analogy)ã®ã‚ˆã†ãªå˜èªãƒ¬ãƒ™ãƒ«ã®ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡ã—æœ‰åŠ¹æ€§ã‚’ç¤ºã—ãŸã€‚
æ–‡åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ç ”ç©¶ã¯ç››ã‚“ã«è¡Œã‚ã‚Œã¦ã„ã‚‹ã‚‚ã®ã®ã€ è¿‘å¹´ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’åº¦å¤–è¦–ã—ãŸ LLaMa3[2]ã‚„ Mistral[25]ç­‰ã®LLM ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ãŒæ³¨ç›®ã‚’é›†ã‚ã¦ã„ã‚‹ã€‚
ä¸€æ–¹ã€ è¨ˆç®—ã‚³ã‚¹ãƒˆã®ä½ã„ SWE ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã¯å°‘ãªãã€ å¤å…¸çš„ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ Sent2Vec[16], æœ€è¿‘ã§ã¯ WordLlama[17]ã‚„Model2Vec[18]ãŒ GitHub ä¸Šã§å…¬é–‹ã•ã‚Œè©±é¡Œã¨ãªã£ãŸã€‚



6 çµè«–

æœ¬ç ”ç©¶ã¯ã€ æ–‡åŸ‹ã‚è¾¼ã¿ã«æœ‰åŠ¹ãªé™çš„å˜èªãƒ™ã‚¯ãƒˆãƒ«(SWE)ã‚’ææ¡ˆã—ãŸã€‚
Transformer ã®æ–‡åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ SWE ã‚’æŠ½å‡ºã—ã€ ä¸»æˆåˆ†åˆ†æã¨çŸ¥è­˜è’¸ç•™ã«ã‚ˆã‚Šç²¾åº¦ã‚’é«˜ã‚ãŸ.æ–‡åŸ‹ã‚è¾¼ã¿ã®ä»£è¡¨çš„ãªè©•ä¾¡ãƒ‡ãƒ¼ã‚¿MTEB ã§ã€ æ—¢å­˜ã® SWE ã‚ˆã‚Šç²¾åº¦ãŒé«˜ã„ã“ã¨ã‚’ç¤ºã—ãŸã€‚



å‚è€ƒæ–‡çŒ®


[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. BERT: Pre-training of deep bidirectional transform-ers for language understanding. In Proceedings of the 2019Conference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers), pp. 4171â€“4186, Minneapolis, Minnesota, June 2019. Association for Com-putational Linguistics.
[2] AI@Meta. Llama 3 model card. 2024.
[3] Nils Reimers and Ir yna Gurevych. Sentence-BERT: Sentence em-beddings using Siamese BERT-networks. In Proceedings of the2019 Conference on Empirical Methods in Natural Lan-guage Processing and the 9th International Joint Confer-ence on Natural Language Processing (EMNLP-IJCNLP),pp. 3982â€“3992, Hong Kong, China, November 2019. Associationfor Computational Linguistics.
[4] Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach,Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. LLM2vec:Large language models are secretly powerful text encoders. In FirstConference on Language Modeling, 2024.
[5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,Llion Jones, Aidan N Gomez, Å ukasz Kaiser, and Illia Polo-sukhin. Attention is all you need. In Advances in Neural Infor-mation Processing Systems 30, pp. 5998â€“6008. Curran Asso-ciates, Inc., 2017.
[6] Niklas Muennighoï¬€, Nouamane Tazi, Loic Magne, and NilsReimers. MTEB: Massive text embedding benchmark. In Pro-ceedings of the 17th Conference of the European Chapterof the Association for Computational Linguistics, pp. 2014â€“2037, Dubrovnik, Croatia, May 2023. Association for Computa-tional Linguistics.
[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Sim-ple contrastive learning of sentence embeddings. In Proceedingsof the 2021 Conference on Empirical Methods in NaturalLanguage Processing, pp. 6894â€“6910, Online and Punta Cana,Dominican Republic, November 2021. Association for Computa-tional Linguistics.
[8] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau,Vishrav Chaudhary, Francisco GuzmÃ¡n, Ar mand Joulin, andEdouard Grave. CCNet: Extracting high quality monolingualdatasets from web crawl data. In Proceedings of the TwelfthLanguage Resources and Evaluation Conference, pp. 4003â€“4012, Marseille, France, May 2020. European Language ResourcesAssociation.
[9] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, VishravChaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, EdouardGrave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un-supervised cross-lingual representation learning at scale. In Pro-ceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics, pp. 8440â€“8451, Online, July2020. Association for Computational Linguistics.
[10] Jiaqi Mu and Pramod Viswanath. All-but-the-top: Simple and ef-fective postprocessing for word representations. In InternationalConference on Learning Representations, 2018.
[11] Geoï¬€rey Hinton, Or iol Vinyals, and Jeï¬€rey Dean. Distilling theknowledge in a neural network. In NIPS Deep Learning andRepresentation Learning Workshop, 2015.
[12] Jiahao Xu, Wei Shao, Lihui Chen, and Lemao Liu. DistillCSE: Dis-tilled contrastive learning for sentence embeddings. In Findingsof the Asso ciation for Computational Linguistics: EMNLP2023, pp. 8153â€“8165, Singapore, December 2023. Association forComputational Linguistics.
[13] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, PengjunXie, and Meishan Zhang. Towards general text embeddings withmulti-stage contrastive learning, 2023.
[14] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochasticoptimization. In Proceedings of the 3rd International Con-ference on Learning Representations, May 2015.
[15] Jeï¬€rey Pennington, Richard Socher, and Christopher Manning.GloVe: Global vectors for word representation. In Proceedingsof the 2014 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pp. 1532â€“1543, Doha, Qatar,October 2014. Association for Computational Linguistics.
[16] Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. Unsuper-vised learning of sentence embeddings using compositional n-gramfeatures. In Proceedings of the 2018 Conference of theNorth American Chapter of the Association for Compu-tational Linguistics: Human Language Technologies, Vol-ume 1 (Long Papers), pp. 528â€“540, New Orleans, Louisiana,June 2018. Association for Computational Linguistics.
[17] D. Lee Miller. Wordllama: Recycled token embeddings from largelanguage models, 2024.
[18] Model2vec: The fastest state-of-the-art static embeddings in theworld, 2024.
[19] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoï¬€.C-pack: Packaged resources to advance general chinese embed-ding, 2023.
[20] H. Kucera and W. N. Francis. Computational analysis ofpresent-day American English. Brown University Press, 1967.
[21] Dmitry Nikolaev and Sebastian PadÃ³. Representation biases in sen-tence transformers. In Proceedings of the 17th Conferenceof the European Chapter of the Association for Compu-tational Linguistics, pp. 3701â€“3716, Dubrovnik, Croatia, May2023. Association for Computational Linguistics.
[22] Kawin Ethayarajh. How contextual are contextualized word repre-sentations? Comparing the geometry of BERT, ELMo, and GPT-2embeddings. In Proceedings of the 2019 Conference on Em-pirical Methods in Natural Language Processing and the9th International Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pp. 55â€“65, Hong Kong, China,November 2019. Association for Computational Linguistics.
[23] Rishi Bommasani, Kelly Davis, and Claire Cardie. InterpretingPretrained Contextualized Representations via Reductions to StaticEmbeddings. In Proceedings of the 58th Annual Meeting ofthe Association for Computational Linguistics, pp. 4758â€“4781, Online, July 2020. Association for Computational Linguis-tics.
[24] Takashi Wada, Timothy Baldwin, Yuji Matsumoto, and Jey HanLau. Unsupervised lexical substitution with decontextualised em-beddings. In Proceedings of the 29th International Confer-ence on Computational Linguistics, pp. 4172â€“4185, Gyeongju,Republic of Korea, October 2022. International Committee onComputational Linguistics.
[25] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, ChrisBamford, Devendra Singh Chaplot, Diego de las Casas, FlorianBressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,LÃ©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven LeScao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, andWilliam El Sayed. Mistral 7b, 2023.