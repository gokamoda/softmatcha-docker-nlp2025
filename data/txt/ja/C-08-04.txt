ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µã‚’ç”¨ã„ãŸæ®µéšçš„äº‹å‰å­¦ç¿’ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®åŠ¹ç‡çš„ãªæ§‹ç¯‰

çŸ¢é‡ ä¸€æ¨¹

1

é«˜ç€¬ ç¿”

1,2

å°æ— é¢¯ä»‹

1

æ¸…é‡ èˆœ

2

éˆ´æœ¨ æ½¤

11

æ±åŒ—å¤§å­¦

2

SB Intuitions æ ªå¼ä¼šç¤¾



yano.kazuki@dc.tohoku.ac.jp {sosuke.kobayashi.b2,jun.suzuki}@tohoku.ac.jp



{sho.takase,shun.kiyono}@sbintuitions.co.jp



æ¦‚è¦

å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å®Ÿå¿œç”¨ã§ã¯ã€7Bï¼Œ13Bï¼Œ70Bã¨ã„ã£ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ç•°ãªã‚‹è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ¢ãƒ‡ãƒ«ç³»åˆ—ï¼‰ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒä¸€èˆ¬çš„ã§ã‚ã‚‹ã€‚
ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®æ§‹ç¯‰ã¯ã€ç´ æœ´ã«ã¯å„ã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«æ§‹ç¯‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€è¨ˆç®—ã‚³ã‚¹ãƒˆã¯åŠ ç®—çš„ã«å¢—åŠ ã™ã‚‹ã€‚
æœ¬ç ”ç©¶ã§ã¯ã€å°ã•ãªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ®µéšçš„ã«å­¦ç¿’ã‚’é€²ã‚ã€ã‚µã‚¤ã‚ºã‚’æ‹¡å¼µã•ã›ãªãŒã‚‰ã€ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã‚’æ§‹ç¯‰ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚
å®Ÿé¨“ã§ã¯ã€ææ¡ˆæ‰‹æ³•ãŒè¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã¤ã¤ã€å€‹åˆ¥ã«ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã‚’å­¦ç¿’ã™ã‚‹å ´åˆã¨æ¯”è¼ƒã—ã¦åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚



1 ã¯ã˜ã‚ã«

å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(LLM)ãŒå®Ÿå¿œç”¨ã§å¹…åºƒãæ´»ç”¨ã•ã‚Œã‚‹ãªã‹ã§ã€ç•°ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«(ä»¥ä¸‹ã€ãƒ¢ãƒ‡ãƒ«ç³»åˆ—)ã‚’ã€æ§‹ç¯‰ãƒ»æä¾›ã™ã‚‹ã“ã¨ãŒä¸€èˆ¬çš„ã¨ãªã£ã¦ã„ã‚‹ã€‚
ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®æä¾›ã¯ã€å¤šæ§˜ãªè¨ˆç®—è³‡æºã®åˆ¶ç´„ã‚„ç”¨é€”ã«å¯¾å¿œã™ã‚‹ãŸã‚ã®é‡è¦ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ãªã£ã¦ã„ã‚‹ã€‚
ä¾‹ãˆã°ã€Llama 2 ã§ã¯ 7Bï¼Œ13Bï¼Œ70B ã®ãƒ¢ãƒ‡ãƒ«[1]ãŒã€Qwen2 ã§ã¯ 0.5Bï¼Œ1.5Bï¼Œ7Bï¼Œ72B ã®ãƒ¢ãƒ‡ãƒ«[2]ãŒå…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚
å°è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã¯ã€æ—¥å¸¸çš„ãªã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹åŠ¹ç‡çš„ãªå‡¦ç†ã¨é«˜é€Ÿãªå¿œç­”ã‚’å‚™ãˆã¤ã¤ã€ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã‚„ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã¨ã„ã£ãŸè¨ˆç®—è³‡æºã®åˆ¶ç´„ãŒå³ã—ã„ç’°å¢ƒã§ã®å±•é–‹ã‚’å¯èƒ½ã«ã™ã‚‹[3]ã€‚
ä¸€æ–¹ã€å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã¯ã€é«˜åº¦ãªæ¨è«–èƒ½åŠ›ã‚„è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã®å‡¦ç†ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹å ´é¢ã§ä½¿ç”¨ã•ã‚Œã€é€šå¸¸ã¯å¤§è¦æ¨¡ãªã‚µãƒ¼ãƒä¸Šã«é…å‚™ã•ã‚Œã‚‹ã€‚
ã“ã®ã‚ˆã†ã«ç•°ãªã‚‹ç‰¹æ€§ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã‚’ç³»åˆ—ã¨ã—ã¦æä¾›ã™ã‚‹ã“ã¨ã§ã€ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®å¹…åºƒã„è¦æ±‚ã«å¿œãˆã‚‹ã“ã¨ãŒå¯èƒ½ã§ã‚ã‚‹ã€‚
ã“ã†ã—ãŸãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®æ§‹ç¯‰ã§ã¯å„ã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ã™ã‚‹æ‰‹ç¶šããŒä¸€èˆ¬çš„ã§â€¦ â€¦ â€¦ â€¦ â€¦â€¦ â€¦ â€¦ â€¦ â€¦â€¦ â€¦ â€¦ â€¦ â€¦ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µãƒ¢ãƒ‡ãƒ«æ‹¡å¼µãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã«ã‚ã‚‹å„ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«å­¦ç¿’ç·è¨ˆç®—é‡ã¯å„ãƒ¢ãƒ‡ãƒ«ã®è¨ˆç®—é‡ã®åˆè¨ˆãƒ¢ãƒ‡ãƒ«æ‹¡å¼µã‚’åˆ©â½¤ã—ã€ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«æ§‹ç¯‰ç·è¨ˆç®—é‡ã¯æœ€â¼¤ã‚µã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã®è¨ˆç®—é‡ã®ã¿2B ãƒ¢ãƒ‡ãƒ«2Bâ½¤ã®è¨ˆç®—é‡4Bâ½¤ã®è¨ˆç®—é‡8Bâ½¤ã®è¨ˆç®—é‡4B ãƒ¢ãƒ‡ãƒ«8B ãƒ¢ãƒ‡ãƒ«2Bâ½¤ã®è¨ˆç®—é‡4Bâ½¤ã®è¨ˆç®—é‡8Bâ½¤ã®è¨ˆç®—é‡â€¦ â€¦ â€¦ â€¦ â€¦â€¦ â€¦ â€¦ â€¦ â€¦â€¦ â€¦ â€¦ â€¦ â€¦å›³ 1 ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®ä¸€èˆ¬çš„ãªæ§‹ç¯‰æ‰‹ç¶šã(å·¦å›³(a))ã¨æœ¬ç ”ç©¶ã§ã®æ§‹ç¯‰æ‰‹ç¶šã(å³å›³(b))ã®æ¦‚è¦å›³ã€‚ã€‚
ã‚ã‚Šã€å¿…è¦ãªè¨ˆç®—è³‡æºã¯å˜ç´”ã«åŠ ç®—çš„ã«å¢—åŠ ã—ã¦ã„ãã€‚
ç‰¹ã«å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ã¯æ•°åƒ GPU æ—¥ã¨ã„ã†è¨ˆç®—è³‡æºã‚’è¦ã™ã‚‹[4]ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã‚’é–‹ç™ºã™ã‚‹éš›ã®ç·è¨ˆç®—ã‚³ã‚¹ãƒˆã¯ç ”ç©¶æ©Ÿé–¢ã‚„ä¼æ¥­ã«ã¨ã£ã¦å¤§ããªè² æ‹…ã¨ãªã£ã¦ã„ã‚‹ã€‚
å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«è¦ã™ã‚‹è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã™ã‚‹æ‰‹æ³•ã¨ã—ã¦ã€å­¦ç¿’æ¸ˆã¿ã®å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’æ‹¡å¼µã—ã¦å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸå€¤ã¨ã™ã‚‹æ‰‹æ³•ãŒæ³¨ç›®ã•ã‚Œã¦ã„ã‚‹[5, 6]ã€‚
æœ¬ç ”ç©¶ã§ã¯ã€ã“ã®æ‰‹æ³•ã‚’ç¹°ã‚Šè¿”ã—é©ç”¨ã™ã‚‹ã“ã¨ã§ã€å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¸ã®æ®µéšçš„ãªæ§‹ç¯‰ã‚’è¡Œã„ã€ãƒ¢ãƒ‡ãƒ«ç³»åˆ—æ§‹ç¯‰ã®ç·è¨“ç·´ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹(å›³ 1)ã€‚
å®Ÿé¨“ã«ã‚ˆã‚Šã€ææ¡ˆæ‰‹æ³•ã§ã¯ã€å€‹ã€…ã®ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«æ§‹ç¯‰ã™ã‚‹å ´åˆã‚ˆã‚Šã‚‚å°‘ãªã„è¨ˆç®—ã‚³ã‚¹ãƒˆã§ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã‚’å¾—ã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚
ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ãŸå­¦ç¿’ç‡ã®æ®µéšçš„ãªèª¿æ•´ã«ã‚ˆã‚Šã€ææ¡ˆæ‰‹æ³•ã«ã‚ˆã‚Šæ§‹ç¯‰ã—ãŸãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã¯ã€å€‹åˆ¥ã«æ§‹ç¯‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’é”æˆå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚
åŠ ãˆã¦ã€æ§‹ç¯‰ã—ãŸãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã«å¯¾ã—ã¦ SFT ã¨ DPO ã«ã‚ˆã‚‹äº‹å¾Œå­¦ç¿’ã‚’å®Ÿæ–½ã—ã€ææ¡ˆæ‰‹æ³•ã®æœ‰åŠ¹æ€§ãŒäº‹å¾Œå­¦ç¿’å¾Œã‚‚ç¶­æŒã•ã‚Œã‚‹ã“ã¨ã‚‚ç¤ºã™ã€‚
â€• 3120 â€•

2 æ–¹æ³•

æœ¬ç ”ç©¶ã§ã¯ã€è¤‡æ•°ã®ã‚µã‚¤ã‚ºã®è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ®µéšçš„ã«æ§‹ç¯‰ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚
æ—¢å­˜ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯ã€å„ã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ã—ã¦ã„ã‚‹ã€‚
ææ¡ˆæ‰‹æ³•ã§ã¯ã€å°ã•ãªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å­¦ç¿’ã‚’é–‹å§‹ã—ã€ãã®ãƒ¢ãƒ‡ãƒ«ã‚’æ‹¡å¼µã—ãªãŒã‚‰æ®µéšçš„ã«å¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
ã“ã®éç¨‹ã§å¾—ã‚‰ã‚Œã‚‹ä¸­é–“æ®µéšã®ãƒ¢ãƒ‡ãƒ«ã‚‚ã€ãã‚Œãã‚Œç‹¬ç«‹ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦åˆ©ç”¨å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚
ææ¡ˆæ‰‹æ³•ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã‚’å€‹åˆ¥ã«æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚ˆã‚Šã‚‚ä½ã„ã‚³ã‚¹ãƒˆã§ç³»åˆ—å…¨ä½“ã‚’æ§‹ç¯‰å¯èƒ½ã§ã‚ã‚‹ã€‚


2.1 ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®æ§‹ç¯‰

ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®å˜èª¿å¢—åŠ åˆ—[ğ‘‹1, ğ‘‹2, . . . , ğ‘‹ğ‘›]ã‚’ãªã™ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã«å¯¾ã—ã¦ã€å¯¾å¿œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç³»åˆ—[ğœƒ1, ğœƒ2, . . . , ğœƒğ‘›]ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
ã“ã“ã§ ğœƒğ‘–âˆˆ â„ğ‘‹ğ‘–ã¯ ğ‘– ç•ªç›®ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¯å˜èª¿å¢—åŠ ã™ã‚‹ï¼ˆğ‘‹ğ‘–+1> ğ‘‹ğ‘–)ã€‚
å„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º ğ‘‹ğ‘–ã«å¯¾ã—ã¦ã€ãã®å­¦ç¿’ã«ç”¨ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ ğ‘‡ğ‘–ã¨ã™ã‚‹ã€‚
ã“ã®ã¨ãã€ã‚µã‚¤ã‚º ğ‘‹ğ‘–ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ã™ã‚‹éš›ã«å¿…è¦ãªè¨ˆç®—é‡ã‚’FLOPs( ğ‘‹ğ‘–, ğ‘‡ğ‘–) = 6ğ‘‹ğ‘–ğ‘‡ğ‘–ã¨å®šç¾©ã™ã‚‹[7]ã€‚



2.2 æ®µéšçš„ãªå­¦ç¿’

ã¾ãšåˆæœŸãƒ¢ãƒ‡ãƒ« ğœƒ1ã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ã™ã‚‹ã€‚
ã“ã‚Œã«ã¯ FLOPs( ğ‘‹1, ğ‘‡1)ã®è¨ˆç®—é‡ã‚’è¦ã™ã‚‹ã€‚
ç¶šãå„æ®µéšã§ã¯ã€ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µæ³• ğ‘“ ã‚’ç”¨ã„ã¦æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹ï¼šğœƒğ‘–+1= ğ‘“ (ğœƒğ‘–; ğ‘‹ğ‘–+1)(ğ‘– â‰¥ 1)(1)ã“ã“ã§ ğ‘“ (Â·; ğ‘‹ğ‘–+1) : â„ğ‘‹ğ‘–â†’ â„ğ‘‹ğ‘–+1ã¯ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µæ³•ã§ã‚ã‚Šã€æ‹¡å¼µå¾Œã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ğœƒğ‘–+1ãŒåŠ¹ç‡çš„ãªå­¦ç¿’ã®åˆæœŸå€¤ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã‚‹ã€‚
æœ¬ç ”ç©¶ã§ã¯ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µæ³•ã¨ã—ã¦ã€bert2BERT [ 5]ã‚’ç”¨ã„ãŸã€‚
1ï¼‰bert2BERT ã§ã¯ã€Transformer ãƒ¢ãƒ‡ãƒ«[8]ã®å¹…ã‚„æ·±ã•ã‚’æ‹¡å¼µã•ã›ã‚‹ã€‚
å…·ä½“çš„ã«ã¯ã€å¹…æ–¹å‘ã®æ‹¡å¼µã§ã¯ç·šå½¢å±¤ã®é‡ã¿ã‚’è¤‡è£½ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®éš ã‚Œå±¤ã®æ¬¡å…ƒã‚’æ‹¡å¤§ã™ã‚‹ã€‚
ã¾ãŸã€æ·±ã•æ–¹å‘ã®æ‹¡å¼µã§ã¯å­¦ç¿’æ¸ˆã¿ã®å±¤ã‚’ä¸Šå±¤ã«è¤‡è£½ã—ã¦ç©ã¿é‡ã­ã‚‹ã€‚
bert2BERT ã«ã‚ˆã‚‹åˆæœŸåŒ–å¾Œã€å„æ®µéš ğ‘– + 1 (ğ‘– â‰¥ 1)ã®ãƒ¢ãƒ‡ãƒ«ã‚’ FLOPs(ğ‘‹ğ‘–+1, ğ‘‡ğ‘–+1) âˆ’âˆ‘ğ‘–ğ‘—=1FLOPs( ğ‘‹ğ‘—, ğ‘‡ğ‘—)ã®è¨ˆç®—é‡ã§è¿½åŠ å­¦ç¿’ã™ã‚‹ã€‚
ã“ã‚Œã¯ã€ã‚µã‚¤ã‚º ğ‘‹ğ‘–+1ã®ãƒ¢ãƒ‡ãƒ«1ï¼‰ åŸè«–æ–‡[5]ã§ã¯ã€æ–°è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–æ–¹æ³•ã¨ã—ã¦ã€AKI ã¨ FPI ã¨ã„ã†äºŒã¤ã®æ‰‹æ³•ãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹ãŒã€æœ¬ç ”ç©¶ã§ã¯ AKI ã‚’ç”¨ã„ãŸã€‚
ã‚’
ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ã™ã‚‹éš›ã®è¨ˆç®—é‡ã‹ã‚‰ã€ãã‚Œã¾ã§ã®æ®µéšã§ä½¿ç”¨ã—ãŸè¨ˆç®—é‡ã‚’å·®ã—å¼•ã„ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚
ã“ã®æ‰‹ç¶šãã«ã‚ˆã‚Šã€ã‚µã‚¤ã‚º ğ‘‹ğ‘–+1ã®ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«å¿…è¦ãªè¨ˆç®—é‡ FLOPs( ğ‘‹ğ‘–+1, ğ‘‡ğ‘–+1)ã§ã€ã‚µã‚¤ã‚º ğ‘‹1ã‹ã‚‰ ğ‘‹ğ‘–+1ã¾ã§ã®ãƒ¢ãƒ‡ãƒ«ç³»åˆ—å…¨ä½“ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚


3 å®Ÿé¨“ 1: ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®äº‹å‰å­¦ç¿’

ææ¡ˆæ‰‹æ³•ã§ã‚ã‚‹ã€æ®µéšçš„ãªå­¦ç¿’ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®æ§‹ç¯‰ã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã€äº‹å‰å­¦ç¿’ã‚’é€šã˜ãŸå®Ÿé¨“ã‚’è¡Œãªã£ãŸã€‚
å…·ä½“çš„ã«ã¯ã€8B ã‚’æœ€å¤§ã‚µã‚¤ã‚ºã¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã«å¯¾ã—ã¦ã€è¨ˆç®—åŠ¹ç‡ã¨æœ€çµ‚çš„ãªæ€§èƒ½ã®ä¸¡é¢ã‹ã‚‰ææ¡ˆæ‰‹æ³•ã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã™ã‚‹ã€‚

3.1 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

äº‹å‰å­¦ç¿’ã®è¨“ç·´ãƒ»é–‹ç™ºãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦FineWeb-Edu [9]ã‚’ä½¿ç”¨ã—ãŸã€‚
FineWeb-Edu ã¯æ•™è‚²çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä¸­å¿ƒã¨ã—ãŸã‚¦ã‚§ãƒ–ã‚³ãƒ¼ãƒ‘ã‚¹ã§ã‚ã‚‹ã€‚
å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«ã¯ GPT-2 [10]ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æ¡ç”¨ã—ãŸã€‚
ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã«ã¯ FineWeb-Edu ã®æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(Valid)ã¨è¤‡æ•°ã®æ¨™æº–çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ãŸã€‚
2ï¼‰

3.2 ãƒ¢ãƒ‡ãƒ«è¨­å®š

æ®µéšçš„å­¦ç¿’æ³•ã¯ã€ä»»æ„ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®å¢—åŠ ã«å¯¾å¿œå¯èƒ½ã§ã‚ã‚‹ãŒã€æœ¬ç ”ç©¶ã§ã¯å„æ®µéšã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ 2 å€ã«ã™ã‚‹è¨­å®šã‚’æ¡ç”¨ã—ãŸã€‚
å…·ä½“çš„ã«ã¯ã€[ğ‘‹1= 1B, ğ‘‹2= 2B, ğ‘‹3= 4B, ğ‘‹4= 8B]ã‹ã‚‰ãªã‚‹ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã‚’ç”¨æ„ã™ã‚‹ã€‚
3ï¼‰ãƒ¢ãƒ‡ãƒ«ã«ã¯ Llama [4]ã¨åŒæ§˜ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ãŸã€‚
ã¾ãŸã€å…¥åŠ›ã®æœ€å¤§ç³»åˆ—é•·ã¯ 1024 ã¨ã™ã‚‹ã€‚
ã•ã‚‰ã«ã€ãƒ¢ãƒ‡ãƒ«ç³»åˆ—å†…ã®å„ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ã•ã›ã‚‹éš›ã®å­¦ç¿’ç‡ã¯ 3.0 Ã— 10âˆ’4ã¨ã™ã‚‹ã€‚


3.3 è¨“ç·´ãƒ‡ãƒ¼ã‚¿é‡

æ®µéšçš„è¨“ç·´æ³•ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º ğ‘‹ğ‘–ã‹ã‚‰ã‚µã‚¤ã‚ºğ‘‹ğ‘–+1ã¸ã®æ‹¡å¤§æ™‚ã«ã€ğ‘‹ğ‘–+1ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰è¨“ç·´ã™ã‚‹å ´åˆã®è¨ˆç®—é‡ã‹ã‚‰ä»Šã¾ã§ ğ‘‹ğ‘–ã«ã‹ã‘ãŸè¨ˆç®—é‡ã®å·®åˆ†ã‚’è¨ˆç®—ã—ã€ãã®åˆ†ã®ãƒ‡ãƒ¼ã‚¿é‡ã‚’ ğ‘‹ğ‘–+1ãƒ¢ãƒ‡ãƒ«ã«è¨“ç·´ã•ã›ã‚‹ã€‚
è¿‘å¹´ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã«ãŠã„ã¦ã¯ã€Chinchilla å‰‡[11]ãŒç¤ºã™æœ€é©å€¤ã‚’å¤§ããè¶…ãˆã‚‹ãƒ‡ãƒ¼ã‚¿é‡ã§ã®è¨“ç·´ãŒä¸€èˆ¬çš„ã¨ãªã£ã¦ã„ã‚‹ã€‚
ã“ã‚Œã¯ã€è¨ˆç®—åŠ¹ç‡ã¯ä½ä¸‹ã™ã‚‹ã‚‚ã®ã®ã€æœ€çµ‚çš„ãªãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å‘ä¸Š2ï¼‰ å…·ä½“çš„ãªã‚¿ã‚¹ã‚¯ã®è©³ç´°ã¯ä»˜éŒ² B ã«è¨˜ã™ã€‚
3ï¼‰ å„ãƒ¢ãƒ‡ãƒ«ã®å…·ä½“çš„ãªå¹…ã‚„æ·±ã•ã«ã¤ã„ã¦ã¯ä»˜éŒ² A ã«è¨˜ã™ã€‚
â€• 3121 â€•ãŒæœŸå¾…ã•ã‚Œã‚‹ãŸã‚ã§ã‚ã‚‹ã€‚
ä¾‹ãˆã°ã€Llama 3-8B [12]ã®äº‹ä¾‹ã§ã¯ã€Chinchilla å‰‡ã«ãŠã‘ã‚‹æœ€é©å€¤ã®ç´„ 100 å€ã¨ãªã‚‹ 15 å…†ãƒˆãƒ¼ã‚¯ãƒ³ã§ã®è¨“ç·´ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ã€‚
ãã“ã§ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰è¨“ç·´ã•ã›ã‚‹ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿é‡ã‚’ Chinchilla å‰‡ã§æ±ºå®šã•ã‚Œã‚‹é‡ã® 2 å€ã«ã™ã‚‹è¨­å®š(2x Chinchilla rule ã¨è¡¨ç¾ã™ã‚‹)ã‚’ç”¨æ„ã—ãŸã€‚
ææ¡ˆã™ã‚‹æ®µéšçš„å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯æœ¬è³ªçš„ã«ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¶ˆè²»ã™ã‚‹æ€§è³ªã‚’æŒã¤ã€‚
ä¾‹ãˆã° 8B ãƒ¢ãƒ‡ãƒ«ã¾ã§ã®æ®µéšçš„å­¦ç¿’ã§ã¯ã€1Bï¼Œ2B,4B ãƒ¢ãƒ‡ãƒ«ã®æ–¹ãŒåŒä¸€ã®è¨ˆç®—é‡ã«ãŠã„ã¦ 8B ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹ç‡ã«å„ªã‚Œã‚‹ã€‚
ãã®ãŸã‚ã€8B ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ã™ã‚‹å ´åˆã¨æ¯”è¼ƒã—ã¦ã€æ®µéšçš„å­¦ç¿’ã§ã¯ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿é‡ãŒå¿…è¦ã¨ãªã‚‹ã€‚
ã“ã®ã‚ˆã†ã«æ®µéšçš„å­¦ç¿’æ³•ã¯å¿…ç„¶çš„ã«ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¶ˆè²»ã™ã‚‹ãŸã‚ã€æ®µéšçš„å­¦ç¿’ã®æ€§èƒ½å‘ä¸ŠãŒå˜ã«æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆã‚Šå¤šãå­¦ç¿’ã—ãŸã“ã¨ã«èµ·å› ã™ã‚‹å¯èƒ½æ€§ãŒè€ƒãˆã‚‰ã‚Œã‚‹ã€‚
ãã“ã§ã€ã“ã®åŠ¹æœã‚’åˆ†é›¢ã—ã¦è©•ä¾¡ã™ã‚‹ãŸã‚ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰ã®å­¦ç¿’ã¨åŒä¸€ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸå®Ÿé¨“è¨­å®šã‚’å°å…¥ã—ãŸã€‚
å…·ä½“çš„ã«ã¯ã€2x Chinchilla rule è¨­å®šã«ãŠã„ã¦ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰ 8B ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹éš›ã«ä½¿ç”¨ã™ã‚‹ 320B ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ¶é™ã—ã€å¿…è¦ã«å¿œã˜ã¦åŒä¸€ãƒ‡ãƒ¼ã‚¿ã®å†å­¦ç¿’ã‚’è¨±å¯ã™ã‚‹è¨­å®šã§ã‚ã‚‹ã€‚
ã“ã®è¨­å®šã§ã¯ã€1B â†’ 2B â†’ 4B ã¾ã§ã®æ®µéšçš„å­¦ç¿’ã¯ 2x Chinchilla rule è¨­å®šã¨åŒæ§˜ã«é€²ã¿ã€8B ã®å­¦ç¿’æ®µéšã§ã®ã¿ãƒ‡ãƒ¼ã‚¿ã®å†å­¦ç¿’ãŒç™ºç”Ÿã™ã‚‹ã€‚
ã“ã®å›ºå®šã®ãƒ‡ãƒ¼ã‚¿é‡ã§å†å­¦ç¿’ã‚’è¨±å¯ã™ã‚‹è¨­å®šã‚’ 8B ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µw/ Fixed Data ã¨è¡¨ç¾ã™ã‚‹ã€‚
8B ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µã¨ã€8B ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ w/ Fixed Data ã¨ã®æ¯”è¼ƒã«ã‚ˆã‚Šã€æ®µéšçš„å­¦ç¿’æ³•ã«ã‚ˆã‚‹æ€§èƒ½å‘ä¸ŠãŒæ–°è¦ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ã«ä¾å­˜ã—ãªã„ã“ã¨ã‚’æ¤œè¨¼ã™ã‚‹ã€‚

3.4 è¨ˆç®—é‡

æ®µéšçš„å­¦ç¿’æ³•ã®ä¸»è¦ãªåˆ©ç‚¹ã®ä¸€ã¤ã¯ã€è¤‡æ•°ã®ã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«å¾—ã‚‰ã‚Œã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
2.1 ç¯€ã§å®šç¾©ã—ãŸã‚ˆã†ã«ã€è¨ˆç®—é‡ã¯å„ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° ğ‘‹ ã¨å­¦ç¿’ãƒˆãƒ¼ã‚¯ãƒ³æ•° ğ‘‡ ã‹ã‚‰FLOPs = 6ğ‘‹ğ‘‡ ã§è¨ˆç®—ã•ã‚Œã‚‹ã€‚
ä¾‹ã¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ç³»åˆ—[1B, 2B, 4B, 8B]ã‚’ 2x Chinchilla rule ã®å…ƒã§å„ãƒ¢ãƒ‡ãƒ«ã‚’ç‹¬ç«‹ã«å­¦ç¿’ã•ã›ã‚‹ã¨ã€å¿…è¦ãª FLOPs ã¯ãã‚Œãã‚Œ[0.24Z, 0.96Z, 3.84Z, 15.4Z]ã¨ãªã‚‹ã€‚
ã¤ã¾ã‚Šç³»åˆ—å…¨ä½“ã®æ§‹ç¯‰ã«ã¯ 20.4ZFLOPs å¿…è¦ã¨ãªã‚‹ã€‚
ä¸€æ–¹ã€ææ¡ˆæ‰‹æ³•ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®æ§‹ç¯‰ã«å¿…è¦ãªè¨ˆç®—é‡ã¯æœ€å¤§ã‚µã‚¤ã‚ºã§ã‚ã‚‹ 8B ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã«å¿…è¦ãªè¨ˆç®—é‡ã¨åŒç­‰ã§ã‚ã‚‹ã€‚
ã—ãŸãŒã£ã¦ã€ææ¡ˆæ‰‹æ³•ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®æ§‹ç¯‰ã§ã¯ 15.4ZFLOPs ã®è¨ˆç®—é‡ã§æ¸ˆã‚€ã€‚
ã¤ã¾ã‚Šã€å€‹å›³ 2 å®Ÿé¨“ 1(3 ç¯€)ã«ãŠã‘ã‚‹å„ãƒ¢ãƒ‡ãƒ«ã®è¨ˆç®—é‡ã«å¯¾ã™ã‚‹æå¤±é–¢æ•°ã®å€¤ã€‚
åˆ¥ã«ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã‚’æ§‹ç¯‰ã™ã‚‹å ´åˆã¨æ¯”ã¹ã€æ®µéšçš„å­¦ç¿’æ³•ã¯è¨ˆç®—é‡ã‚’ç´„ 25%å‰Šæ¸›å¯èƒ½ã§ã‚ã‚‹ã€‚
ã“ã‚Œã¯ã€æ®µéšçš„å­¦ç¿’æ³•ãŒå¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®æ§‹ç¯‰ã«ãŠã„ã¦ã€è¨ˆç®—åŠ¹ç‡ã®é¢ã§å„ªä½æ€§ã‚’æŒã¤ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

3.5 å­¦ç¿’ç‡èª¿æ•´

å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ãŠã„ã¦ã€é«˜ã„å­¦ç¿’ç‡ã®è¨­å®šãŒæ€§èƒ½å‘ä¸Šã«å¯„ä¸ã—ã†ã‚‹ãŒ[13]ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã»ã©å­¦ç¿’ã®ä¸å®‰å®šæ€§ãŒå¢—ã™ãŸã‚ã€ã‚ˆã‚Šå°ã•ãªå­¦ç¿’ç‡ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚‹[4]ã€‚
ä¾‹ãˆã°ã€äºˆå‚™å®Ÿé¨“ã«ãŠã„ã¦ã€1.5 Ã— 10âˆ’3ã®å­¦ç¿’ç‡ã§ã¯ 1B ãƒ¢ãƒ‡ãƒ«ã§ã¯å®‰å®šã—ãŸå­¦ç¿’ãŒå¯èƒ½ã§ã‚ã£ãŸãŒã€8B ãƒ¢ãƒ‡ãƒ«ã§ã¯æå¤±å€¤ã®ã‚¹ãƒ‘ã‚¤ã‚¯ãŒç™ºç”Ÿã—ã€å­¦ç¿’ãŒç ´ç¶»ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚
æ®µéšçš„å­¦ç¿’æ³•ã§ã¯å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å­¦ç¿’ã‚’é–‹å§‹ã™ã‚‹ã¨ã„ã†ç‰¹å¾´ãŒã‚ã‚‹ã€‚
ãã®ãŸã‚ã€å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æ™‚ã«ã¯é«˜ã„å­¦ç¿’ç‡ã§æ€§èƒ½ã‚’é«˜ã‚ã¤ã¤ã€å¤§ãã„ãƒ¢ãƒ‡ãƒ«ã§ã¯å­¦ç¿’ç‡ã‚’ä¸‹ã’ã¦å­¦ç¿’ã‚’å®‰å®šã•ã›ã‚‹ã¨ã„ã£ãŸæˆ¦ç•¥ã‚’æ¡ç”¨å¯èƒ½ã§ã‚ã‚‹ã€‚
ãã“ã§æœ¬å®Ÿé¨“ã§ã¯ã€1B ãƒ¢ãƒ‡ãƒ«ã§ã® 1.5 Ã— 10âˆ’3ã‹ã‚‰8B ãƒ¢ãƒ‡ãƒ«ã§ã® 3.0 Ã— 10âˆ’4ã¾ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®å¢—åŠ ã«å¿œã˜ã¦å­¦ç¿’ç‡ã‚’æ®µéšçš„ã«ä¸‹ã’ã¦ã„ãï¼šâ€¢ 1B (From Scratch): 1.5 Ã— 10âˆ’3â€¢ 2B (ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ): 1 .1 Ã— 10âˆ’3â€¢ 4B (ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ): 7 .0 Ã— 10âˆ’4â€¢ 8B (ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ): 3 .0 Ã— 10âˆ’4

3.6 çµæœ

è¡¨ 1 ã«ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰ã®å­¦ç¿’ã¨ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µã«ã‚ˆã‚‹æ®µéšçš„å­¦ç¿’ã®å„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ãŠã‘ã‚‹äº‹å‰å­¦ç¿’è©•ä¾¡çµæœã‚’ç¤ºã™ã€‚
ã¾ãŸå›³ 2 ã«å„ãƒ¢ãƒ‡ãƒ«ã®è¨ˆç®—é‡ã«å¯¾ã™ã‚‹æå¤±é–¢æ•°ã®å€¤ã‚’ç¤ºã™ã€‚
å®Ÿé¨“çµæœã‹ã‚‰ã¯ã€ææ¡ˆæ‰‹æ³•â€• 3122 â€•è¡¨ 1 äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡çµæœã€‚
1B ã‹ã‚‰ 8B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ãŠã„ã¦ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰ã®å­¦ç¿’ã¨ææ¡ˆæ‰‹æ³•ã®æ€§èƒ½ã‚’ã€Perplexity ã¨ 7 ã¤ã®ä¸‹æµã‚¿ã‚¹ã‚¯ã® Accuracy ã§æ¯”è¼ƒã—ãŸã€‚
æ§‹ç¯‰æ–¹æ³•Perplexity â†“ Accuracy â†‘Valid Wikitext LAMBADA ARC-e ARC-c Winogrande PIQA OBQA HellaSwag1B From Scratch 11.6 19.7 42.2 65.9 37.5 56.9 73.0 39.2 55.02BFrom Scratch 10.4 17.1 48.5 66.9 38.4 56.6 75.3 41.6 58.0ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ 10.1 16.2 52.6 71.8 42.2 61.9 75.0 40.8 62.54BFrom Scratch 9.18 14.0 51.7 71.7 43.2 59.2 76.7 40.8 63.3ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ 8.72 13.1 55.7 74.4 47.8 65.2 77.4 45.8 68.18BFrom Scratch 7.85 10.2 55.2 74.5 47.4 62.4 77.0 46.4 67.9ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ 7.64 10.1 59.0 76.7 48.3 65.8 78.3 46.6 71.0ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ w/ Fixed Data 7.64 9.8 59.0 76.1 50.6 64.7 78.2 47.2 71.2ã®æ–¹ãŒã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ã—ãŸã‚‚ã®ã«æ¯”ã¹æ€§èƒ½ãŒå„ªä½ã§ã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚‹ã€‚
ã“ã‚Œã‚‰ã®çµæœã¯ã€æ®µéšçš„å­¦ç¿’ã®æœ‰åŠ¹æ€§ã‚’ç¤ºã™ã¨ã¨ã‚‚ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®å¢—åŠ ã«å¿œã˜ãŸå­¦ç¿’ç‡ã®æ®µéšçš„ãªèª¿æ•´ãŒã€ã•ã‚‰ãªã‚‹æ€§èƒ½å‘ä¸Šã«çµã³ã¤ãã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
4ï¼‰ã¾ãŸã€8B ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ w/ Fixed Data ã®è¨­å®šã§ã¯ã€320B ãƒˆãƒ¼ã‚¯ãƒ³ã®å›ºå®šã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ç”¨ã„ãŸå ´åˆã§ã‚‚ã€8B ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µã¨ã»ã¼åŒç­‰ã®æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚
ä»¥ä¸Šã®çµæœã‹ã‚‰ã€æ®µéšçš„å­¦ç¿’ã¨é©åˆ‡ãªå­¦ç¿’ç‡ã®èª¿æ•´ã«ã‚ˆã£ã¦ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰ã®å­¦ç¿’ã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’é”æˆã§ãã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚
ã¾ãŸã€å›ºå®šã®ãƒ‡ãƒ¼ã‚¿é‡ã‚’ç”¨ã„ãŸå®Ÿé¨“ã‹ã‚‰ã€ã“ã®æ‰‹æ³•ã®åŠ¹æœãŒæ–°è¦ãƒ‡ãƒ¼ã‚¿é‡ã®å¢—åŠ ã«ã‚ˆã‚‹ã‚‚ã®ã§ã¯ãªã„ã“ã¨ã‚‚ç¤ºã•ã‚ŒãŸã€‚



4 å®Ÿé¨“ 2: ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®äº‹å¾Œå­¦ç¿’

æœ¬å®Ÿé¨“ã§ã¯ã€æ®µéšçš„ã«æ§‹ç¯‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã« SFT ãŠã‚ˆã³ DPO [14]ã«ã‚ˆã‚‹äº‹å¾Œå­¦ç¿’ã‚’å®Ÿæ–½ã™ã‚‹ã€‚
æœ¬å®Ÿé¨“ã«ã‚ˆã‚Šã€ç¯€ 3 ã®çµæœã¨åŒæ§˜ã«ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ãŒå„ªä½ãªæ€§èƒ½ã‚’ç¤ºã™ã‹ã©ã†ã‹ã‚’æ¤œè¨¼ã™ã‚‹

4.1 å®Ÿé¨“è¨­å®š

å®Ÿé¨“ 1 ã§æ§‹ç¯‰ã—ãŸãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã«å¯¾ã—ã¦äº‹å¾Œå­¦ç¿’ã‚’å®Ÿæ–½ã™ã‚‹ã“ã¨ã§ã€æ®µéšçš„å­¦ç¿’ã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã™ã‚‹ã€‚
äº‹å¾Œå­¦ç¿’ã«ã¯ Meng ã‚‰[15]ã®è¨­å®šã‚’æ¡ç”¨ã—ãŸã€‚
å…·ä½“çš„ã«ã¯ã€æ•™å¸«ä»˜ããƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSFTï¼‰ã‚’UltraChat-200k [16]ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦è¡Œã„ã€ç¶šã„ã¦Ultrachat Feedback [17]ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸDPO [14]ã‚’è¡Œã£ãŸã€‚
SFT ã®å­¦ç¿’ç‡ã¯ 3.0 Ã— 10âˆ’5ï¼ŒDPOã®å­¦ç¿’ç‡ã¯ 5.0 Ã— 10âˆ’7ã¨ã—ã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§çµ±ä¸€ã—ãŸã€‚
å„ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”å“è³ªè©•ä¾¡ã«ã¯ MT-Bench [18]ã‚’æ¡ç”¨ã—ãŸã€‚
æ¯”è¼ƒå¯¾è±¡ã¨ã—ã¦ã€å€‹åˆ¥ã«ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹4ï¼‰ å­¦ç¿’ç‡ã‚’èª¿æ•´ã›ãšå›ºå®šã—ãŸå ´åˆã®çµæœã‚’ä»˜éŒ² C ã«ã¦ç¤ºã™ã€‚
è¡¨ 2 äº‹å¾Œå­¦ç¿’å¾Œã®ãƒ¢ãƒ‡ãƒ«è©•ä¾¡çµæœã€‚
SFT ã¨ DPO ã«ã‚ˆã‚‹äº‹å¾Œå­¦ç¿’ã‚’å®Ÿæ–½ã—ã€MT-Bench ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã—ãŸã€‚
æ§‹ç¯‰æ–¹æ³• MT-Bench â†‘2BFrom Scratch 2.04ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ 3.224BFrom Scratch 3.02ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ 3.638BFrom Scratch 3.45ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ 3.96ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µw/ Fixed Data 3.98ã‚‰å­¦ç¿’ã•ã›ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆ2B, 4B, 8Bï¼‰ã‚’ç”¨æ„ã—ã€åŒæ§˜ã®äº‹å¾Œå­¦ç¿’ã‚’å®Ÿæ–½ã—ãŸã€‚



4.2 çµæœ

è¡¨ 2 ã«å„ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®äº‹å¾Œå­¦ç¿’å¾Œã® MT-Bench ã‚¹ã‚³ã‚¢ã‚’ç¤ºã™ã€‚
æ®µéšçš„ã«æ§‹ç¯‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚³ã‚¢ã¯ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚’ä¸Šå›ã£ã¦ã„ãŸã€‚
ã“ã®çµæœã¯ã€äº‹å‰å­¦ç¿’æ™‚ã¨åŒæ§˜ã«ã€æ®µéšçš„å­¦ç¿’ã¨é©åˆ‡ãªå­¦ç¿’ç‡èª¿æ•´ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ãŒäº‹å¾Œå­¦ç¿’ã«ãŠã„ã¦ã‚‚æœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚



5 ãŠã‚ã‚Šã«

æœ¬ç ”ç©¶ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µã‚’ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã®åŠ¹ç‡çš„ãªæ§‹ç¯‰æ–¹æ³•ã‚’ææ¡ˆã—ãŸã€‚
ææ¡ˆæ‰‹æ³•ã«ã‚ˆã£ã¦ã€å°ã•ãªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ®µéšçš„ã«è¨“ç·´ã‚’è¡Œã†ã“ã¨ã«ã‚ˆã£ã¦å‰¯æ¬¡çš„ã«ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã‚’æ§‹ç¯‰å¯èƒ½ã§ã‚ã‚‹ã€‚
å®Ÿé¨“ã«ã‚ˆã‚Šã€8B ã‚µã‚¤ã‚ºã‚’æœ€å¤§ã‚µã‚¤ã‚ºã¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã«ãŠã„ã¦ã€ææ¡ˆæ‰‹æ³•ãŒãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã‚’å€‹åˆ¥ã«æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã¨æ¯”è¼ƒã—ã¦ã€ç´„ 25%ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚
ã•ã‚‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ãŸå­¦ç¿’ç‡ã®æ®µéšçš„ãªèª¿æ•´ã«ã‚ˆã‚Šã€äº‹å‰å­¦ç¿’ãŠã‚ˆã³äº‹å¾Œå­¦ç¿’ã®ä¸¡æ–¹ã«ãŠã„ã¦ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰è¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚
â€• 3123 â€•



è¬è¾

æœ¬ç ”ç©¶ã®ä¸€éƒ¨ã¯ã€JST ãƒ ãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆå‹ç ”ç©¶é–‹ç™ºäº‹æ¥­ JPMJMS2011-35 (fundamental research)ã®æ”¯æ´ã‚’å—ã‘ãŸã‚‚ã®ã§ã™ã€‚

å‚è€ƒæ–‡çŒ®


[1] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Alma-hairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhar-gava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer,Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal,Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kar-das, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric MichaelSmith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AurelienRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:Open foundation and ï¬ne-tuned chat models, 2023.
[2] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou,Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong,Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, JianhongTu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, JinzeBai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, KexinYang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, RuiMen, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, TianhangZhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou,Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu,Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu,Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technicalreport, 2024.
[3] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ah-mad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao,Harkirat Behl, et al. Phi-3 technical report: A highly capable language modellocally on your phone. arXiv preprint arXiv:2404.14219, 2024.
[4] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro,Faisal Azhar, et al. Llama: Open and eï¬ƒcient foundation language models.arXiv preprint arXiv:2302.13971, 2023.
[5] Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, FengyuWang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. bert2BERT:Towards reusable pretrained language models. In Smaranda Muresan, PreslavNakov, and Aline Villavicencio, editors,Proceedings of the 60th AnnualMeeting of the Association for Computational Linguistics (Volume1: Long Papers), pp. 2134â€“2148, Dublin, Ireland, May 2022. Associationfor Computational Linguistics.
[6] Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, ReynoldCheng, Yike Guo, and Jie Fu. Stacking your transformers: A closer look atmodel growth for eï¬ƒcient LLM pre-training. In The Thirty-eighth AnnualConference on Neural Information Processing Systems, 2024.
[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,Amanda Askell, et al. Language models are few-shot learners. Advances inneural information processing systems, Vol. 33, pp. 1877â€“1901, 2020.
[8] A Vaswani. Attention is all you need. Advances in Neural InformationProcessing Systems, 2017.
[9] Guilherme Penedo, Hynek KydlÃ­Äek, Loubna Ben allal, Anton Lozhkov,Margaret Mitchell, Colin Raï¬€el, Leandro Von Werra, and Thomas Wolf. Theï¬neweb datasets: Decanting the web for the ï¬nest text data at scale. In TheThirty-eight Conference on Neural Information Processing SystemsDatasets and Benchmarks Track, 2024.
[10] Alec Radford, Jeï¬€ Wu, Rewon Child, David Luan, Dar io Amodei, and IlyaSutskever. Language models are unsupervised multitask learners, 2019.
[11] Jordan Hoï¬€mann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Jo-hannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican,George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero,Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and LaurentSifre. An empirical analysis of compute-optimal large language model train-ing. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,editors, Advances in Neural Information Pro cessing Systems, 2022.
[12] AI@Meta. The llama 3 herd of models, 2024.
[13] Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. Spike nomore: Stabilizing the pre-training of large language models. arXiv preprintarXiv:2312.16903, 2023.
[14] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Ste-fano Ermon, and Chelsea Finn. Direct preference optimization: Your lan-guage model is secretly a reward model. In Thirty-seventh Conferenceon Neural Information Processing Systems, 2023.
[15] Yu Meng, Mengzhou Xia, and Danqi Chen. SimPO: Simple preferenceoptimization with a reference-free reward. In The Thirty-eighth AnnualConference on Neural Information Processing Systems, 2024.
[16] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu,Maosong Sun, and Bowen Zhou. Enhancing chat language models by scalinghigh-quality instructional conversations. In Houda Bouamor, Juan Pino, andKalika Bali, editors, Proceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing, pp. 3029â€“3051, Singapore,December 2023. Association for Computational Linguistics.
[17] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu,Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and MaosongSun. ULTRAFEEDBACK: Boosting language models with scaled AI feed-back. In Forty-ï¬rst International Conference on Machine Learning,2024.
[18] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, ZhanghaoWu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, HaoZhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge withMT-bench and chatbot arena. In Thirty-seventh Conference on NeuralInformation Processing Systems Datasets and Benchmarks Track,2023.
[19] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.Pointer sentinel mixture models. In International Conference on Learn-ing Representations, 2017.
[20] Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham,Raï¬€aella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, andRaquel FernÃ¡ndez. The LAMBADA dataset: Word prediction requiringa broad discourse context. In Katrin Erk and Noah A. Smith, editors, Pro-ceedings of the 54th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pp. 1525â€“1534, Berlin,Germany, August 2016. Association for Computational Linguistics.
[21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.Winogrande: An adversarial winograd schema challenge at scale. Commu-nications of the ACM, Vol. 64, No. 9, pp. 99â€“106, 2021.
[22] Yonatan Bisk, Rowan Zellers, Ronan bras, Jianfeng Gao, and Choi Yejin.Piqa: Reasoning about physical commonsense in natural language. Pro-ceedings of the AAAI Conference on Artiï¬cial Intelligence, Vol. 34,pp. 7432â€“7439, 04 2020.
[23] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.HellaSwag: Can a machine really ï¬nish your sentence? In Anna Korho-nen, David Traum, and LluÃ­s MÃ rquez, editors, Proceedings of the 57thAnnual Meeting of the Association for Computational Linguistics,pp. 4791â€“4800, Florence, Italy, July 2019. Association for ComputationalLinguistics.
[24] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,Carissa Schoenick, and Oyvind Tafjord. Think you have solved questionanswering? try arc, the ai2 reasoning challenge, 2018.
[25] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suitof armor conduct electricity? a new dataset for open book question answer-ing. In Ellen Riloï¬€, David Chiang, Julia Hockenmaier, and Junâ€™ichi Tsujii,editors, Proceedings of the 2018 Conference on Empirical Methodsin Natural Language Processing, pp. 2381â€“2391, Brussels, Belgium,October-November 2018. Association for Computational Linguistics.â€• 3124 â€•

è¡¨ 3 å„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ãŠã‘ã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­å®šéš ã‚Œå±¤ã®æ¬¡å…ƒæ•° FFN ã®æ¬¡å…ƒæ•°å±¤æ•°ãƒ˜ãƒƒãƒ‰æ•°1B 2048 7168 18 162B 2560 8960 22 204B 3200 11200 27 258B 4096 14336 33 32è¡¨ 4 äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å›ºå®šå­¦ç¿’ç‡ã‚’ç”¨ã„ãŸè©•ä¾¡çµæœã€‚
å„ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ç‡ã¯ 3.0 Ã— 10âˆ’4ã§ã‚ã‚‹ã€‚
æ§‹ç¯‰æ–¹æ³•Perplexity â†“ Accuracy â†‘Valid Wikitext LAMBADA ARC-e ARC-c Winogrande PIQA OBQA HellaSwag1B From Scratch 12.0 20.6 42.0 62.3 34.6 55.5 70.5 35.6 51.62B ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ 10.5 17.2 47.9 67.7 38.4 58.1 72.1 40.0 58.14B ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ 9.17 14.0 50.8 72.6 42.1 59.6 76.2 44.0 63.88Bãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ 8.04 10.6 54.1 75.7 46.7 61.3 76.8 46.2 68.2ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ w/ Fixed Data 8.04 10.4 54.1 75.2 46.8 63.5 77.2 45.8 68.1

A ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º

å®Ÿé¨“ã§ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«ç³»åˆ—ã§ã¯ã€Llama ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åŸºã«ã€1Bï¼Œ2Bï¼Œ4Bï¼Œ8B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ãŸã€‚
è¡¨ 3 ã«ã€å„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ãŠã‘ã‚‹å…·ä½“çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­å®šã‚’ç¤ºã™ã€‚
ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µã«ä¼´ã„ã€éš ã‚Œå±¤ã®æ¬¡å…ƒæ•°ã€FFN ã®æ¬¡å…ƒæ•°ã€å±¤æ•°ï¼Œãƒ˜ãƒƒãƒ‰æ•°ã‚’æ®µéšçš„ã«å¤§ããã—ã¦ã„ã‚‹ï¼

B è©•ä¾¡ã‚¿ã‚¹ã‚¯

è©•ä¾¡ã«ã¯ã€FineWeb-Edu ã®æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆValidï¼‰ã¨ WikiText [19]ã«ãŠã‘ã‚‹ Perplexity ã‚’ç”¨ã„ãŸã€‚
ã¾ãŸã€äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ç·åˆçš„ã«è©•ä¾¡ã™ã‚‹ãŸã‚ã€è¤‡æ•°ã®ä¸‹æµã‚¿ã‚¹ã‚¯ã§ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ€§èƒ½ã‚’æ¸¬å®šã—ãŸã€‚
å…·ä½“çš„ã«ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ï¼ˆLAMBADA [20]ï¼‰ã€å¸¸è­˜æ¨è«–ï¼ˆWinoGrande [21]ï¼ŒPIQA [22]ï¼ŒHellaSwag [23]ï¼‰ã€è³ªå•å¿œç­”ï¼ˆARC-eï¼ŒARC-c [24]ï¼ŒOBQA [25]ï¼‰ã®å„ã‚¿ã‚¹ã‚¯ã‚’æ¡ç”¨ã—ãŸã€‚


C å›ºå®šå­¦ç¿’ç‡ã«ãŠã‘ã‚‹å®Ÿé¨“

å®Ÿé¨“ 1( 3)ã«ãŠã„ã¦ã¯ã€æ®µéšçš„å­¦ç¿’ã«ãŠã„ã¦ã€å„ãƒ¢ãƒ‡ãƒ«ã§èª¿æ•´ã•ã‚ŒãŸå­¦ç¿’ç‡ã‚’ç”¨ã„ãŸã€‚
æœ¬ç¯€ã§ã¯ã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’ç‡ã‚’ 3.0 Ã— 10âˆ’4ã«å›ºå®šã—ãŸå®Ÿé¨“ã‚’è¡Œãªã£ãŸã€‚
è¡¨ 4 ã«ã€1B ã‹ã‚‰ 8B ã¾ã§ã®å„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ãŠã‘ã‚‹è©•ä¾¡çµæœã‚’ç¤ºã™ã€‚
â€• 3125 â€•