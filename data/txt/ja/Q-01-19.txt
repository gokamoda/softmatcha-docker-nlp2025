å¤§è¦æ¨¡ç”»åƒè¨€èªãƒ¢ãƒ‡ãƒ«ã¯ç‰©ä½“ã®è£å´ã‚’èªè­˜ã§ãã‚‹ã‹ï¼Ÿç‰©ä½“ã®è¦‹ãˆãªã„éƒ¨åˆ†ã®èªè­˜ã‚’å•ã†ã‚¿ã‚¹ã‚¯ã®ææ¡ˆ

ç«¹ä¸­ èª 

1

â€ƒè°·ä¸­ ç³

21

ä¸‰è±é›»æ©Ÿæ ªå¼ä¼šç¤¾â€ƒ

2

æ±äº¬å¤§å­¦



Takenaka.Makoto@bc.MitsubishiElectric.co.jp hyanaka@is.s.u-tokyo.ac.jp



æ¦‚è¦

äººé–“ã¯è¦–è¦šçš„ãªæƒ…å ±ã¨è¨€èªçš„ãªæƒ…å ±ã‚’å—ã‘å–ã‚‹ã¨ã€ãã‚Œã«é–¢é€£ã™ã‚‹äº‹å‰çŸ¥è­˜ã‚’ç”¨ã„ã¦ç‰©äº‹ã® â€œè¦‹ãˆãªã„éƒ¨åˆ†â€ ã‚’æƒ³åƒã™ã‚‹ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
ç‰©ä½“ã®è¦‹ãˆãªã„éƒ¨åˆ†ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã¯ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®åˆ†é‡ã§ã¯ç²¾åŠ›çš„ã«ç ”ç©¶ã•ã‚Œã¦ã„ã‚‹ãŒã€å¤šãã¯å¯¾è±¡ç‰©ã®ä¸€éƒ¨ãŒéš ã‚ŒãŸç‰©ä½“ã‚’å¯¾è±¡ã¨ã—ãŸç‰©ä½“èªè­˜ã§ã‚ã‚Šã€è£å´ã‚„å´é¢ãªã©è¦‹ãˆãªã„éƒ¨åˆ†ã«ã¯ç„¦ç‚¹ã¯å½“ã¦ã‚‰ã‚Œã¦ã„ãªã„ã€‚
ãã“ã§æœ¬ç ”ç©¶ã§ã¯ã€ç‰©ä½“ã®è£å´ã‚„å´é¢ã®èªè­˜èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®ã‚¿ã‚¹ã‚¯ã‚’ææ¡ˆã—ã€ç·šç”»ã¨å†™å®Ÿç”»åƒã‚’ç”¨ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ãŸã€‚
å®Ÿé¨“ã§ã¯ã€ææ¡ˆã‚¿ã‚¹ã‚¯ã‚’ç”¨ã„ã¦äººé–“ã¨ä»£è¡¨çš„ãªå¤§è¦æ¨¡ç”»åƒè¨€èªãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ãŸã€‚
ãã®çµæœã€äººé–“ã«ã¨ã£ã¦ç°¡å˜ãªã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€æœ€å…ˆç«¯ã®å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã§ã‚‚äººé–“ã®æ€§èƒ½ã«ã¯åŠã°ãªã„ã“ã¨ã‚’ç¢ºèªã—ãŸã€‚
1 ã¯ã˜ã‚ã«æœ€è¿‘ã®å¤§è¦æ¨¡ç”»åƒè¨€èªãƒ¢ãƒ‡ãƒ«ã®è‘—ã—ã„é€²åŒ–[1, 2, 3]ã«ä¼´ã„ã€‚
ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ã®é‡è¦æ€§ã‚‚å¢—ã—ã¦ã„ã‚‹ã€‚
ã“ã‚Œã¾ã§ã«ã€ã•ã¾ã–ã¾ãªè¦³ç‚¹ã‹ã‚‰VLM ã®èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹ã€‚
[4, 5, 6, 7, 8, 9]. ã“ã‚Œã‚‰ã®ã‚¿ã‚¹ã‚¯ã¯ã„ãšã‚Œã‚‚ã€ç‰©ã®è¦‹ãˆã‚‹éƒ¨åˆ†ã‚’èªè­˜ã™ã‚‹èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«è¨­è¨ˆã•ã‚Œã¦ã„ã‚‹ã€‚
ä¸€æ–¹ã€æœ¬ç ”ç©¶ã®èˆˆå‘³ã¯ã€ç‰©ã®è¦‹ãˆãªã„å´é¢ã‚’èªè­˜ã™ã‚‹èƒ½åŠ›ã§ã‚ã‚‹ã€‚
æœ€æ–°ã® VLM ãŒã‚¦ã‚§ãƒ–ã‚¹ã‚±ãƒ¼ãƒ«ã®å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ã¨ã€ç‰©ã®è¦‹ãˆãªã„éƒ¨åˆ†ã‚’æƒ³åƒã™ã‚‹èƒ½åŠ›ã‚’ VLM ã«æœŸå¾…ã™ã‚‹ã“ã¨ã¯ä¸æ€è­°ã§ã¯ãªã„ã€‚
è¦‹ãˆãªã„éƒ¨åˆ†ã‚’åˆ†é¡ã™ã‚‹ã¨ã€( ğ‘– )ä¸€éƒ¨ãŒä»–ã®ç‰©ä½“ã«éš ã•ã‚Œã¦ã„ã‚‹å ´åˆã€( ğ‘–ğ‘– )ç‰©ä½“ã®è£å´ã‚„å´é¢ã®ã‚ˆã†ã«è¦–è¦šçš„ã«ç„¡æƒ…å ±ãªå ´åˆã€ã«å¤§åˆ¥ã§ãã‚‹ã€‚
å‰è€…ã¯ã‚¢ãƒ¢ãƒ¼ãƒ€ãƒ«è£œå®Œ(Amodal completion)ã«é–¢é€£ã—ã¦ãŠã‚Šã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®åˆ†é‡ã§ã¯å‹¢åŠ›çš„ã«ç ”ç©¶ã•ã‚Œã¦ã„ã‚‹ã€‚
ä»£è¡¨çš„ãªã‚¿ã‚¹ã‚¯ã¨ã—ã¦ã¯ã€ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä¼´ã†ç‰©ä½“èªè­˜ã‚„ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯[10, 11, 12]ãªã©ãŒã‚ã‚‹ã€‚
æœ¬ç ”ç©¶ãŒå¯¾è±¡ã¨ã™ã‚‹ã®ã¯å¾Œè€…ã§ã‚ã‚‹ã€‚
ææ¡ˆã‚¿ã‚¹ã‚¯ã«é–¢é€£ã™ã‚‹å…ˆè¡Œç ”ç©¶ã«ã¯ã€PerspectiveTaking [13]ã‚„CLEVR-MRT [14]ãŒã‚ã‚‹ã€‚
ã“ã‚Œã‚‰ã®ã‚¿ã‚¹ã‚¯ã¯ç‰©ã®è¦‹ãˆãªã„éƒ¨åˆ†ã«ç„¦ç‚¹ã¯å½“ã¦ã¦ãŠã‚‰ãšã€è¦–ç‚¹ç²å¾—[15, 16]ã‚„å¿ƒçš„å›è»¢[17, 18]ã‚’é‡è¦–ã—ã¦ã„ã‚‹ã€‚
ãã“ã§æˆ‘ã€…ã¯ã€è£å´ã‚„å´é¢ãªã©ã®è¦‹ãˆãªã„éƒ¨åˆ†ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸäºŒã¤ã®ã‚¿ã‚¹ã‚¯ã‚’ææ¡ˆã™ã‚‹ã€‚
ææ¡ˆã‚¿ã‚¹ã‚¯ã®å…¥å‡ºåŠ›ã®ä¾‹ã‚’å›³ 1 ã«ç¤ºã™ã€‚
å›³ 1a ã®åå¯¾å´äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸç”»åƒã‚’åå¯¾å´ã‹ã‚‰è¦‹ãŸã¨ãã®è¦‹ãˆæ–¹ã«å¯¾å¿œã™ã‚‹ç”»åƒã‚’é¸æŠã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ã€‚
å›³ 1b ã®è¦–ç‚¹æ¨è«–ã‚¿ã‚¹ã‚¯ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸç”»åƒã¨ã€ãã®ç”»åƒã«é–¢ã™ã‚‹è¦–ç‚¹ã®èª¬æ˜æ–‡ã®æ•´åˆæ€§ã‚’å•ã†äºŒå€¤åˆ†é¡ã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ã€‚
æœ¬è«–æ–‡ã®ä¸»ãªè²¢çŒ®ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹ã€‚
âœ“ ç‰©ä½“ã®è¦‹ãˆãªã„éƒ¨åˆ†ã‚’èªè­˜ã™ã‚‹èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹2 ã¤ã®ã‚¿ã‚¹ã‚¯ã‚’ææ¡ˆã™ã‚‹ã€‚
âœ“ ç·šç”»ã¨å†™å®Ÿç”»åƒã‚’ä½¿ç”¨ã—ã¦ã€ææ¡ˆã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯å…¬é–‹äºˆå®šã§ã‚ã‚‹ã€‚
âœ“ ææ¡ˆã‚¿ã‚¹ã‚¯ã‚’ç”¨ã„ã¦ãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªãƒ¢ãƒ‡ãƒ«ãŠã‚ˆã³ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚‚äººé–“ã«åŠã°ãªã„ã“ã¨ã‚’ç¤ºã™ã€‚
2 ææ¡ˆã‚¿ã‚¹ã‚¯æœ¬ç« ã§ã¯ã€ææ¡ˆã‚¿ã‚¹ã‚¯ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ã®æµã‚Œã‚’è¿°ã¹ã‚‹ã€‚

åå¯¾å´äºˆæ¸¬ã‚¿ã‚¹ã‚¯

å…¥å‡ºåŠ›ã®ä¾‹ã‚’å›³ 1a ã«ç¤ºã™ã€‚
ã“ã®ã‚¿ã‚¹ã‚¯ã¯ã€ã‚¯ã‚¨ãƒªç”»åƒã®åå¯¾å´ã‹ã‚‰è¦‹ãŸè¦‹ãˆæ–¹ã«å¯¾å¿œã™ã‚‹ç”»åƒã‚’é¸æŠè‚¢ã‹ã‚‰é¸ã¶å•é¡Œã§ã‚ã‚‹ã€‚
å…¥åŠ›ã™ã‚‹ç”»åƒã¯ã€å¯¾è±¡ç‰©ã®æ­£é¢ç”»åƒ(å›³ 1a Fig.1)ã€ã‚¯ã‚¨ãƒªç”»åƒæ­£é¢ç”»åƒã‚¯ã‚¨ãƒªç”»åƒé¸æŠè‚¢ç”»åƒFig.1 Fig.5 Fig.6 Fig.7ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆThese images are diï¬€erent views of the same object.The Fig numbers are located near each ï¬gure.The descriptions of Fig.1 and Fig.5 are as follows.Fig.1 is a front isometric view of a table accessoryshowing our new design.Fig.5 is a left view of Fig.1.Question:Which ï¬gure is the opposite view of Fig.5?
å‡ºåŠ›Fig.6 3 Fig.7 7(a)åå¯¾å´äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã®å…¥å‡ºåŠ›æ­£é¢ç”»åƒè¦–ç‚¹åŸºæº–ç”»åƒã‚¯ã‚¨ãƒªç”»åƒFig.1 Fig.2 Fig.3ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆThe three images are diï¬€erent views of the same ob-ject. The Fig numbers arelocated near each ï¬gure.The descriptions of Fig.1 and Fig.2 are as follows.FIG. 1 is a perspective view of a portable light design.FIG. 2 is a front elevational view of FIG. 1.Question:Is the following description correct forFig.3?
FIG. 3 is a top plan view of FIG. 1.å‡ºåŠ›Incorrect. 3 Correct. 7(b)è¦–ç‚¹æ¨è«–ã‚¿ã‚¹ã‚¯ã®å…¥å‡ºåŠ›å›³ 1: æ„åŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å…¥å‡ºåŠ›ã®ä¸€ä¾‹ã€‚
ã‚¹ãƒšãƒ¼ã‚¹ã®éƒ½åˆä¸Šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ä¸€éƒ¨ã®ã¿è¡¨ç¤ºã—ã¦ã„ã‚‹ã€‚
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å…¨æ–‡ã¯Appendix A ã‚’å‚ç…§ã•ã‚ŒãŸã„ã€‚
(å›³ 1a Fig.5)ã€ãŠã‚ˆã³å›ç­”ã®é¸æŠè‚¢ç”»åƒã¨ã—ã¦ 2 ã¤ã®ç”»åƒ(å›³ 1a Fig.6, Fig.7)ã§ã‚ã‚‹ã€‚
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦ã€æ­£é¢ç”»åƒã®èª¬æ˜æ–‡ã€ã‚¯ã‚¨ãƒªç”»åƒã®èª¬æ˜æ–‡ã€ãŠã‚ˆã³è³ªå•æ–‡ã‚’å…¥åŠ›ã™ã‚‹ã€‚
æ­£é¢ç”»åƒã¨ã‚¯ã‚¨ãƒªç”»åƒã®èª¬æ˜ã«ã¯å¯¾è±¡ç‰©ã«é–¢ã™ã‚‹æƒ…å ±ã¨ã€è¦–ç‚¹æƒ…å ±ãŒè¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã€‚
ãªãŠæœ¬ç¨¿ã®å®Ÿé¨“ã§ã¯ã€å…¥åŠ›ç”»åƒæšæ•°ã®å¢—åŠ ã«ã‚ˆã‚‹å•é¡Œã®é›£åŒ–ã‚’é˜²ããŸã‚å…¥åŠ›ç”»åƒæ•°ã¯ 4 æšã«åˆ¶é™ã™ã‚‹ã€‚

è¦–ç‚¹æ¨è«–ã‚¿ã‚¹ã‚¯

å…¥å‡ºåŠ›ã®ä¾‹ã‚’å›³ 1b ã«ç¤ºã™ã€‚
ã“ã®ã‚¿ã‚¹ã‚¯ã¯ã€ã‚¯ã‚¨ãƒªç”»åƒã¨ã€ãã®ç”»åƒã®è¦–ç‚¹èª¬æ˜ã®æ•´åˆæ€§ã‚’å•ã† Yes orNo ã®è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ã€‚
å…¥åŠ›ã™ã‚‹ç”»åƒã¯ã€å¯¾è±¡ç‰©ã®æ­£é¢ç”»åƒ(å›³ 1b Fig.1)ã€ã‚¯ã‚¨ãƒªç”»åƒ(å›³ 1b Fig.3)ï¼ŒãŠã‚ˆã³ã€å·¦å³ã‚„å‰å¾Œã®è¦–ç‚¹ã®åŸºæº–ã‚’ä¸ãˆã‚‹ãŸã‚ã®è¦–ç‚¹åŸºæº–å›³(å›³ 1b Fig.2)ã§ã‚ã‚‹ã€‚
ã¾ãŸã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦ã€æ­£é¢ç”»åƒã®èª¬æ˜æ–‡ã€è¦–ç‚¹åŸºæº–ç”»åƒã®èª¬æ˜æ–‡ã€ãŠã‚ˆã³è³ªå•æ–‡ã¨ã‚¯ã‚¨ãƒªç”»åƒã®èª¬æ˜æ–‡ã‚’å…¥åŠ›ã™ã‚‹ã€‚
ãªãŠã€å®Ÿé¨“ã§ã¯ã€è¦–ç‚¹åŸºæº–ç”»åƒã‚’å…¥åŠ›ã™ã‚‹å ´åˆã¨ã—ãªã„å ´åˆã¨ã§æ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹ã€‚
3 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®ç”»åƒã§ã®æ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹ãŸã‚ã€ç·šç”»ã¨å†™å®Ÿç”»åƒã‚’ç”¨ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
ç·šç”»ã¯æ„åŒ ãƒ‡ãƒ¼ã‚¿ã®å›³ã‚’ç”¨ã„ã€å†™å®Ÿç”»åƒã¯ä¸€èˆ¬ç‰©ä½“ã®3D ã‚¹ã‚­ãƒ£ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç”»åƒã‚’ç”¨ã„ã‚‹ã€‚
3.1

æ„åŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

ç”»åƒã¯ DeepPatent ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ[19]1ï¼‰ã®ä¸€éƒ¨ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
DeepPatent ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ç±³å›½ç‰¹è¨±å•†æ¨™åº(USPTO2ï¼‰)ã‹ã‚‰åé›†ã•ã‚ŒãŸå¤§è¦æ¨¡ãªæ„åŒ ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚Šã€USPTO ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æ¡ä»¶3ï¼‰ã«å‰‡ã‚Š CC0 1.04ï¼‰ã«åŸºã¥ã„ã¦å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚
ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯ 45,000 ä»¶ã®æ„åŒ ã‹ã‚‰åé›†ã•ã‚ŒãŸ350,000 ã®ç”»åƒã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã€‚
å„ç”»åƒã«ã¯ 1 ã¤ä»¥ä¸Šã®å›³ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€å„å›³ã®èª¬æ˜æ–‡ã¯æ„åŒ ã®å…¬é–‹ç•ªå·ã‚’æ‰‹æ›ã‹ã‚Šã« Web ã‹ã‚‰åé›†å¯èƒ½ã§ã‚ã‚‹ã€‚
æ„åŒ ã®å›³ã®ç‰¹å¾´ã¯ã€( ğ‘– )ç·šç”»ã§ã‚ã‚Šã€( ğ‘–ğ‘– )åŒã˜ç‰©ä½“ã«ã¤ã„ã¦ã€è¤‡æ•°ã®ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰æã‹ã‚Œã¦ãŠã‚Šã€(ğ‘–ğ‘–ğ‘–)å„å›³ãŒã©ã“ã‹ã‚‰è¦‹ãŸå›³ãªã®ã‹ã¨ã„ã†è¦–ç‚¹æƒ…å ±ãŒè‡ªç„¶è¨€èªã§è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã¨ã„ã†ã¨ã“ã‚ã«ã‚ã‚‹ã€‚
æœ¬ç¨¿ã®å®Ÿé¨“ã§ã¯ã€CPC ã‚¯ãƒ©ã‚¹ã® Dï¼ŒEï¼ŒF ã‹ã‚‰ 147 ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ãŸã€‚
ã“ã‚Œã‚‰ã®ã‚¯ãƒ©ã‚¹ã«é™å®šã—ãŸç†ç”±ã¯ã€å®¹æ˜“ã«å¤‰å½¢ã™ã‚‹ç‰©ä½“ã‚„æ¥µç«¯ã«è–„ã„ç‰©ä½“ãªã©ã‚’å¯èƒ½ãªé™ã‚Šæ’é™¤ã™ã‚‹ãŸã‚ã§ã‚ã‚‹ã€‚
å„ç”»åƒã«ã¯ä¸€ã¤ä»¥ä¸Šã®å›³ãŒå«ã¾ã‚Œã€å„å›³ã«ã¯ â€œFig. Nâ€ ã¨ã„ã†å½¢å¼ã§å›³ç•ªå·ãŒä»˜ä¸ã•ã‚Œã¦ã„ã‚‹ã€‚
ææ¡ˆã‚¿ã‚¹ã‚¯ã®å…¥åŠ›å˜ä½ã¯ç”»åƒã§ã¯ãªãå›³ã§ã‚ã‚‹ãŸã‚ã€ç”»åƒã‹ã‚‰å›³ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
ã“ã®ä½œæ¥­ã¯æ‰‹ä½œæ¥­ã§å®Ÿæ–½ã—ã€ã“ã®ã¨ãåŒæ™‚ã«å›³ç•ªå·ã‚’ä»˜ä¸ã™ã‚‹ã€‚
å›³ã¨èª¬1ï¼‰ https://github.com/GoFigure-LANL/DeepPatent-dataset2ï¼‰ www.uspto.gov3ï¼‰ https://www.uspto.gov/terms-use-uspto-websites4ï¼‰ https://creativecommons.org/publicdomain/zero/1.0/ãƒˆãƒ¼ã‚¯ãƒ³æ•°ç”»åƒã‚½ãƒ¼ã‚¹ã‚¿ã‚¹ã‚¯ size mean min maxæ„åŒ åå¯¾å´äºˆæ¸¬ 335 80 67 98æ„åŒ è¦–ç‚¹æ¨è«–(åŸºæº–æœ‰) 294 85 68 106æ„åŒ è¦–ç‚¹æ¨è«–(åŸºæº–ç„¡) 294 73 60 91GSO åå¯¾å´äºˆæ¸¬ 150 69 63 74GSO è¦–ç‚¹æ¨è«–(åŸºæº–æœ‰) 300 71 71 71GSO è¦–ç‚¹æ¨è«–(åŸºæº–ç„¡) 300 61 61 61è¡¨ 1: ç”»åƒã‚½ãƒ¼ã‚¹åˆ¥ã®å„ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚¹ãƒšãƒ¼ã‚¹åˆ†å‰²ã—ãŸã¨ãã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€‚
æ˜æ–‡ã®å¯¾å¿œä»˜ã‘ã¯ã€å›³ç•ªå·ã®å®Œå…¨ä¸€è‡´ã«ã‚ˆã‚‹ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã«ã‚ˆã£ã¦è‡ªå‹•ã§è¡Œã†ã€‚
3.2 GSO ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”»åƒã¯ã€Google Scanned Objects ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(GSO)[20, 21]ã«å«ã¾ã‚Œã‚‹ç”»åƒã‚’ä½¿ç”¨ã—ã€æ„åŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨åŒæ§˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
GSOã¯ 1030 ã®ç‰©ä½“ã® 3D ã‚¹ã‚­ãƒ£ãƒ³ç”»åƒã§æ§‹æˆã•ã‚Œã€CCBY 4.05ï¼‰ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚
GSO ã«ã¯ã€å„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ã¤ãå‰é¢ã€è£é¢ã€å·¦å´ã€å³å´ã‹ã‚‰ã®ç”»åƒãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€‚
å‰é¢ãªã©ã®è¦–ç‚¹ãƒ©ãƒ™ãƒ«ã¯æ˜ç¤ºçš„ã«ä»˜ä¸ã•ã‚Œã¦ã„ãªã„ãŒã€ãƒ•ã‚¡ã‚¤ãƒ«åã®æ•´æ•°ãŒè¦–ç‚¹ãƒ©ãƒ™ãƒ«ã«å¯¾å¿œã—ã¦ã„ã‚‹ãŸã‚è‡ªå‹•çš„ã«ãƒ©ãƒ™ãƒªãƒ³ã‚°å¯èƒ½ã§ã‚ã‚‹ã€‚
ãŸã ã—è¦–ç‚¹æƒ…å ±ãŒè¨˜è¿°ã•ã‚ŒãŸèª¬æ˜æ–‡ã¯å«ã¾ã‚Œãªã„ãŸã‚ï¼Œâ€œFig.2 is a front view of the object.â€ ã®ã‚ˆã†ãªèª¬æ˜æ–‡ã‚’äººé–“ãŒä½œæˆã™ã‚‹ã€‚
ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆã¯è¡¨ 1 ã«ã¾ã¨ã‚ã‚‹ã€‚
4 å®Ÿé¨“4.1 å®Ÿé¨“è¨­å®šè©•ä¾¡å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ä»£è¡¨çš„ãªãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã‚‹ã€‚
ãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªãƒ¢ãƒ‡ãƒ«ã¯å®Ÿé¨“æ™‚ç‚¹ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ GPT-4o[1]ï¼ŒClaude 3.5 Sonnet6ï¼‰ã€ãŠã‚ˆã³ Gemini 1.5Pro[22]ã§ã‚ã‚‹ã€‚
ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¯ DeepSeek-VL [23], Mantis [24], MiniCPMv2.6 [25], Pixtral-12B7ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
top-pï¼Œtop-kã€æ¸©åº¦ãªã©ã®æ¨è«–æ™‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã™ã¹ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’æŒ‡å®šã™ã‚‹ã€‚
ã¾ãŸã€â€œLetâ€™s think step by step.â€ [26]ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ«å°¾ã«ä»˜ä¸ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚‚è©•ä¾¡ã™ã‚‹ã€‚
å®Ÿé¨“ã¯å„ãƒ¢ãƒ‡ãƒ«å„ã‚¿ã‚¹ã‚¯ã«ã¤ã 3 å›è¡Œã„æ­£è§£ç‡ã®å¹³å‡ã‚’æœ€çµ‚ã‚¹ã‚³ã‚¢ã¨ã™ã‚‹ã€‚
äººé–“ã«ã‚ˆã‚‹è©•ä¾¡ã¯ 2 åã®ã‚¢ãƒãƒ†ãƒ¼ã‚¿ã«ã‚ˆã£ã¦5ï¼‰ https://creativecommons.org/licenses/by/4.0/6ï¼‰ https://www.anthropic.com/news/claude-3-5-sonnet7ï¼‰ https://mistral.ai/news/pixtral-12b/å®Ÿæ–½ã™ã‚‹ã€‚
2 åã®è©•ä¾¡çµæœã®ã‚«ãƒƒãƒ‘ä¿‚æ•°ã¯ã€è¡¨ 2 ã®ã‚«ãƒ©ãƒ é †ã«å·¦ã‹ã‚‰ 0.760ï¼Œ0.656ï¼Œ0.571ï¼Œ0.759ï¼Œ0.625ï¼Œ0.625 ã§ã‚ã£ãŸã€‚
4.2 å®Ÿé¨“çµæœå„ãƒ¢ãƒ‡ãƒ«ã®å„ã‚¿ã‚¹ã‚¯ã®æ­£ç­”ç‡ã‚’è¡¨ 2 ã«ç¤ºã™ã€‚
åå¯¾å´äºˆæ¸¬ãŠã‚ˆã³è¦–ç‚¹åŸºæº–ã‚’å…¥åŠ›ã—ãŸå ´åˆã®è¦–ç‚¹æ¨è«–ã®äººé–“ã®çµæœã«ã‚ˆã‚‹ã¨ã€ã„ãšã‚Œã‚‚æ­£è§£ç‡ã¯ 95%ã‚’è¶…ãˆã¦ãŠã‚Šäººé–“ã«ã¨ã£ã¦ã¯éå¸¸ã«ç°¡å˜ãªã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚
è¦–ç‚¹åŸºæº–ã‚’å…¥åŠ›ã—ãªã„å ´åˆã¯ã€è¦–ç‚¹åŸºæº–ãŒå®šã¾ã‚‰ãªã„ãŸã‚å·¦å³ã‚„å‰å¾Œã«ä¸å®šæ€§ãŒæ®‹ã‚‹ã“ã¨ã‹ã‚‰äººé–“ã®æ€§èƒ½ã¯æ‚ªåŒ–ã—ã¦ã„ã‚‹ã€‚
ã»ã¨ã‚“ã©ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯ãƒ©ãƒ³ãƒ€ãƒ ä»¥ä¸‹ã§ã‚ã‚‹ã€‚
ä¸€æ–¹ã€ãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚³ã‚¢ã¯ã°ã‚‰ã¤ãã¯ã‚ã‚‹ã‚‚ã®ã®ã€æ¦‚ã­ Gemini ã®æ€§èƒ½ãŒä½ãã€Claude ã¨ GPT-4o ã¯æ¯”è¼ƒçš„é«˜æ€§èƒ½ã§ã‚ã‚‹ã€‚
ã¾ãŸã€CoT ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”¨ã„ãŸå ´åˆã€ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚¹ã‚³ã‚¢ã®æ”¹å–„ãŒã¿ã‚‰ã‚ŒãŸã‚‚ã®ã®ã€ãã®åŠ¹æœã¯å°ã•ã„ã€‚
ä»¥ä¸‹ã§ã¯ã€Claude ã®å‡ºåŠ›ã‚’ä¾‹ã«ã„ãã¤ã‹ã®è¦³ç‚¹ã‹ã‚‰å‡ºåŠ›ã‚’åˆ†æã™ã‚‹ã€‚


è¨€èªæƒ…å ±ã®åŠ¹æœ

åå¯¾å´äºˆæ¸¬ã®çµæœã‹ã‚‰ã€äººé–“ã§ã¯è¨€èªæƒ…å ±ãŒç„¡ãã¨ã‚‚å®¹æ˜“ã«æ­£è§£ã§ãã‚‹ã‚ˆã†ãªäº‹ä¾‹ã§ã‚‚ã€ãƒ¢ãƒ‡ãƒ«ã¯ã¾ãšãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰åå¯¾å´ã®è¨€èªè¡¨ç¾ã‚’æ¨æ¸¬ã—ã€ãã®çµæœã‚’æ‰‹ãŒã‹ã‚Šã«åå¯¾å´ã‹ã‚‰è¦‹ãŸç”»åƒã‚’æ¢ã™å‚¾å‘ãŒã‚ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚
ãã“ã§ã€è¨€èªæƒ…å ±ã‚’æ¥µåŠ›æ’é™¤ã™ã‚‹ãŸã‚ã«ã€ã‚¯ã‚¨ãƒªç”»åƒã®èª¬æ˜æ–‡ã‚’å…¥åŠ›ã›ãšã«å®Ÿé¨“ã‚’è¡Œã£ãŸã€‚
ãã®çµæœã€Claude ã®æ­£è§£ç‡ã¯ 0.500 ã§ã‚ã‚Šã€äººé–“ã‚’å¤§å¹…ã«ä¸‹å›ã‚‹çµæœã¨ãªã£ãŸã€‚
ã“ã®çµæœã¯ã€æœ€å…ˆç«¯ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚ã£ã¦è¨€èªæƒ…å ±ã«å¼·ãä¾å­˜ã—ã¦æ¨è«–ã‚’è¡Œãªã£ã¦ã„ã‚‹ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã™ã‚‹ã€‚


ã‚¨ãƒ©ãƒ¼åˆ†æ

ãƒ¢ãƒ‡ãƒ«ã¨äººé–“ã®ä¸¡æ–¹ãŒèª¤ç­”ã—ãŸäº‹ä¾‹ã‚’å›³ 2 ã«ç¤ºã™ã€‚
ã“ã®ä¾‹ã¯ã€è£½å“ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®å‰é¢ç”»åƒã®åå¯¾å´ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ã€‚
å‰å¾Œå·¦å³ã®å´é¢ãŒã»ã¼åŒã˜ã‚µã‚¤ã‚ºã®æ­£æ–¹å½¢ã§ã‚ã‚‹ãŸã‚ã€è¦–è¦šæƒ…å ±ã ã‘ã§ã¯äººé–“ã«ã¨ã£ã¦ã‚‚é›£ã—ã„äº‹ä¾‹ã§ã‚ã‚‹ã€‚
Claude (CoT)ã®æŒ¯ã‚‹èˆã„ã¯ã€ã€Œè£é¢ã«ã¯æ „é¤Šæˆåˆ†è¡¨ç¤ºãŒã‚ã‚‹ã ã‚ã†ã€ã¨ã„ã†äº‹å‰çŸ¥è­˜ã‚’æ‰‹æ›ã‹ã‚Šã«ã—ã¦ç®±ã®å„å´é¢ã«ãƒ—ãƒªãƒ³ãƒˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’èª­ã¿å–ã‚Šã€å›ç­”ã‚’è©¦ã¿ã¦ã„ã‚‹ã“ã¨ãŒèª­ã¿å–ã‚Œã‚‹ã€‚
çµæœçš„ã«ã¯ä¸æ­£è§£ã ãŒã€ã“æ„åŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ GSO ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåå¯¾å´äºˆæ¸¬è¦–ç‚¹æ¨è«–è¦–ç‚¹æ¨è«–(w/o ref.) åå¯¾å´äºˆæ¸¬è¦–ç‚¹æ¨è«–è¦–ç‚¹æ¨è«–(w/o ref.)Random 0.333 0.500 0.500 0.333 0.500 0.500Human 0.960 0.969 0.816 0.957 0.958 0.923#params Open-source modelsdeepseek-vl-7b-chat 7B 0.040 (0.051) 0.623 (0.431) 0.506 (0.458) 0.260 (0.207) 0.517 (0.477)
0.500 (0.503)Mantis-8B-siglip-llama3 8B 0.026 (0.023) 0.500 (0.461) 0.488 (0.446) 0.413 (0.460) 0.543 (0.557) 0.543 (0.460)MiniCPM-V-2 6 8B 0.382 (0.346) 0.624 (0.595) 0.547 (0.549) 0.467 (0.451) 0.506 (0.577) 0.513 (0.570)Pixtral-12B-240912B 0.704 (0.755) 0.578 (0.560)0.497 (0.545) 0.487 (0.673) 0.591 (0.577)0.605 (0.620)Proprietary modelsGemini 1.5 Pro (Accessed in early Oct. 2024) 0.632 (0.567) 0.502 (0.528) 0.414 (0.441) 0.842 (0.807) 0.582 (0.509) 0.582 ( 0.517)Claude 3.5 Sonnet (claude-3-5-sonnet-20240620) 0.823 (0.830) 0.761 (0.775) 0.687 (0.691) 0.747 (0.753) 0.647 (0.643) 0.583 (0.586)GPT-4o (gpt-4o-2024-08-06) 0.888 (0.866) 0.916 (0.901) 0.738 (0.708) 0.893 (0.873) 0.637 (0.727) 0.697 ( 0.720)è¡¨ 2: å„ãƒ¢ãƒ‡ãƒ«å„ã‚¿ã‚¹ã‚¯ã®æ­£è§£ç‡ã€‚
æ•°å€¤ã¯ 3 å›ã®è©¦è¡Œã®æ­£è§£ç‡ã®å¹³å‡ã‚’è¡¨ã—()å†…ã®æ•°å€¤ã¯ CoT ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®çµæœã§ã‚ã‚Šã€èµ¤å­—ã¯ CoT ã§ã‚¹ã‚³ã‚¢ãŒå‘ä¸Šã—ãŸã“ã¨ã‚’ç¤ºã—ã€é’å­—ã¯æ‚ªåŒ–ã—ãŸã“ã¨ã‚’ç¤ºã™ã€‚
â€œè¦–ç‚¹æ¨è«–ï¼ˆw/o ref.ï¼‰â€ ã¯è¦–ç‚¹åŸºæº–ç”»åƒã¨ãã®èª¬æ˜æ–‡ã‚’å…¥åŠ›ã—ãªã„å ´åˆã‚’æ„å‘³ã™ã‚‹ã€‚
ã®ã‚ˆã†ãªæ¨è«–éç¨‹ã¯äººé–“ã«ã‚ˆã‚‹ãã‚Œã«è¿‘ã—ã„æŒ¯ã‚‹èˆã„ã§ã‚ã‚‹ã€‚
5 é–¢é€£ç ”ç©¶ææ¡ˆã‚¿ã‚¹ã‚¯ã®é¡ä¼¼ã‚¿ã‚¹ã‚¯ã¨ã—ã¦ BLINK [4]ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ multi-view reasoning ãŒè¿‘ã„ã€‚
ã“ã®ã‚¿ã‚¹ã‚¯ã¯ã€åŒã˜ç‰©ä½“ã‚’ 2 ã¤ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰æ’®å½±ã—ãŸã¨ãã® 2 ã¤ã®ç”»åƒã®å›è»¢æ–¹å‘ï¼ˆæ™‚è¨ˆå‘¨ã‚Š/åæ™‚è¨ˆå‘¨ã‚Šï¼‰ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ã€‚
ã¾ãŸã€CLEVR-MRT [14]ã¯ã€CLEVR ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ[27]ã«åŸºã¥ãã€å¿ƒçš„å›è»¢èƒ½åŠ›[17]ã‚’è©•ä¾¡ã™ã‚‹ VQA ã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ã€‚
PerspectiveTaking ã‚¿ã‚¹ã‚¯[13]ã¯ã€ç‰©ã®ä½ç½®ã‚„æ–‡å­—ï¼ˆMãŠã‚ˆã³ Wï¼Œ6 ãŠã‚ˆã³ 9ï¼‰ãŒä»–è€…è¦–ç‚¹ã‹ã‚‰ã©ã®ã‚ˆã†ã«è¦‹ãˆã‚‹ã‹ã‚’å•ã†ã‚¿ã‚¹ã‚¯ã¨ã—ã¦ææ¡ˆã•ã‚Œã¦ã„ã‚‹ã€‚
ã“ã®ã‚¿ã‚¹ã‚¯ã¯ã€åˆ¥è¦–ç‚¹ã‹ã‚‰ã®è¦‹ãˆæ–¹ã‚’å•ã†ã¨ã„ã†æ„å‘³ã§æˆ‘ã€…ã®ã‚¿ã‚¹ã‚¯ã«è¿‘ã—ã„ãŒã€è¦–ç‚¹ç²å¾—[18]ã¨ã„ã†å¿ƒç†å­¦çš„ãªç¾è±¡ã®æ¤œè¨¼ã‚’é‡è¦–ã—ã¦ãŠã‚Šã€è¦‹ãˆãªã„éƒ¨åˆ†ã®èªè­˜èƒ½åŠ›ã‚’æ¸¬ã‚‹ã‚¿ã‚¹ã‚¯ã¨ã¯ç•°ãªã‚‹ã€‚
6 ãŠã‚ã‚Šã«æˆ‘ã€…ã¯ã€ç‰©ã®è¦‹ãˆãªã„éƒ¨åˆ†ã‚’æƒ³åƒã™ã‚‹èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ææ¡ˆã—ã€ç·šç”»ã¨å†™å®Ÿç”»åƒã‚’ç”¨ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ãŸã€‚
å®Ÿé¨“ã§ã¯ã€ææ¡ˆã‚¿ã‚¹ã‚¯ã§ä»£è¡¨çš„ãª VLM ã‚’è©•ä¾¡ã—ã€äººé–“ã®æ€§èƒ½ã¨æ¯”è¼ƒã—ãŸã€‚
å®Ÿé¨“ã®çµæœã€ææ¡ˆã‚¿ã‚¹ã‚¯ã¯äººé–“ã«ã¨ã£ã¦éå¸¸ã«ç°¡å˜ã§ã‚ã‚‹ãŒã€æœ€ã‚‚å„ªã‚ŒãŸæ€§èƒ½ã§ã‚ã‚‹ GPT-4o ã§ã•ãˆã‚‚äººé–“ã«ã¯åŠ£ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚
æˆ‘ã€…ã®ææ¡ˆã‚¿ã‚¹ã‚¯ãŒã€ã“ã‚Œã¾ã§æ˜ç¤ºçš„ã«è©•ä¾¡ã•ã‚Œã¦ã„ãªã‹ã£ãŸ VLMã®èƒ½åŠ›ã®ä¸€ã¤ã®å´é¢ã‚’æµ®ãå½«ã‚Šã«ã™ã‚‹ã“ã¨ã§ã€å°†æ¥ã® VLM ã®é–‹ç™ºã‚’ä¿ƒé€²ã™ã‚‹ã“ã¨ã‚’æœŸå¾…ã™ã‚‹ã€‚
Fig.1 Fig.2 Fig.3 Fig.4ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:These images are diï¬€erent views of the same object. The Fig numbers are locatednear each ï¬gure.The descriptions of Fig.2 and Fig.1 are as follows.Fig.2 is a front view of the object.Fig.1 is an image of Fig.2 viewed from the left.Question: Which ï¬gure is the opposite view of Fig.2?
Letâ€™s think step-by-step. Please state your conclusion in the ï¬rst line of yourresponse.Your answer:ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›:The opposite view of Fig.2 is Fig.3. 7Hereâ€™s the step-by-step reasoning:1. Fig.2 shows the front view of a Nescafe Dolce Gusto Cappuccino box.2. In packaging, the opposite side of the front is typically the back, which usuallycontains nutritional information and other details.3. Looking at the other ï¬gures:- Fig.1 shows a side of the box with some product information and a small image.- Fig.3 displays detailed nutritional facts and information in multiple languages.- Fig.4 shows an array of colorful capsules, likely representing diï¬€erent ï¬‚avors.4. Among these, Fig.3 is the only one that matches what weâ€™d expect to see on theback of a food product packaging - comprehensive nutritional information.5. Therefore, Fig.3, containing the nutritional facts, is most likely the opposite view(back) of the front view shown in Fig.2.å›³ 2: ç®±ã®è£å´ã®äºˆæ¸¬ã«å¤±æ•—ã—ãŸä¾‹ã€‚
CoT ã«ã‚ˆã‚Šæ¨è«–éç¨‹ã‚’å‡ºåŠ›ã—ãŸã¨ã“ã‚ã€ãƒ¢ãƒ‡ãƒ«ã¯ç®±å´é¢ã®è¨˜è¼‰å†…å®¹ã‚’æ‰‹æ›ã‹ã‚Šã«è£å´ã®äºˆæ¸¬ã‚’è©¦ã¿ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹(ãƒã‚¤ãƒ©ã‚¤ãƒˆéƒ¨åˆ†)ã€‚



è¬è¾

æœ¬ç ”ç©¶ã®ä¸€éƒ¨ã¯ JST ã•ããŒã‘ JPMJPR21C8 ã®æ”¯æ´ã‚’å—ã‘ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚

å‚è€ƒæ–‡çŒ®


[1] OpenAI et al. GPT-4 Technical Report, 2024.
[2] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu,and Enhong Chen. A survey on multimodal large language models,2024.
[3] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li,Lijuan Wang, and Jianfeng Gao. Multimodal Foundation Models:From Specialists to General-Purpose Assistants. arXiv e-prints,page arXiv:2309.10020, 2023.
[4] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang,Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, and RanjayKrishna. BLINK: Multimodal Large Language Models Can Seebut Not Perceive, 2024.
[5] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu,Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, YuxuanSun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, MingYin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang,Huan Sun, Yu Su, and Wenhu Chen. MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmarkfor Expert AGI. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pages9556â€“9567, 2024.
[6] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang,Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun,Yu Su, Wenhu Chen, and Graham Neubig. MMMU-Pro: A MoreRobust Multi-discipline Multimodal Understanding Benchmark.arXiv e-prints, page arXiv:2409.02813, 2024.
[7] Song Dingjie, Shunian Chen, Guiming Hardy Chen, Fei Yu, XiangWan, and Benyou Wang. MileBench: Benchmarking MLLMs inLong Context. In First Conference on Language Modeling,2024.
[8] Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiao-geng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang,Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu,Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, ShengZhang, Hoifung Poon, and Muhao Chen. MuirBench: A Compre-hensive Benchmark for Robust Multi-image Understanding. arXive-prints, page arXiv:2406.09411, 2024.
[9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang,Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin,and Feng Zhao. Are we on the right way for evaluating largevision-language models?, 2024.
[10] Kaziwa Saleh, Sandor Szenasi, and Zoltan Vamossy. Occlu-sion Handling in Generic Object Detection: A Review. In2021 IEEE 19th World Symposium on Applied Machine In-telligence and Informatics (SAMI). IEEE, 2021.
[11] Dan Zeng, Raymond N. J. Veldhuis, and Luuk J. Spreeuwers. Asurvey of face recognition techniques under occlusion. IET Biom. ,10:581â€“606, 2020.
[12] Rohit Mohan and Abhinav Valada. Amodal panoptic segmentation.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 21023â€“21032,2022.
[13] Bridget Leonard, Kristin Woodard, and Scott O. Murray. Failuresin perspective-taking of multimodal ai systems, 2024.
[14] Christopher Beckham, Martin Weiss, Florian Golemo, SinaHonari, Derek Nowrouzezahrai, and Christopher Pal. Visual Ques-tion Answering From Another Perspective: CLEVR Mental Rota-tion Tests, 2022.
[15] J. Piaget and B. Inhelder. Psychology Of The Child. The Psy-chology of the Child. Basic Books, 1969.
[16] John H. Flavell. The development of knowledge about visualperception. Nebraska Symposium on Motivation. NebraskaSymposium on Motivation, 25:43â€“76, 1977.
[17] Roger N. Shepard and Metzler Jacqueline. Mental rotation ofthree-dimensional objects. Science, 171(3972):701â€“703, 1971.
[18] Andrew Surtees, Ian Apperly, and Dana Samson. Similaritiesand diï¬€erences in visual and spatial perspective-taking processes.Cognition, 129(2):426â€“438, 2013.
[19] Michal Kucer, Diane Oyen, Juan Castorena, and Jian Wu. Deep-Patent: Large scale patent drawing recognition and retrieval. In2022 IEEE/CVF Winter Conference on Applications of Com-puter Vision (WACV), pages 557â€“566, 2022.
[20] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman,Ryan Hickman, Krista Reymann, Thomas B. McHugh, and VincentVanhoucke. Google Scanned Objects: A High-Quality Dataset of3D Scanned Household Items. In 2022 International Conferenceon Robotics and Automation (ICRA), pages 2553â€“2560, 2022.
[21] GoogleResearch. Scanned objects by google research.
[22] Gemini Team et al. Gemini: A family of highly capable multimodalmodels, 2024.
[23] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong,Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang,Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and ChongRuan. DeepSeek-VL: Towards Real-World Vision-Language Un-derstanding, 2024.
[24] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, QianLiu, and Wenhu Chen. MANTIS: Interleaved Multi-Image Instruc-tion Tuning, 2024.
[25] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui,Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He,Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang,Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, GuoyangZeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. MiniCPM-V: AGPT-4V Level MLLM on Your Phone, 2024.
[26] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Mat-suo, and Yusuke Iwasawa. Large Language Models are Zero-ShotReasoners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,and Kyunghyun Cho, editors, Advances in Neural InformationProcessing Systems, 2022.
[27] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. CLEVR: ADiagnostic Dataset for Compositional Language and ElementaryVisual Reasoning. In 2017 IEEE Conference on Computer Vi-sion and Pattern Recognition (CVPR), pages 1988â€“1997, 2017.

A å‚è€ƒæƒ…å ±å®Ÿé¨“ã§ä½¿ç”¨ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€Œè¤‡æ•°ã®å…¥åŠ›ç”»åƒã¯åŒã˜å¯¾è±¡ç‰©ã«ã¤ã„ã¦ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰è¦‹ãŸç”»åƒã§ã‚ã‚‹ã€ã“ã¨ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ•™ãˆã‚‹ã“ã¨ã‚’æ„å›³ã—ã¦å®Ÿé¨“çš„ã«æ±ºå®šã—ãŸã€‚
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å…¨ä½“ã‚’ Appendix A.1,Appendix A.2 ã«ç¤ºã™ã€‚
å¤ªå­—ã®éƒ¨åˆ†ã¯äº‹ä¾‹æ¯ã«ç•°ãªã‚Šã€ãã‚Œä»¥å¤–ã¯ã™ã¹ã¦ã®äº‹ä¾‹ã§å…±é€šã§ã‚ã‚‹ã€‚
ä¸€éƒ¨ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¯ä»•æ§˜ã«å¾“ã£ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å…ˆé ­ã« < ğ‘–ğ‘šğ‘ğ‘”ğ‘’ > ãªã©ã® image token ã‚’æŒ¿å…¥ã™ã‚‹ã€‚
A.1 åå¯¾å´äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“These images are diï¬€erent views of the same object.The Fig numbers are located near each ï¬gure.The descriptions of Fig.1 and Fig.6 are as follows.FIG. 1 is a top perspective view of a of a wall-mounted laun-dry bin cabinet showing my new design. FIG. 6 is a frontelevation view of FIG. 1.Question: Which ï¬gure is the opposite view of Fig.6?
Please answer in a straightforward manner. Do not include aquestion in your response.(Letâ€™s think step-by-step. Please state your conclusion in the ï¬rstline of your response. )Your answer :A.2 è¦–ç‚¹æ¨è«–ã‚¿ã‚¹ã‚¯ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“The three images are diï¬€erent views of the same object.The ï¬g numbers are located near each ï¬gure.The descriptions of Fig.1 and Fig.6 are as follows.FIG. 1 is a top perspective view of a of a wall-mounted laun-dry bin cabinet showing my new design. FIG. 6 is a frontelevation view of FIG. 1.Question: Is the following description correct for Fig.7?
FIG. 7 is a rear elevation view of FIG. 1.If this statement is correct, answer â€œcorrectâ€ otherwise, answerâ€œincorrectâ€.Please answer in a straightforward manner. Do not include aquestion in your response.(Letâ€™s think step-by-step. Please state your conclusion in the ï¬rstline of your response. )Your answer :