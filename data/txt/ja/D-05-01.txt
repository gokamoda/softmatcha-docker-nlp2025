構成性の度合いはカオスの縁で最も高くなる—ESN モデルと TopSim 指標を用いたケーススタディ—

上田亮



東京大学



ryoryoueda@is.s.u-tokyo.ac.jp



概要

我々には想像もつかないような帰納バイアスにおいて、想像もつかないような記号体系が最適であったかもしれないのに、構成性が言語の必然的な性質であると信じて良いのだろうか？ この問いに答えるためには、言語の進化（文化進化）のみならず、人類の進化（生物進化）にも思いを馳せながら、構成性と帰納バイアスの関係を考察する必要がある。
事例研究として、Echo State Network モデルがカオスの縁と呼ばれる超パラメタ領域において構成的な（高い TopSim スコアをもつ）記号体系に適合・汎化するバイアスをもつことを実験的に示し、構成的な言語が創発する必然性への傍証を試みる。


1 はじめに

大規模言語モデル（LLM）が躍進している裏で、言語創発（Language Emergence）又は創発コミュニケーション（Emergent Communication; EmCom, EC）と呼ばれる、計算言語学や進化言語学の派生分野が一部で注目を集めている[1] （以降では EC と呼ぶことにする)。
EC は、エージェント同士でコミュニケーションをさせたときに生じる言語のようなプロトコル（創発言語）を研究対象とし、これが人間の言語らしくなる条件を探求することによって間接的に言語のメカニズムの解明を志す—いわゆる構成論的アプローチをとる分野である。
とりわけ、近年の国内コミュティにおいては、記号創発ロボティクス（Symbol Emergence in Robotics）との融合を通して生成的 EC とでも言うべき新たな潮流が生まれつつある[2, 3, 4]。
生成的 EC は、これまで主にマルチエージェント強化学習やゲーム理論の範疇として扱われてきた EC の諸問題を、おおよそ等価なベイズ的学習・推論として再解釈することによって『言語とコミュニケーションの創発』の問題をより一般的な枠組みから捉え直そうという構想・立場である。
あるいは、EC とはエージェントが協調して言語という表現を作る表現学習であって、そのうち特にベイズ的なアプローチをとる立場とも言える1）．生成的 EC の構想の下、ベイジアンネットワークのようなフレームワークで EC の問題を捉え直すことには一定の意義がある一方で、計算言語学において伝統的に研究対象とされてきた構成性（Compositionality）のような性質を説明するにはやや不十分に思われる。
構成性は明らかに帰納バイアスに関わる問題であり、その性質は具体的なモデルの定義に依存するため、グラフィカルモデル上の頂点や矢印の類では（少なくとも簡潔には）表せないだろう。
ベイズ的なアプローチをとるという基本的な方針からはそれほど乖離することなく、この問題に対する良い道具立てを用意することはできないだろうか？
ここで、従来の意味での EC や、それに関連する進化言語学の先行研究に視点を戻してみると、言語において構成性が創発するのは「構成性の低い言語（らしきもの）に比べて学習しやすい（適合しやすく汎化もしやすい）から」だと考えられているようである。
有名な例として Iterated Learning Model（ILM）[6, 7]という、親世代から子世代への言語継承を考慮した言語創発モデルがある。
ILM の枠組みでは、言語を継承する過程がボトルネックとなり、言語はそのボトルネックを潜り抜けることができるように “進化” していく（文化進化）。
文化進化の帰結として、言語の構成性が向上していくことが人間や LSTM エージェントを用いた実験で確認されている[8, 9] （実験記号論）。
しかしながら、ここでもやはり構成性は帰納バイアスに依存した性質であることに注意する必要がある。
即ち、高い構成性をもつ1） 谷口[5]はこの立場に立脚し、言語とは感覚運動機能を有する人間の集団が作る世界モデルであり、LLM はこの世界モデルの模倣しているという仮説を唱えている（CPC 仮説)。
言語がそうでない言語に比べて（我々の直感においても実験記号論の結果においても）学習しやすいのは、文化進化よりも遥かに緩やかで長い時間スケールにおいて起こってきた人類の進化（生物進化）の帰結としてもたらされた特定の帰納バイアスにおいて偶然そうなのであって、必然的にもたらされたものではないかもしれない。
構成的な言語で思考する我々人類には最早想像もつかない帰納バイアスにおいて、想像もつかない記号体系が学習しやすいと見做されることもあり得たかもしれない。
これに対して適切に反論するためには、生物進化の帰結として必然的にもたらされる帰納バイアス（パラメタ領域）は如何なるものであるのか？ その帰納バイアスのもとで本当に構成性は最適であり得るのか？ という問いに答えなければならない。
また、近年の EC の発展は深層学習に依るところが大きい[10]。
深層学習はエージェントの学習能力を劇的に向上させ、面白い実験結果をもたらしてくれるようになった一方で、複雑な非線形性によってその解釈は難しく、帰納バイアスと構成性についての議論を一層困難なものにしている。
以上のような背景と問題意識から、理論的に扱いやすく、“技巧倒れ” しておらず（下手に複雑化することによって解釈性や認知的妥当性を損なってしまうことなく）、生物進化の帰結に関わる議論をするのに必要な知見がある程度蓄積しているものであって（例えば、進化計算による超パラメタ探索の結果どのような領域に収斂しやすそうかなどの議論ができると良い）、一方である程度意味のある議論ができるように最低限の学習能力を備えたモデルを人工エージェントとして採用し、議論をすることが望ましい。
本稿では、そのようなモデルとして EchoState Network（ESN）[11, 12, 13]を採用する。
ESNは、回帰型ニューラルネットワーク（RNN）の一種であり、所謂 Simple RNN と同様の構造をもち、特に線形出力層以外をランダム初期化時の値のまま固定（凍結）したモデルである。
著しい制約がある一方，（直感に反して）それなりの学習性能をもつことが時系列データを扱う研究で知られている[14]。
興味深いことに、秩序相とカオス相と呼ばれる超パラメタ領域の間にある極々狭い領域—カオスの縁—において、ESN は特に高い性能を発揮することが知られている[15]。
カオスの縁は、複雑系科学や人工生命、生命進化の文脈で古くから用いられ、カオスの縁こそが知能や生命（らしさ）の創発する領域なのではないかと目されてきた[16]。
従って、この性質は生物進化の収斂先を議論するのに相応しいといえよう。
また、ESN はガウス過程（GP）回帰モデルとして用いることもできるため、ベイズ的な立場をとる生成的 EC の価値観ともそれほど乖離しない。
実験においては、構成性の度合いを測るための指標として Topographic Similarity（TopSim）[17]を採用することとした。
人工的に構築したデータセットに対し、ESN に基づく GP 回帰モデルを適用した結果、カオスの縁においてモデルの TopSim が特に高くなることが明らかになった。



2 問題設定

𝕄 をメッセージ空間、ℤ を意味空間とする。
メッセージm∈𝕄からそれに対応する意味z∈ℤを予測するタスクを考え、これを ESN GP 回帰モデルで解く。
直感的には ILM における親から子への継承に相当し、親を教師とするベイズ教師あり学習をする格好となる2）。
アルファベットサイズを 𝑁inとし、メッセージの最大長を 𝐿 とする。
このときメッセージ空間 𝕄 を次のように定義する3）：𝕄 : =n𝑚1· · · 𝑚ℓ𝑚1,...,𝑚ℓ −1∈ {2,..., 𝑁in},𝑚ℓ=1, 1≤ℓ ≤ 𝐿o. (1)一方で、意味空間 ℤ は 𝑁out次元 Euclid 空間とする4）．ここで、メッセージと意味のペアから成るデータセット 𝔻 = {(m𝑖, x𝑖)}𝑁data𝑖=1を考える。
𝔻 の TopSim スコア（以降の節で定義する）は所与のパラメタ 𝜌dataにおおよそ一致するようになっているものとする。
これを 1 : 1 の大きさで訓練データ 𝔻trainと評価データ 𝔻evalに分割して学習と評価に使う。

3 ESN

入力系列 {𝒖𝑡∈ ℝ𝑁in}𝑇𝑡=1に対して、時刻 𝑡 におけるESN の（隠れ）状態 𝒉𝑡∈ ℝ𝑁recおよび出力 𝒐𝑡∈ ℝ𝑁outは典型的には以下のように定義される[13]：𝒉𝑡: = (1 − 𝑎) 𝒉𝑡 −1+ 𝑎 𝑓 (𝑾rec𝒉𝑡 −1+ 𝑾in𝒖𝑡)(2)𝒐𝑡: = 𝑾out𝒉𝑡. (3)2） ここで、GP 回帰モデルには「不確実性を考慮する 1 人の子エージェントによる推論」と「点推定する子エージェント集団の分布」という 2 通りの解釈があり得るように思われるが、本稿ではその解釈を保留しておく。
3） 1 をメッセージの終端を意味する特別な記号として扱う。
4） そもそも意味とは何か？ という問題に立ち入ることは避けるが、当分野がニューラルネットワークのパラダイムに従い続ける限り、データの変換過程で必ず経由する実数ベクトルを便宜的に意味と呼ぶのは不自然ではないように思う。
ここで、 𝑓 (·)は要素ごとの（非線形）写像であり、tanh や ReLU などが用いられる。
𝑎 ∈ (0, 1]は漏れ率と呼ばれる超パラメタであり、機械学習分野における残差接続に類似した機構を提供する。
3 つの行列𝑾rec∈ ℝ𝑁rec×𝑁rec, 𝑾in∈ ℝ𝑁rec×𝑁in, 𝑾out∈ ℝ𝑁out×𝑁recのうち 𝑾rec, 𝑾inはランダム初期化をしたのち固定（凍結）し，𝑾outだけを最適化する。
𝑾rec, 𝑾inの初期化は 𝜎rec, 𝜎in> 0 を超パラメタとして以下のようにして行うこととする5）：(𝑾rec)𝑖 𝑗i.i.d∼N0,𝜎2rec𝑁rec, (𝑾in)𝑖 𝑗i.i.d∼N0, 𝜎2in. (4)入力に関する記号の濫用入力 𝒖𝑡∈ ℝ𝑁inは 𝑁in次元であるとしたが、本稿で実際に考えることになる入力記号は有限の自然数であり、行列演算をそのまま適用することができない。
入力を one-hot 符号化することでこの問題に対処するが、これは言語処理分野における一般的なプラクティスであるため以降の議論ではわざわざ明示しない6）．初期状態の決め方 𝒉0をゼロベクトルではなくランダム系列を 100 ステップ入力して得られたベクトルとする。
これは RC 分野における一般的なプラクティスである（便宜的に “warm up” と呼ぶ)。


4 構成性の指標 TopSim

メッセージ空間 𝕄、意味空間 ℤ にはそれぞれ距離関数 𝑑𝕄(·, ·), 𝑑ℤ(·, ·)が備わっているとする。
また、メッセージと意味のペアから成るデータセット𝔻 = {(m𝑖, z𝑖)}𝑁𝑖が与えられているとし、距離関数とデータセットを用いて以下のデータ𝕏 : = {(𝑑𝕄(m𝑖, m𝑗), 𝑑ℤ(z𝑖, z𝑗) | 1 ≤ 𝑖 < 𝑗 ≤ 𝑁}, (5)を考える。
このとき、𝔻 に関する TopSim 𝜌(𝔻)を、データ 𝕏 の Pearson 相関係数とする7）8）。
本稿では、𝑑𝕄を Levenshtein 距離、𝑑ℤを Euclid 距離とする。


5 ESN を用いる意義

LSTM [18]や Transformer [19]のような比較的最近のモデルを使わず、わざわざ ESN を採用する意義5） 𝑾recの初期化はスペクトル半径を制御する形で行われることの方が多い[13]が、各成分を独立に初期化する方が “創発” に関する研究としては自然と考えこのようにしている。
6） あえて明示的に書くなら 𝒉𝑡= (1 − 𝑎)𝒉𝑡 −1+ 𝑎 𝑓 (𝑾rec𝒉𝑡 −1+𝑾inOneHot(𝒖𝑡)) . あるいは(𝑾in):,𝒖𝑡などとしてもよい。
7） [17]に倣って Pearson 相関係数を用いているが、Spearman相関係数を用いることも多い。
8） これが構成性の指標であることは直感的ではないではないかもしれないが、構成性が「写像の構造保存性」として特徴づけられるならば、TopSim は「写像がどの程度距離を保存するか」を測る指標であるため、そこまで不自然ではない。
について
は
第 1 節でも触れたが、本節で少し補足的な説明を加えておく。

5.1 理論的な扱いやすさ

出力層 𝑾out以外を凍結していることから、回帰層は古典的機械学習の意味での素性関数 𝝓(m)と見做せる。
故に ESN は線形回帰モデルである：
𝒐 = 𝑾out𝝓(m). (6)実際、ESN を用いた研究ではリッジ回帰によって𝑾outを求めることが多い。
さらに、𝑘 (m, m′) : =1𝑁rec⟨𝝓(m), 𝝓(m′)⟩ + 𝜎2out𝛿(m, m′)(7)などとおけば、𝑘 (·, ·)はカーネル関数として使えるため、GP 回帰[20, 21]を適用することもできる[22, 23]。
ここで、⟨·, ·⟩ は通常の内積であり、𝛿(·, ·)は 2 引数が同一のとき 1、それ以外のとき 0 を返す関数である。
訓練用メッセージデータ {m𝑖}𝑁train𝑖=1，訓練用意味データ{y𝑖}𝑁train𝑖=1、評価用メッセージデータ{m∗𝑖}𝑁eval𝑖=1が与えられたとき、以下の 4 行列(𝑲)𝑖1𝑖2: = 𝑘 (m𝑖1, m𝑖2), (𝑲∗)𝑖3𝑖1: = 𝑘 (m∗𝑖3, m𝑖1),(𝒀)𝑖1𝑖5: =(y𝑖1)𝑖5, (𝑲∗∗)𝑖3𝑖4: = 𝑘 (m∗𝑖3, m∗𝑖4)(8)を用いれば(ただし 1 ≤ 𝑖1, 𝑖2≤ 𝑁train, 1 ≤ 𝑖3, 𝑖4≤𝑁eval, 1 ≤ 𝑖5≤ 𝑁out)、評価用メッセージデータに対する意味の予測分布は以下のように表される：N(𝑲∗𝑲−1𝒀, 𝑲∗∗− 𝑲∗𝑲−1𝑲T∗). (9)

5.2 認知的な妥当性

認知的に妥当と思われるモデルを採用することは EC 分野においても重要であるが、LSTM やTransformer のような比較的最近のモデルが人間らしいといえるかどうかについては未だはっきりとした結論が得られていない。
1 つの妥協案として、できるだけシンプルなモデルを採用することがあり得る．ESN は、非常に簡素な連続時間微分方程式で記述されるニューロンモデルの自然な離散化として導けることが知られている[24]
。



5.3 複雑系科学（カオスの縁）の再訪

ESN の研究ではカオスの縁という概念がしばしば登場する。
カオスの縁とは、秩序相とカオス相の間に存在する狭い超パラメタ領域であり、この領域において ESN の性能が最も高くなることが経験的に知られている。
ESN が秩序相、カオス相、カオスの図 1 超パラメタ 𝜎rec(横軸)及び 𝜎in(縦軸)と各指標との関係。
縁のいずれに位置しているのか確認するため最大リヤプノフ指数（MLE）[25]がしばしば用いられる：𝜆mle: = lim𝑇→∞1𝑇log𝜕𝒉𝑇𝜕𝒉𝑇 −1𝜕𝒉𝑇 −1𝜕𝒉𝑇 −2· · ·𝜕𝒉1𝜕𝒉0. (10)∥ · ∥ は行列のスペクトルノルムとする。
𝜆mle< 0のとき秩序相、𝜆mle> 0 のときカオス相、そして𝜆mle≈ 0 のときカオスの縁を意味する。
𝜆mle< 0 の場合、𝒉𝑡が “小さく” なろうとするため、過去の入力情報の記憶がすぐに失われてしまう。
逆に 𝜆mle> 0の場合、𝒉𝑡が “大きく” なろうとするため、過去の小さなノイズを拡大して無秩序な状態になってしまう．𝜆mle≈ 0 のときにのみ、頑健性と柔軟性を両立したちょうどよい状態が保たれる。


6 実験

第 2 節で述べた問題設定を考え、超パラメタ𝜎rec, 𝜎inと評価指標の関係について議論する（その他の超パラメタは表 1 を参照）9）10）．表 1 𝜎rec, 𝜎in以外の超パラメタ。
𝑁rec𝑁in𝑁out𝑁data𝜌data𝑎 𝐿 𝜎out𝑓512 32 32 1024 0.7 1 64 1 tanh

6.1 評価指標

予測に基づく TopSim 評価用データからの入力(メッセージ)と、それに対する GP 回帰の推定値𝑲∗𝑲−1𝒀 の間の TopSim を評価指標として用いる。
特徴ベクトルに基づく TopSim ESN が特徴関数𝝓(·)と見做せることは既に述べたが、特徴関数から得られた特徴ベクトルを“意味”と見做すことによっても TopSim を定義できる。
モデルエビデンス訓練データに対する対数周辺尤度（モデルエビデンス）は、学習パラメタ 𝑾outを周辺化することによって超パラメタの影響を明らかにするのに役立つ。
9） 人工データセットの作成方法については付録 A を参照。
10） また、議論を補強するため、教師データが全く構成的でない場合(𝜌data= 0)についても実験を行った（付録 B 参照)。
汎化性能 GP 回帰モデルによる、評価用データに対する対数尤度、即ち汎化性能を測定する。
MLE 𝜆mle≈ 0 となる領域がカオスの縁であり、カオスの縁は生物進化による超パラメタの収斂先として尤もらしい領域であると期待できる。
定義から明らかなように厳密な計算は困難であるため、“warm up” で得た 𝒉0を初期状態とし、𝑇 = 100 として 𝑇 ステップのランダム系列を入力して得た状態列𝒉0, . . . , 𝒉𝑇から推定した。



6.2 TopSim はカオスの縁で最も高くなる

𝜎rec, 𝜎inと各指標の関係を図 1 に示す。
カオスの縁（𝜆mle≈ 0）においてモデルの性能（モデルエビデンス・汎化性能）が最も高くなると同時に、TopSimも最も高くなることが分かる。
この結果は「言語の構成性の創発は学習のしやすさに起因する」という従来の EC の視点に対して「モデルの学習能力が最大となるカオスの縁において特徴ベクトルに基づくTopSim が最も高くなるため、モデルは TopSim の高い予測をしやすくなる」という新視点を与える。



7 今後の課題

本稿の内容は実験結果の報告に留まった。
今後、カオスの縁において TopSim が高くなる原理、特に𝝓(m)の距離構造について議論する必要がある。
ここで、∥𝝓(m)−𝝓(m′)∥∝p𝑘 (m,m)+𝑘 (m′,m′)−2𝑘 (m,m′)という関係に注目すると11）、カーネル関数の解析解を得て12）、それを上手く解釈する必要があることが伺える。
これに対しては、ESN の構造の簡潔さに加え、出力層以外全て凍結していることから、Deep Neural Network as Gaussian Process（DNN-GP）[23, 26, 27]の方法論が適用できると考えられるため、今後検証する予定である。
11） ただし 𝜎out= 0 の場合に限る。
12） つまり、初期化に起因するランダム性を極限 𝑁rec→ ∞で “周辺化” する。



謝辞

本研究は JSPS 科研費 JP23KJ0768，JST ACT-X（JPMJAX24C5）の助成を受けたものです。

参考文献


[1] 上田亮, 谷口忠大, 鈴木麗璽, 江原広人, 中村友昭, 岩村入吹, 橋本敬. 言語とコミュニケーションの創発に関する構成論的研究の展開. 認知科学, Vol. 31, No. 1,2024.
[2] Tadahiro Taniguchi, Yuto Yoshida, Yuta Matsui,Nguyen Le Hoang, Akira Taniguchi, and YoshinobuHagiwara. Emergent communication through metropolis-hastings naming game with deep generative models. Adv.Robotics, Vol. 37, No. 19, 2023.
[3] Ryo Ueda and Tadahiro Taniguchi. Lewis’s signaling gameas beta-vae for natural word lengths and segments. InThe Twelfth International Conference on LearningRepresentations, ICLR 2024, Vienna, Austria, May7-11, 2024. OpenReview.net, 2024.
[4] Tadahiro Taniguchi, Ryo Ueda, Tomoaki Nakamura,Masahiro Suzuki, and Akira Taniguchi. Generative emer-gent communication: Large language model is a collectiveworld model, 2024.
[5] Tadahiro Taniguchi. Collective predictive coding hypoth-esis: symbol emergence as decentralized bayesian infer-ence. Frontiers in Robotics and AI, Vol. 11, , 2024.
[6] Simon Kirby. Spontaneous evolution of linguisticstructure-an iterated learning model of the emergence ofregularity and irregularity. IEEE Transactions on Evo-lutionary Computation, Vol. 5, No. 2, 2001.
[7] Simon Kirby, Monica Tamariz, Hannah Cornish, andKenny Smith. Compression and communication in thecultural evolution of linguistic structure. Cognition, Vol.141, , 2015.
[8] Simon Kirby, Hannah Cornish, and Kenny Smith. Cumula-tive cultural evolution in the laboratory: An experimentalapproach to the origins of structure in human language.Proceedings of the National Academy of Sciences,Vol. 105, No. 31, 2008.
[9] Yi Ren, Shangmin Guo, Matthieu Labeau, Shay B. Co-hen, and Simon Kirby. Compositional languages emergein a neural iterated learning model. In 8th Inter-national Conference on Learning Representations,ICLR 2020, Addis Ababa, Ethiopia, April 26-30,2020. OpenReview.net, 2020.
[10] Angeliki Lazaridou and Marco Baroni. Emergent multi-agent communication in the deep learning era, 2020.
[11] Herbert Jaeger. The “echo state” approach to analysingand training recurrent neural networks. Technical report,German National Research Center for Information Tech-nology GMD Technical Report 148, 2001. Erratum noteavailable at https://www.ai.rug.nl/minds/uploads/EchoStatesTechRepErratum.pdf.
[12] Mantas Lukoˇseviˇcius and Herbert Jaeger. Reservoir com-puting approaches to recurrent neural network training.Computer Science Review, Vol. 3, No. 3, 2009.
[13] Mantas Lukoˇseviˇcius. A Practical Guide to Apply-ing Echo State Networks. Springer Berlin Heidelberg,Berlin, Heidelberg, 2012.
[14] Gouhei Tanaka, Toshiyuki Yamane, Jean Benoit H´eroux,Ryosho Nakane, Naoki Kanazawa, Seiji Takeda, HidetoshiNumata, Daiju Nakano, and Akira Hirose. Recent ad-vances in physical reservoir computing: A review. NeuralNetworks, Vol. 115, , 2019.
[15] Guillermo B. Morales and Miguel A. Mu˜noz. Optimalinput representation in neural systems at the edge of chaos.Biology, Vol. 10, No. 8, 2021.
[16] S.A. Kauﬀman. At Home in the Universe: TheSearch for Laws of Self-organization and Complex-ity. Oxford paperbacks. Oxford University Press, 1995.
[17] Henry Brighton and Simon Kirby. Understanding linguis-tic evolution by visualizing the emergence of topographicmappings. Artif. Life, Vol. 12, No. 2, 2006.
[18] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Comput., Vol. 9, No. 8, 1997.
[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser,and Illia Polosukhin. Attention is all you need. In I. Guyon,U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-wanathan, and R. Garnett, editors, Advances in NeuralInformation Processing Systems, Vol. 30. Curran As-sociates, Inc., 2017.
[20] Carl Edward Rasmussen and Christopher K. I. Williams.Gaussian Processes for Machine Learning. The MITPress, 2006.
[21] 持橋大地, 大羽成征. ガウス過程と機械学習. 機械学習プロフェッショナルシリーズ. 講談社サイエンティフィク, 東京, 2019.
[22] Sotirios P. Chatzis and Yiannis Demiris. Echo state gaus-sian process. IEEE Trans. Neural Networks, Vol. 22,No. 9, 2011.
[23] Michiel Hermans and Benjamin Schrauwen. Recurrentkernel machines: Computing with inﬁnite echo state net-works. Neural Comput., Vol. 24, No. 1, 2012.
[24] Herbert Jaeger, Mantas Lukoˇseviˇcius, Dan Popovici, andUdo Siewert. Optimization and applications of echo statenetworks with leaky-integrator neurons. Neural Net-works, Vol. 20, No. 3, 2007. Echo State Networks andLiquid State Machines.
[25] Holger Kantz and Thomas Schreiber. Nonlinear TimeSeries Analysis. Cambridge University Press, 2 edition,2003.
[26] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S.Schoenholz, Jeﬀrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes.In 6th International Conference on Learning Rep-resentations, ICLR 2018, Vancouver, BC, Canada,April 30 - May 3, 2018, Conference Track Proceed-ings. OpenReview.net, 2018.
[27] Greg Yang. Wide feedforward or recurrent neural networksof any architecture are gaussian processes. In Advances inNeural Information Processing Systems 32: AnnualConference on Neural Information Processing Sys-tems 2019, NeurIPS 2019, December 8-14, 2019,Vancouver, BC, Canada, 2019.




A 人工データセットの構築方法

本節では、本編にて言及した人工データセットの作成方法、即ち、人工メッセージデータ 𝕄dataと意味データ 𝕄dataについて説明する。
データセット全体のサイズを 𝑁dataとし、所与の TopSim を 𝜌dataとする。
まず、メッセージ空間 𝕄 上の一様 uni-gram モデル（終端記号 1 を生成するまで、あるいは最大長 𝐿 の 1 つ手前に達するまで記号を一様分布から生成する）から 𝑁data個になるまでメッセージを繰り返しサンプリングする。
このとき、全てのメッセージが互いに異なるようにする（もし同一のメッセージをサンプリングしてしまった場合は棄却し、サンプリングし直す）。
さもなくば、グラム行列 𝑲 が逆行列を持たなくなってしまうからである。
このようにして得られたメッセージデータを 𝕄data: = { m𝑖}𝑁data𝑖=1とする。
次に、一時的な意味データ ℤ′: = { z′𝑖∈ ℝ𝑁out}𝑁data𝑖=1を次のように初期化する：z′𝑖i.i.d∼N(0, 𝑰). (11)このとき、以下の目的関数 𝐽 を考える：𝐽 : = (𝜌data− 𝜌(𝕏))2. (12)ここで、𝕏 は𝕏 : =n𝑑𝕄(m𝑖, m𝑗), 𝑑ℤ(z′′𝑖, z′′𝑗)| 1 ≤ 𝑖 < 𝑗 ≤ 𝑁datao(13)とする。
ただし、各 𝑖 について z′′𝑖: = (z′𝑖− 𝔼𝑘[z′𝑘])/pVar𝑘[z′𝑘]とする（途中で意味データの中心やスケールが変化してしまうのを防ぐため）。
この目的関数 𝐽 を ℤ′に関して勾配法で最小化する。
𝐽 の収束後、意味データ ℤdataを（z′𝑖ではなく）z′′𝑖の集合として得る。

B 教師データが全く構成的でない場合 (𝜌

data

= 0) の結果

図 2 𝜌data= 0 とした場合の、超パラメタ 𝜎rec（横軸）及び 𝜎in（縦軸）と指標の関係。
𝜌data= 0 とした場合の結果を図 2 に示す。
本編での結果とは異なり、モデルの性能（モデルエビデンス・汎化性能）はカオスの縁ではなく秩序相（𝜆mle< 0）において最も高くなっている。
これは、メッセージと意味の間の関係が全くの出鱈目（ランダム）であるために、どんな入力に対しても正規分布に近い予測分布を返すのが有利になるからであると考えられる。
実際、𝜎rec, 𝜎inが小さくなるほど 𝜎outの影響が支配的になり、𝑘 (m, m′) ≈ 𝜎2out𝛿(m, m′)となることからこのことが伺えよう。
だが、このような設定においてもやはりカオスの縁（𝜆mle≈ 0）において予測に基づく TopSim が最も高くなる傾向にあることが分かる。
このことから、エージェントが初めからカオスの縁に位置していさえいれば、たとえ “原始の言語” が全く構成的でなかったとしても、世代を経るごとに言語の構成性が高くなるような圧力がかかることが示唆される。