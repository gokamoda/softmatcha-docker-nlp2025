SvMoE: MoE ルータの教師あり学習

村田栄樹 河原大輔



早稲田大学理工学術院



{eiki.1650-2951@toki., dkw@}waseda.jp



概要

Mixture-of-Experts はそのパラメタ数に対して計算コストが小さく、 大規模言語モデルの実用に向けて重要な技術である。
しかし、 エキスパートを選択するルーティングでは選択が偏り、 効率的なパラメタの利用が難しいという問題がある。
それに対して、 エキスパート選択を均一にする追加損失が使われるが言語モデル性能に干渉することがわかっている。
本研究では、 TF-IDF を教師信号とした教師あり学習でMixture-of-Experts のルーティングを訓練することを提案する。
ケーススタディとして法律へのドメイン特化を扱い、 追加損失なしの提案手法は追加損失を使用するベースラインに比肩する結果を得た。


1 はじめに

Mixture-of-Experts (MoE)は、 エキスパートと呼ばれる複数のモジュールの出力をアンサンブルする技術である[1, 2]. アンサンブルには、 ルータと呼ばれるモジュールの出力を重みとして使用する。
深層学習[3]や言語モデリング[4]にも適用され、 特にエキスパートの選択を疎にしたもの[4]はそのパラメタ数に対して計算コストが小さいという利点がある。
しかし、 MoE のルータによるエキスパートの選択ではその分布が偏る問題がある。
近年の言語モデルにおける研究ではこれを避けるために、 ルーティングを均一に近づけるための Load-Balancing 損失(LB損失)を言語モデル損失に加えて訓練される[4, 5, 6].一方で、 LB 損失は言語モデルの性能に対して干渉するという問題もある[7].この問題を踏まえて本研究では、 MoE 全体の訓練の前にルータを教師あり学習で訓練する、 SvMoE(Supervised Mixture-of-Experts)を提案する。
各エキスパートの担当するドキュメントを事前に定義し、 正規化された TF-IDF を教師信号として使用することでエキスパートの学習した知識とルーティングを対応させる。
ここでは訓練データの TF-IDF がそのトークンが得意なエキスパートに対応することを仮定している。
つまり、 (1)定義されたドキュメントでエキスパートをそれぞれ訓練し(2)それらをマージしてMoE モデルとして、 (3) TF-IDF でルータを訓練したのちに(4) MoE 全体を訓練することにより LB 損失を使用せずに適切な学習を実現する。
ケーススタディとして日本の法律へのドメイン特化を扱う。
提案手法による日本の法律文を使用した継続事前学習を事前学習済みモデルに対して行い、司法試験のベンチマーク[8]で評価する。
提案手法により訓練されたルータでは TF-IDF に対応した選択が確認され、 LB 損失を用いない提案手法は LB 損失を使用したベースラインとほぼ同等の結果を得た。


2 関連研究



2.1 Mixture-of-Experts

MoE は Transformer ベースの言語モデルにも多く活用されており[5, 6, 9, 10, 11, 12, 13, 14], そのほとんどは Transformer ブロックの MLP 層を並列に並べた形式で言語モデリングを目的として訓練される。
MoE はパラメタ数に対して効率的であるが、 訓練時に特定のエキスパートのみが多く活性化されることが知られている[4]. これはエキスパートの専門性を損ない、 パラメタの効率的な利用を妨げるため、 均一なルーティングを促す LB 損失が採用される[4, 5, 6]. LB 損失は訓練時に通常の言語モデル損失に加えられる。
しかし、 LB 損失は言語モデルの性能に悪い影響を及ぼすことを報告した研究[7]もある。
この研究では、 言語モデル性能と LB 損失のトレードオフがあるためハイパーパラメタの調整が必要であることを指摘し、 LB 損失を使用せずにルーティングを改善している。
具体的には、 動的に学習されるバイアス項をルータの出力に加えることで言語モデル損失の勾配に干渉しない Load-Balancing を提案し、 後続の研究でも利用されている[14]. 本研究もこの立場から、LB 損失を使用せずに MoE モデルの性能を引き出すことを目指す。

2.2 ドメイン特化モデル

事前学習済み Transformer モデルの登場以降、 医療や金融などのドメインに特化したモデルが開発されている[15, 16, 17, 18]. これらは、 ドメイン特化のための事前学習や通常の事前学習済みモデルを継続事前学習することで構築される。
Branch-Train-Merge (BTM)[19]は、 個別に訓練された複数のドメイン特化モデルをマージすることにより、 性能を改善している。
ただし、 MLP 層を並列化するのではなく Transformer モデル全体を並列化し、 トークンロジットの段階でのマージとなっている。
また、 C-BTM [20]ではクラスタリングされた学習コーパスで BTM の手法を用いることで、 大規模なデータセットでの並列化された訓練を実現した。
Branch-Train-MiX (BTX)[21]では、 BTM と同様にドメインで分割されたデータを使用して事前学習済みの Transfor mer をそれぞれ ﬁne-tuning したのちに、 それらの MLP 層を並列に統合することで MoE モデルとする。
ルーティングのための MoE モデル全体の訓練では、 LB 損失が使用される。
本研究では、 BTX をベースラインとして LB 損失を使用しないルーティングの学習を試みる。

3 Supervised MoE

本研究では LB 損失の悪影響を考慮し、 事前にルータをドキュメントの TF-IDF によって教師あり学習する SvMoE を提案する。
特に、 ドメインとしてさらにサブドメインに分割できるようなものを想定し、エキスパートは各サブドメインに特化する設定とする。
SvMoE は、 エキスパートの訓練、 エキスパートのマージ、 ルータの教師あり学習、 MoE モデル全体のﬁne-tuning の 4 段階からなる。



3.1 ルータの教師データ作成

SvMoE のエキスパートは分割されたサブドメインに対してそれぞれ特化するように訓練される。
ここでは、 エキスパートの訓練されたデータに合わせてルーティングするための教師データを作成する。
まず、 𝑁 個のサブドメインを定義する。
あるドメインのデータD0に対して、 𝑀 個のカテゴリ(𝑀 > 𝑁)が定義されていることを前提とする。
カテゴリをそれぞれドキュメントとして TF-IDF を計算する。
この TF-IDF を特徴量として、クラスタリングを実行することで 𝑁 個のサブドメインデータ𝑑𝑛1 ≤ 𝑛 ≤ 𝑁, 𝑑𝑖∩ 𝑑𝑗= ∅ ( ∀𝑖 ≠ 𝑗)を得る。
そして、D=𝑁𝑛=1𝑑𝑛を学習対象のデータセットとする。
次に、 ルータ訓練時の教師信号となる TF-IDF を取得する。
サブドメインデータをそれぞれドキュメントとして TF-IDF を計算することで、 𝑁 × |𝑉 | の行列を得る(𝑉 は語彙). 以降、各トークン 𝑡 ∈ 𝑉 に対応する正規化された TF-IDF ベクトルをTFIDF𝑡=tﬁdf𝑡,1, tﬁdf𝑡,2, . . . , tﬁdf𝑡, 𝑁∈ ℝ𝑁と表す。
ただし、𝑁𝑛=1tﬁdf𝑡,𝑛= 1 (∀𝑡 ∈ 𝑉)である。



3.2 モデルの構築

エキスパートの訓練 3.1 節で用意したDで 𝑁 個のエキスパート {𝐸1, . . . , 𝐸𝑁} を訓練する。
1 つの事前学習済み Transformer モデルを言語モデリングを目的としてそれぞれ訓練することで、 各サブドメインに特化したエキスパート群を得る。
エキスパートのマージ得られたエキスパート群をマージして MoE モデルとする。
BTX に倣い、各 Transformer ブロックについてエキスパート群のFFN 層を並列に並べることでエキスパート数が 𝑁の MoE 層とする。
Attention 層などの他の層については平均処理によってマージする。
ただし、 MoE 層のルータについてはランダムに初期化する。
ルータの教師あり学習マージされた MoE モデルを 3.1 節で得た TF-IDF 信号で訓練する。
モデルのブロック数を 𝐿 とすると、 各ブロックの MoE 層がルータを持つため、 𝐿 個のルータが存在する。
訓練バッチ 𝑏 = 𝑡1𝑡2. . . 𝑡|𝑏|(𝑡𝑖∈ 𝑉)を入力したときの目的損失LSvMoEを以下で定める。
LSvMoE=𝑡 ∈𝑏𝐿𝑙=1lCE(softmax(𝑅𝐿𝑙), TFIDF𝑡)ただし、 𝑅𝐿𝑙∈ ℝ𝑁は 𝑙 ブロック目のルータのロジット、 softmax(·)はソフトマックス関数、lCE(·, ·)はクロスエントロピー損失である。
LSvMoEによってルータのパラメタを訓練することにより、 訓練されたデータに多く含まれたトークンをそのエキスパートに割り当てることを促す。
ただし、 ルータを除くモデルのパラメタはこの段階では更新しない。
MoE モデルの ﬁne-tuning ルータを学習したMoE モデル全体を、 言語モデリングを目的として再び学習する。
このとき、 LB 損失は用いない。
この段階を持って、 提案手法の最終的なモデルとする。
表 1: サブドメインごとのトークン数。
単位はすべて百万トークン。
ID 訓練検証テスト合計1 23.0 3.2 1.9 28.12 18.0 2.0 2.6 22.63 24.0 3.5 2.4 29.94 32.1 3.8 4.7 40.55 31.3 4.3 4.1 39.76 73.4 5.9 8.1 87.47 21.0 2.9 2.5 26.48 39.5 2.0 5.3 46.9

4 モデル構築実験



4.1 設定

データ処理と訓練の設定、 および評価方法について述べる。
𝑁 = 8 として、 モデルとトークナイザはすべて llm-jp/llm-jp-3-1.8b を用いる。
このトークナイザは TF-IDF の計算にも使用する。
データセット日本の法律条文データを対象に実験する。
e-Gov から取得した条文に対して法令分類をカテゴリ(𝑀 = 50)としたものをD0とする。
トークナイザを使用して TF-IDF を計算し、 要素数が等しくなるようなスペクトラルクラスタリング1）によって 8 つのサブドメインに分割されたものをDとする。
テスト分割を 8:1:1 で行う。
表 1 にサブドメインごとのトークン数を示す。
モデルまず、 得られた 8 つの法律サブドメインそれぞれを使用して、 言語モデリングで llm-jp/llm-jp-3-1.8b を訓練し 8 つのエキスパートモデルを得る。
これら 8 つのエキスパートを 3.2 節の手続きに従って、 MoE モデルへのマージ、 ルータの訓練、 全体を ﬁne-tuning したものが提案手法のモデル(SvMoE)である。
比較として、 全体の ﬁne-tuning 時に LB 損失を加えたものも評価する。
また、 8 つのエキスパートをマージし、 全体をﬁne-tuning したモデル(BTX)をベースラインとして比較する。
BTX ベースラインについても、 全体のﬁne-tuning 時に LB 損失を加えたものと加えなかったものをそれぞれ評価する。
MoE アーキテクチャを持つ全てのモデルにおいて、 LB 損失の係数 𝛼 は BTX に則り、 𝛼 = 0.01 とする。
また、 選択されるエキスパート数は 2 で固定する。
さらに、 MoE ではない通常の Transformer モデルのベースライン(Dense)も用意する。
これは同じ1） https://github.com/anamabo/Equal-Size-Spectral-Clusteringllm-jp/llm-jp-3-1.8b を、 サブドメインごとではなくデータ全量を使用して訓練したものである。
評価Dに対して評価し、 ベースラインと提案手法を比較する。
ただし、 継続事前学習による知識の定着度合いを測ることが目的であるため、 訓練・検証・テストセットのそれぞれで評価する。
評価指標として、 以下の 3 つを用いる。
• PPL: 与えられたテキストに対する、 モデルのパープレキシティの平均。
• CMR𝑘(条件付き平均順位): ルータロジットの値による n 番目のエキスパートの順位 𝑟𝑅𝐿𝑛が 𝑘 であったときに、 TF-IDF による n 番目のエキスパートの順位 𝑟TFIDF𝑛の平均。
つまり、CMR𝑘= 𝔼[𝑟TFIDF𝑛| 𝑟𝑅𝐿𝑛= 𝑘]である。
この値が 𝑘に近いほど TF-IDF に従ってルーティングされているといえる。
𝑘 = 1, 2 について報告する。
• 𝑆𝑅𝐿: ルータロジットのエントロピーの平均。
この値が小さいほどルータの確信度が大きいと解釈する。

4.2 定量評価

表 2 にDのテストセットに対しての評価結果を示す。
残りのセットについては付録 A に結果を示す。
まず、 パープレキシティについては Dense が最も良く、 BTX と SvMoE がそれに続く結果となった。
BTX と SvMoE の両方で、 LB 損失を使わない設定でより良い結果を得た。
これは、 先行研究[7]で指摘されたように LB 損失が言語モデル性能に影響を与えた結果である。
条件付き平均順位 CMR𝑘については、 SvMoE がBTX より良い結果を示した。
これは、 TF-IDF を教師信号とした訓練により、 それに従ったルーティングが言語モデルの推論時にも可能だということを示している。
ルータロジットのエントロピー 𝑆𝑅𝐿も同様に、 SvMoE がより良い結果を示した。
ルータの訓練により、 対応するエキスパートを確実にアクティブにすることができていると考えられる。
まとめると、 提案手法は想定通り TF-IDF に従ったルーティングが可能だが、 それはパープレキシティの改善には繋がらなかったといえる。
MoE はしばしば統語情報によってエキスパートを選択する[13, 22]. 本研究ではより頻度情報を重視したことが、 パープレキティの改善に繋がらなった要因として考えられる。
表 2: 訓練に使用したデータのテストセットに対する評価結果。
最も良い値と次に良い値をそれぞれ太字と下線で示す。
提案手法は最下列である。
モデル LB 損失 PPL CMR1CMR2𝑆𝑅𝐿Dense - 1.156±0.462 - - -BTX✓ 1.187±0.407 4.063±2.237 4.742±2.206 1.805±0.198× 1.169±0.401
2.847±2.025 4.058±2.152 1.705±0.261SvMoE✓ 1.245±0.443 2.538±2.097 3.503±2.374 0.465±0.316× 1.199±0.434
2.533±2.092 3.518±2.365 0.466±0.319十二┊\n┊クロロ┊メチル┊メチル┊エー┊テル┊を┊含有┊する┊製剤┊その他┊の┊物┊。┊ただし、┊クロロ┊メチル┊メチル┊エー┊テル┊の┊含有量┊が┊重量┊の┊一┊パーセント┊以下┊の┊ものを┊除く┊。┊\n┊十三┊\n┊五┊酸化┊バナ┊ジ┊ウム┊を┊含有┊する┊製剤┊その他┊の┊物┊。┊ただし、┊五┊酸化┊バナ┊ジ┊ウム┊の┊含有量┊が┊重量┊の┊一┊パーセント┊以下┊の┊ものを┊除く┊。┊\n┊十三┊の┊二┊\n┊コ┊バルト┊又┊はその┊無┊機┊化合物┊を┊含有┊する┊製剤┊その他┊の┊物┊。┊ただし、┊コ┊バルト┊又┊はその┊無┊機┊化合物┊の┊含有量┊が┊重量┊の┊一┊パーセント┊以下┊の┊ものを┊除く┊。┊\n┊十四┊\n┊コール┊タール┊を┊含有┊する┊製剤┊その他┊の┊物┊。┊ただし、┊コール┊タール┊の┊含有量┊が┊重量┊の┊五┊パーセント┊以下┊の┊ものを┊除く┊。┊(a) SvMoE十二┊\n┊クロロ┊メチル┊メチル┊エー┊テル┊を┊含有┊する┊製剤┊その他┊の┊物┊。┊ただし、┊クロロ┊メチル┊メチル┊エー┊テル┊の┊含有量┊が┊重量┊の┊一┊パーセント┊以下┊の┊ものを┊除く┊。┊\n┊十三┊\n┊五┊酸化┊バナ┊ジ┊ウム┊を┊含有┊する┊製剤┊その他┊の┊物┊。┊ただし、┊五┊酸化┊バナ┊ジ┊ウム┊の┊含有量┊が┊重量┊の┊一┊パーセント┊以下┊の┊ものを┊除く┊。┊\n┊十三┊の┊二┊\n┊コ┊バルト┊又┊はその┊無┊機┊化合物┊を┊含有┊する┊製剤┊その他┊の┊物┊。┊ただし、┊コ┊バルト┊又┊はその┊無┊機┊化合物┊の┊含有量┊が┊重量┊の┊一┊パーセント┊以下┊の┊ものを┊除く┊。┊\n┊十四┊\n┊コール┊タール┊を┊含有┊する┊製剤┊その他┊の┊物┊。┊ただし、┊コール┊タール┊の┊含有量┊が┊重量┊の┊五┊パーセント┊以下┊の┊ものを┊除く┊。┊(b)
BTX十二┊\n┊クロロ┊メチル┊メチル┊エー┊テル┊を┊含有┊する┊製剤┊その他┊の┊物┊。┊ただし、┊クロロ┊メチル┊メチル┊エー┊テル┊の┊含有量┊が┊重量┊の┊一┊パーセント┊以下┊の┊ものを┊除く┊。┊\n┊十三┊\n┊五┊酸化┊バナ┊ジ┊ウム┊を┊含有┊する┊製剤┊その他┊の┊物┊。┊ただし、┊五┊酸化┊バナ┊ジ┊ウム┊の┊含有量┊が┊重量┊の┊一┊パーセント┊以下┊の┊ものを┊除く┊。┊\n┊十三┊の┊二┊\n┊コ┊バルト┊又┊はその┊無┊機┊化合物┊を┊含有┊する┊製剤┊その他┊の┊物┊。┊ただし、┊コ┊バルト┊又┊はその┊無┊機┊化合物┊の┊含有量┊が┊重量┊の┊一┊パーセント┊以下┊の┊ものを┊除く┊。┊\n┊十四┊\n┊コール┊タール┊を┊含有┊する┊製剤┊その他┊の┊物┊。┊ただし、┊コール┊タール┊の┊含有量┊が┊重量┊の┊五┊パーセント┊以下┊の┊ものを┊除く┊。┊(c) TF-IDF┊1┊2┊3┊4┊5┊6┊7┊8┊(d)配色の凡例図 1: エキスパート選択の比較。
図 1a,1b は 𝑙 = 15 を報告。
特定化学物質障害予防規則より抜粋。



4.3 定性評価

図 1 にエキスパート選択の例を示す。
入力した文章は 𝑑4のテストセットのものである。
SvMoE では入力トークンの TF-IDF が高くなっている𝐸5, 𝐸6が多く選択されている.一方でBTXでは、 やや 𝐸4が多いもののどのエキスパートも比較的均等に選択された。
例えば “クロロ” は 𝑑4, 𝑑5のTF-IDF が高く 𝑑1ではほとんど現れない。
SvMoE では “クロロ” とその次の単語を推論する際にそれぞれ 𝐸4, 𝐸6が使用されるが、 BTX では 𝐸3, 𝐸1が使用されることが観察された。
定性的にも、 SvMoE は BTXと比較して入力トークンの頻度情報を用いた選択がされることが確認された。


5 ダウンストリームタスクでの評価

4.2 節では SvMoE による改善が見られなかった。
そこで、 ダウンストリームタスクにおいて提案手法とベースラインを比較する。
先行研究[8]で構築された司法試験ベンチマークを対象とする。

5.1 設定

司法試験は正しいものの組み合わせを選ぶなど複雑な形式であるため、 緩和タスクとして設定された各文の正誤を問う正誤判定タスクを扱う。
正誤表 3: 司法試験正誤判定タスクの結果。
モデル LB 損失正解率(%)Dense - 48.33BTX✓ 51.67× 47.22SvMoE✓ 48.33× 51.11判定タスクは、 ある文に対してその内容があっていれば 1 をそうでなければ 0 を出力するものである。
Few-shot の設定で、 4 節で構築した 5 つのモデルを評価する。
ショットとして 2019 年度の問題をランダムに 5 つ与え、 2023 年度の 180 問を評価対象とする。

5.2 結果

表 3 に正誤判定タスクの結果を示す。
BTX ベースラインは、 LB 損失を使用した場合に最も高いスコアとなり、 使用しない設定では 4 ポイント以上スコアが下落した。
一方で SvMoE では、 LB 損失を使用しな場合でも高いスコアを示し、 LB 損失を使用した BTXベースラインとほぼ同等の結果となった。
LB 損失を使用する SvMoE では、 使用しないものと比較して 3ポイント程度のスコア下落となった。
4.2 節のパープレキシティでの評価で SvMoE はベースラインと比較して良い結果ではなかったが、ダウンストリームタスクではベースラインに比肩する性能を示した。
表 3 からも LB 損失がモデルの最終的な性能に小さくない影響を与えることを考慮すると、 提案手法はハイパーパラメタの設定も必要ないことなどから、 一定の有用性があると言える。

6 おわりに

本研究では LB 損失がモデルの性能に与える影響を考慮して、 LB 損失を使用せずに適切にルータを学習するフレームワーク SvMoE を提案した。
提案手法に基づいた MoE モデルを構築し、 TF-IDF に従ったルーティングが可能であることを確認した。
また、 ダウンストリームタスクでは LB 損失を使用するベースラインに比肩する結果を得た。



謝辞

本研究は「戦略的イノベーション創造プログラム(SIP)」「統合型ヘルスケアシステムの構築」JPJ012425 の補助を受けて実施した。

参考文献


[1] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Ge-oﬀrey E. Hinton. Adaptive mixtures of local experts. NeuralComputation, Vol. 3, No. 1, pp. 79–87, 1991.
[2] M.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts andthe em algorithm. In Proceedings of 1993 International Con-ference on Neural Networks (IJCNN-93-Nagoya, Japan),Vol. 2, pp. 1339–1344 vol.2, 1993.
[3] David Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learningfactored representations in a deep mixture of experts. arXiv, 2013.abs/1312.4314.
[4] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, AndyDavis, Quoc Le, Geoﬀrey Hinton, and Jeﬀ Dean. Outrageouslylarge neural networks: The sparsely-gated mixture-of-experts layer.In International Conference on Learning Representations,2017.
[5] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen,Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, andZhifeng Chen. {GS}hard: Scaling giant models with conditionalcomputation and automatic sharding. In International Confer-ence on Learning Representations, 2021.
[6] William Fedus, Barret Zoph, and Noam Shazeer. Switch transform-ers: Scaling to trillion parameter models with simple and eﬃcientsparsity. Journal of Machine Learning Research, Vol. 23, No.120, pp. 1–39, 2022.
[7] Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and DamaiDai. Auxiliary-loss-free load balancing strategy for mixture-of-experts, 2024. abs/2408.15664.
[8] チェジョンミン, 笠井淳吾, 坂口慶祐. 日本の司法試験を題材とした gpt モデルの評価. 言語処理学会第 30 回年次大会発表論文集, 2024.
[9] Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Car-los Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, MostafaDehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints. In The Eleventh Interna-tional Conference on Learning Representations, 2023.
[10] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, DmitryLepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams WeiYu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zong-wei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, MariePellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lu-cas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen,and Claire Cui. Glam: Eﬃcient scaling of language models withmixture-of-experts. arXiv, 2021. abs/2112.06905.
[11] NLLB Team, Marta R. Costa-juss`a, James Cross, Onur C¸ elebi,Maha Elbayad, Kenneth Heaﬁeld, Kevin Heﬀernan, ElaheKalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun,Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,John Hoﬀman, Semarley Jarrett, Kaushik Ram Sadagopan, DirkRowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip FazilAyan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao,Vedanuj Goswami, Francisco Guzm´an, Philipp Koehn, Alexan-dre Mourachko, Christophe Ropers, Saﬁyyah Saleem, HolgerSchwenk, and Jeﬀ Wang. No language left behind: Scaling human-centered machine translation. arXiv, 2022. abs/2207.04672.
[12] Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Ja-son Wei, Hyung Won Chung, Barret Zoph, William Fedus, XinyunChen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunx-uan Li, Vincent Y Zhao, Hongkun Yu, Kurt Keutzer, Trevor Dar-rell, and Denny Zhou. Mixture-of-experts meets instruction tun-ing: A winning combination for large language models. In TheTwelfth International Conference on Learning Representa-tions, 2024.
[13] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, ArthurMensch, Blanche Savary, Chris Bamford, Devendra Singh Chap-lot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,Gianna Lengyel, Guillaume Bour, Guillaume Lample, L´elio Re-nard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock,Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven LeScao, Th´eophile Gervet, Thibaut Lavril, Thomas Wang, Timoth´eeLacroix, and William El Sayed. Mixtral of experts. arXiv, 2024.abs/2401.04088.
[14] DeepSeek-AI. Deepseek-v3 technical report. arXiv, 2024.abs/2412.19437.
[15] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wul-czyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis,Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin,Sami Lachgar, Philip Mansﬁeld, Sushant Prakash, Bradley Green,Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, YunLiu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, JoelleBarral, Dale Webster, Greg S. Corrado, Yossi Matias, ShekoofehAzizi, Alan Karthikesalingam, and Vivek Natarajan. Towardsexpert-level medical question answering with large language mod-els. arXiv, 2023. abs/2305.09617.
[16] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, MarkDredze, Sebastian Gehrmann, Prabhanjan Kambadur, DavidRosenberg, and Gideon Mann. Bloomberggpt: A large languagemodel for ﬁnance. arXiv, 2023. abs/2303.17564.
[17] Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler, andMarkus Leippold. Climatebert: A pretrained language model forclimate-related text. arXiv, 2022. abs/2110.12010.
[18] Masanori Hirano and Kentaro Imajo. Construction of domain-speciﬁed japanese large language model for ﬁnance through con-tinual pre-training. arXiv, 2024. abs/2404.10555.
[19] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, TimAlthoﬀ, Noah A. Smith, and Luke Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language mod-els. In First Workshop on Interpolation Regularizers andBeyond at NeurIPS 2022, 2022.
[20] Suchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, TimAlthoﬀ, Noah A. Smith, and Luke Zettlemoyer. Scaling ex-pert language models with unsupervised domain discovery, 2023.abs/2303.14177.
[21] Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu,Xi Victoria Lin, Baptiste Roziere, Jacob Kahn, Shang-Wen Li,Wen tau Yih, Jason E Weston, and Xian Li. Branch-train-mix:Mixing expert LLMs into a mixture-of-experts LLM. In FirstConference on Language Modeling, 2024.
[22] Dongyang Fan, Bettina Messmer, and Martin Jaggi. TO-WARDS AN EMPIRICAL UNDERSTANDING OF MOE DE-SIGN CHOICES. In ICLR 2024 Workshop on Mathematicaland Empirical Understanding of Foundation Models, 2024.




A モデル構築の詳細

00.10.20.30.40.50.60.70.80 2000 4000 6000 8000 10000 12000 14000損失ステップBTX w/ LB損失BTX w/o LB損失SvMoE w/ LB損失SvMoE w/o LB損失図 2: 提案手法とベースラインの損失曲線。
クラスタリングの詳細本文でも触れたように、 e-Gov2）から取得したカテゴリを TF-IDF を特徴としてクラスタリングする。
ただしデータ量に偏りがあるため、 ‘国税’, ‘金融・保険’ および ‘地方財政’ については 1 カテゴリを 1 サブドメインとして固定する。
つまり、 47 カテゴリについて 5 クラスタを作るようにクラスリングする。
クラスタリングの結果を表 4 に示す。
モデル訓練と評価の詳細モデル訓練時の損失曲線は図 2 のようになった。
訓練の最初期には、 ルータを訓練済みの SvMoE がランダムに初期化する BTX より低い損失を示した。
しかし、 最終的な損失は LB 損失を使用しない BTX が最も良い結果を示した。
ただし5 節で示したように損失はモデルの性能に必ずしも直結しない。
次に表 5 に訓練および検証セットでのモデルの評価を示す。
テストセットについて表 2 で示した結果と同様の評価である。
全体としてテストセットでの結果と同様の傾向となったが、 訓練セットで他のセットと比較して良い結果を示した。
また、 テストセットでのエキスパート選択の頻度をカウントした結果を図 3 に示す。
表 1 に示すように入力データ数に偏りがあるため、 SvMoE はそれに対応したエキスパートが多く選択されている。
一方で BTX では、 特に LB 損失を使用した場合に入力データの分布によらず均等に選択されていることがわかる。
また、 入力層や出力層の近辺ではどの場合においても偏りが出ることがわかる。
表 4: 各サブドメインに割り当てられた法令分類のカテゴリ。
ID カテゴリ名1 ‘刑事’, ‘国会’, ‘鉱業’, ‘事業’, ‘商業’, ‘土地’, ‘文化’, ‘司法’, ‘民事’, ‘外事’2 ‘水産業’, ‘国債’, ‘地方自治’, ‘産業通則’, ‘災害対策’3 ‘憲法’, ‘財務通則’, ‘郵務’, ‘行政手続’, ‘都市計画’, ‘道路’, ‘社会福祉’, ‘社会保険’, ‘林業’, ‘貨物運送’4 ‘国有財産’, ‘行政組織’, ‘国家公務員’, ‘国土開発’, ‘労働’, ‘統計’, ‘教育’, ‘海運’, ‘農業’, ‘防衛’5 ‘観光’, ‘警察’, ‘消防’, ‘工業’, ‘電気通信’, ‘環境保全’, ‘外国為替・貿易’, ‘厚生’, ‘陸運’, ‘河川’, ‘航空’, ‘建築・住宅’6 ‘国税’7 ‘金融・保険’8 ‘地方財政’表 5: 訓練に使用したデータに対する評価結果。
各セット内で最も良い値と次に良い値をそれぞれ太字と下線で示す。
データモデル LB 損失 PPL CMR1CMR2𝑆𝑅𝐿訓練Dense - 1.042±0.087 - - -BTX✓ 1.089±0.159 4.043±2.254 4.755±2.205 1.802±0.201× 1.069±0.145 2.802±2.035 4.087±2.150 1.691±0.269SvMoE✓ 1.141±0.196 2.473±2.078 3.543±2.369 0.449±0.316× 1.099±0.202 2.477±2.079 3.563±2.360 0.450±0.318検証Dense - 1.154±0.331 - - -BTX✓ 1.188±0.320 4.145±2.214 4.749±2.201 1.803±0.199× 1.169±0.316 3.057±2.086 4.086±2.152 1.716±0.256SvMoE✓ 1.243±0.348 2.687±2.164 3.558±2.397 0.477±0.327× 1.201±0.340 2.681±2.161 3.569±2.385 0.476±0.3291 2 3 4 5 6 7 8Expert ID242322212019181716151413121110987654321Layer ID2.56e+04 3.04e+07 3.65e+05 7.00e+05 1.88e+04 9.87e+03 9.30e+04 3.16e+072.79e+06 1.47e+06 1.78e+06 1.02e+07 1.09e+07 2.30e+07 5.76e+06 7.37e+066.97e+06 2.04e+06 8.10e+06 8.66e+06 7.79e+06 1.62e+07 3.58e+06 9.99e+065.39e+06 1.78e+06 6.94e+06 9.30e+06 1.01e+07 1.22e+07 6.37e+06 1.12e+077.58e+06 3.94e+06 6.84e+06 1.25e+07 3.73e+06 1.67e+07 3.68e+06 8.34e+066.96e+06 2.58e+06 4.12e+06 8.39e+06 9.01e+06 1.47e+07
6.33e+06 1.12e+078.05e+06 2.17e+06 7.73e+06 7.93e+06 1.21e+07 1.47e+07 1.72e+06 8.96e+066.50e+06 3.23e+06 6.25e+06 1.08e+07 8.95e+06 1.29e+07 4.63e+06 1.00e+075.69e+06 1.89e+06 4.59e+06 8.98e+06 1.08e+07 1.37e+07 7.67e+06 1.00e+075.98e+06 3.18e+06 7.54e+06 6.99e+06 1.07e+07 1.65e+07 4.36e+06 8.06e+065.77e+06 3.28e+06 1.11e+07 1.04e+07 5.66e+05 1.82e+07 4.51e+06 9.44e+066.51e+06 2.31e+06 5.48e+06 5.17e+06 1.35e+07 1.73e+07 3.05e+06 9.92e+065.11e+06 4.64e+06 7.84e+06 7.41e+06 9.32e+06 1.59e+07 3.73e+06 9.37e+068.36e+06 1.96e+06 4.26e+06 7.82e+06 9.95e+06 1.52e+07 6.99e+06 8.78e+067.90e+06 2.03e+06 4.10e+06 9.43e+06 9.05e+06 2.34e+07 1.08e+06 6.33e+069.11e+06 4.04e+05 1.22e+07 2.16e+06 6.73e+06 2.19e+07 2.12e+06 8.66e+063.97e+06 4.70e+06 5.39e+06 8.27e+06 1.05e+07 2.01e+07 1.45e+06 8.86e+061.39e+07 3.62e+05 1.78e+06 9.40e+06 8.82e+06 1.37e+07 5.48e+06 9.86e+061.16e+06 1.82e+06 1.16e+07 7.57e+06 7.73e+06 1.36e+07 8.47e+06 1.13e+071.10e+06 7.39e+05 9.79e+06 1.55e+06
1.96e+07 2.69e+07 2.77e+05 3.31e+061.35e+06 2.81e+05 1.66e+06 1.25e+07 1.09e+06 1.58e+07 1.03e+06 2.96e+079.63e+06 7.50e+05 5.84e+05 1.03e+07 9.75e+06 3.07e+07 4.85e+05 1.18e+064.18e+06 2.63e+05 3.03e+06 4.47e+06 1.96e+07 3.14e+07 1.84e+05 1.32e+055.06e+04 1.95e+04 2.50e+04 3.30e+04 3.14e+07 3.16e+07 2.13e+04 1.25e+05012345678log10(count)(a) BTX (LB 損失なし)1 2 3 4 5 6 7 8Expert ID242322212019181716151413121110987654321Layer ID6.04e+04 6.55e+06 1.21e+05 7.17e+04 2.49e+07 1.91e+03 1.25e+04 3.16e+073.92e+06 2.03e+06 9.94e+06 8.79e+06 9.36e+06 5.56e+06 1.67e+07 7.02e+066.05e+06 1.60e+07 5.01e+06 1.70e+07 4.00e+06 6.55e+06 5.33e+06 3.33e+068.60e+06 1.05e+07 7.16e+06 8.24e+06 6.80e+06 6.11e+06 8.40e+06 7.50e+067.46e+06 1.22e+07 7.68e+06 8.64e+06 6.40e+06 6.66e+06 8.74e+06 5.55e+067.19e+06 9.42e+06 7.92e+06 1.08e+07 5.43e+06 7.34e+06 1.00e+07 5.19e+067.25e+06 8.31e+06 1.43e+07 6.23e+06 3.44e+06 7.47e+06 8.76e+06 7.55e+068.78e+06 1.01e+07 6.94e+06 1.04e+07
7.62e+06 6.70e+06 6.90e+06 5.90e+066.99e+06 8.42e+06 5.63e+06 1.21e+07 6.66e+06 6.82e+06 8.17e+06 8.54e+067.91e+06 8.95e+06 7.89e+06 6.92e+06 8.80e+06 7.96e+06 7.91e+06 6.95e+065.51e+06 5.41e+06 6.34e+06 9.42e+06 8.02e+06 1.03e+07 9.21e+06 9.09e+061.06e+07 5.11e+06 3.61e+06 1.25e+07 9.69e+06 9.58e+06 6.21e+06 6.05e+066.18e+06 8.60e+06 8.27e+06 6.90e+06 1.09e+07 8.21e+06 8.76e+06 5.51e+068.61e+06 9.12e+06 5.24e+06 6.55e+06 9.94e+06 9.04e+06 7.93e+06 6.87e+066.84e+06 8.23e+06 9.49e+06 1.32e+07 7.99e+06 9.78e+06 3.60e+06 4.19e+066.48e+06 2.59e+06 1.36e+07 1.62e+06 1.40e+07 7.11e+06 8.65e+06 9.26e+065.58e+06 3.88e+06 5.94e+06 8.18e+06 8.19e+06 8.79e+06 1.37e+07 8.98e+061.23e+07 1.16e+07 9.56e+06 2.75e+06 6.43e+06 5.61e+06 4.52e+06 1.05e+078.42e+06 6.11e+06 3.51e+06 1.07e+07 1.05e+07 5.84e+06 9.32e+06 8.94e+065.85e+06 1.18e+07 3.34e+06 2.07e+06 4.58e+06 1.32e+07 8.21e+06 1.42e+072.43e+06 3.54e+06 3.43e+06 3.25e+06 2.76e+07 1.70e+07 2.71e+06 3.33e+068.53e+06 7.13e+06
4.80e+06 1.96e+06 3.26e+06 2.53e+07 4.76e+06 7.58e+068.33e+05 9.13e+06 3.05e+06 3.16e+07 1.51e+05 1.91e+04 1.67e+07 1.76e+063.10e+07 2.31e+05 3.16e+07 5.39e+02 5.00e+02 2.66e+02 3.02e+05 1.25e+05012345678log10(count)(b) BTX (LB 損失あり)1 2 3 4 5 6 7 8Expert ID242322212019181716151413121110987654321Layer ID4.62e+02 3.16e+07 1.71e+02 1.41e+04 5.79e+02 3.16e+07 1.37e+04 1.84e+022.00e+04 3.13e+07 1.95e+03 1.63e+05 1.32e+05 3.15e+07 2.42e+03 1.33e+051.30e+06 2.46e+07 2.16e+05 2.56e+06 2.85e+06 2.74e+07 1.17e+06 3.26e+062.87e+06 1.88e+07 1.41e+06 4.18e+06 5.23e+06 2.36e+07 2.57e+06 4.65e+062.43e+06 2.14e+07 7.76e+05 3.03e+06 3.71e+06 2.62e+07 1.32e+06 4.43e+062.48e+06 1.99e+07 1.42e+06 3.22e+06 3.51e+06 2.63e+07 1.56e+06 4.84e+062.99e+06 8.51e+06 1.66e+06 5.71e+06 4.87e+06 2.41e+07 2.92e+06 1.26e+072.54e+06 8.96e+06 4.39e+05 5.41e+06 4.04e+06 2.62e+07 1.74e+06 1.40e+073.07e+06 6.48e+06 1.42e+06 6.60e+06 7.05e+06 2.28e+07 2.64e+06 1.32e+072.31e+06 6.73e+06
5.05e+05 4.71e+06 5.39e+06 2.67e+07 1.39e+06 1.56e+072.01e+06 4.51e+06 4.37e+05 4.44e+06 4.92e+06 2.76e+07 1.64e+06 1.77e+071.34e+06 5.50e+06 8.24e+04 2.34e+06 3.02e+06 2.97e+07 3.50e+05 2.10e+071.35e+06 4.13e+06 1.13e+05 2.34e+06 3.02e+06 2.94e+07 8.13e+05 2.22e+071.42e+06 2.20e+06 1.55e+05 2.50e+06 3.52e+06 2.94e+07 4.16e+05 2.37e+073.64e+05 6.21e+05 9.86e+04 7.23e+05 1.16e+06 3.11e+07 1.77e+05 2.91e+079.35e+05 2.24e+05 2.07e+05 1.54e+06 2.27e+06 3.02e+07 2.76e+05 2.76e+079.35e+05 1.93e+05 3.93e+05 2.10e+06 3.07e+06 2.96e+07 5.30e+05 2.64e+072.45e+06 2.74e+05 2.59e+05 3.19e+06 3.87e+06 2.81e+07 6.53e+05 2.45e+072.36e+06 1.62e+05 5.10e+05 3.94e+06 4.40e+06 2.79e+07 8.42e+05 2.32e+079.39e+05 1.17e+05 2.74e+05 3.37e+06 3.32e+06 2.93e+07 5.83e+05 2.54e+071.01e+06 2.12e+05 5.14e+05 3.91e+06 2.87e+06 2.95e+07 1.56e+06 2.37e+077.80e+05 5.64e+05 1.12e+06 5.01e+06 4.42e+06 2.86e+07 1.74e+06 2.10e+071.03e+06 1.27e+06 6.45e+06 9.43e+06 3.95e+06 2.43e+07 4.81e+06
1.20e+071.98e+05 1.47e+05 6.06e+05 5.20e+06 2.89e+05 3.11e+07 1.16e+06 2.46e+07012345678log10(count)(c) SvMoE (LB 損失なし)1 2 3 4 5 6 7 8Expert ID242322212019181716151413121110987654321Layer ID3.52e+02 3.16e+07 1.24e+02 1.45e+04 1.41e+04 3.16e+07 4.65e+02 2.10e+021.51e+04 3.13e+07 1.89e+03 2.67e+05 1.28e+05 3.15e+07 2.43e+03 1.29e+051.40e+06 2.41e+07 6.87e+05 2.93e+06 2.93e+06 2.65e+07 1.29e+06 3.42e+062.89e+06 1.88e+07 1.72e+06 4.10e+06 4.98e+06 2.36e+07 2.32e+06 4.87e+062.07e+06 2.23e+07 6.46e+05 2.38e+06 2.91e+06 2.67e+07 2.08e+06 4.22e+062.14e+06 2.12e+07 1.05e+06 3.01e+06 3.07e+06 2.67e+07 1.31e+06 4.80e+063.14e+06 9.61e+06 1.43e+06 5.38e+06 4.96e+06 2.46e+07 2.19e+06 1.20e+072.77e+06 9.85e+06 3.58e+05 5.57e+06 4.63e+06 2.56e+07 1.89e+06 1.26e+073.19e+06 7.15e+06 2.51e+05 6.96e+06 6.39e+06 2.36e+07 2.52e+06 1.32e+072.47e+06 6.36e+06 2.46e+05 4.90e+06 5.68e+06 2.65e+07 1.24e+06 1.59e+071.88e+06 4.48e+06 1.45e+05 3.88e+06 4.12e+06 2.83e+07 1.40e+06
1.91e+071.71e+06 5.99e+06 1.27e+05 2.35e+06 2.73e+06 2.98e+07 3.31e+05 2.03e+071.45e+06 3.80e+06 1.06e+05 2.33e+06 3.45e+06 2.92e+07 4.78e+05 2.25e+071.22e+06 1.92e+06 1.19e+05 1.46e+06 2.79e+06 3.01e+07 1.98e+05 2.55e+074.98e+05 5.36e+05 1.45e+05 1.44e+06 1.36e+06 3.08e+07 1.01e+05 2.84e+077.57e+05 1.19e+05 1.59e+05 1.29e+06 2.41e+06 3.04e+07 2.50e+05 2.79e+078.81e+05 1.31e+05 1.20e+05 2.36e+06 2.78e+06 2.96e+07 8.67e+05 2.66e+071.56e+06 2.15e+05 3.96e+05 2.94e+06 3.95e+06 2.87e+07 5.29e+05 2.50e+071.52e+06 1.72e+05 7.54e+05 2.98e+06 4.95e+06 2.83e+07 1.08e+06 2.36e+071.08e+06 2.21e+05 5.36e+05 4.23e+06 3.25e+06 2.92e+07 7.45e+05 2.40e+071.26e+06 5.28e+05 6.11e+05 6.09e+06 4.20e+06 2.79e+07 1.59e+06 2.11e+072.00e+06 4.04e+05 1.13e+06 4.93e+06 3.66e+06 2.79e+07 1.16e+06 2.21e+071.43e+06 3.93e+05 1.21e+06 6.87e+06 2.57e+06 2.76e+07 1.77e+06 2.14e+073.42e+06 1.23e+05 1.24e+05 4.41e+06 1.30e+06 3.06e+07 2.90e+06 2.04e+07012345678log10(count)(d) SvMoE (LB 損失あり)図 3: 各モデルのレイヤごとのエキスパート選択の頻度。
2） https://laws.e-gov.go.jp/bulkdownload