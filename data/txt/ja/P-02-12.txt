部分空間の擬似直交性によるTransformer 言語モデルの内部表現の解釈

前田晃弘

1,4

 鳥居拓馬

2

 日髙 昇平

1

 井之上直也

1

 大関洋平

31

北陸先端科学技術大学院大学 

2

東京電機大学 

3

東京大学 

4

日本学術振興会特別研究員



akihiro.maeda@jaist.ac.jp



概要

本研究は、Transformer 言語モデルにおける内部表現を解釈するために、擬似直交性の概念を新たに導入してそのアテンション層および FFN 出力層の部分空間の幾何的関係を分析する。
FFN 層のウェイト行列の行空間が語彙空間と擬似直交していることを示した上で、FFN 層からの出力が意味的概念を担うコンセプトベクトルとして機能し、内部表現の文脈化に寄与している可能性を明らかにする。


1 はじめに



1.1 Mechanistic interpretability

Mechanistic interpretability（MI: 内部機序解釈可能性）[1]と呼ばれる一連の研究では、深層学習モデルの内部表現を解釈するため学習済みニューラルネットワークをリバースエンジニアリングする。
パラメータや内部表現を直接数理的に分析して、解釈可能な計算回路や特徴量を特定し、その機序の説明(mechanistic explanation)を与える。
MI 研究は大規模言語モデル(LLM)において特に活発である[2]。
LLM の内部機序には依然として多くの未解明な側面が存在する。
その解明は性能改善に加え、自然言語の構造や意味合成[3]に関する新たな知見をもたらすことが期待される。


1.2 Logit Lens: 単語分布への写像

MI 研究で注目される手法の一つとして、Logitlens [4]がある。
これは、LLM の内部表現を単語埋め込み行列を用いて単語分布へ写像する（“logit” を計算する）ことで、人が理解できる表現（単語）に対応づけ解釈する手法である。
例えば、[5]はGPT2の Feed Forward Network (FFN)層の出力ウェイトへLogit lens を適用すると意味的な概念を共有する単語群が現れることを実証的に示した。
一方で、Logit lens が有効でない場合も報告され、内部表現を解釈できるよう復号化するための改善方法が研究されている[6]. また、[7]は、FFN 層とは異なる変換を用いて Attention 層の作用を解釈している。
これらの観察事実は Transformer の各層が異なる性質の計算を行うことを示唆する。


1.3 部分空間の疑似直交性

単語分散表現に関する先行研究[8, 9]は、単語ベクトルの集合が代数系としてのベクトル空間をなしており、語義や特徴が線形部分空間に対応する構造にあることを示唆する（線形表現仮説[10])。
本研究では、Transformer の各層が異なる線形部分空間を構成している可能性に着眼し、その幾何的な関係を調べる。
本研究の新規な試みとして、ノイズを許容して直交性の定義を緩めた擬似直交という概念を適用し、次元数以上の擬似的な直交基底がTransformer 各層の部分空間をなすことを示す。
FFN層が概念を表現するベクトル（コンセプトベクトル）を事前学習しており、その出力が単語表現を文脈化して意味を再構成している可能性を指摘する。


2 Transformer 各層のなす部分空間



2.1 Transformer の概要 [11]

式（1–6）に概要を示す。
Transformer は、トークン長 𝑛 の入力文を埋め込み行列 𝐸 ∈ ℝ𝑉 ×𝑑により 𝑛個の 𝑑 次元ベクトルへ変換した上で（𝑉 は語彙サイズ）、各トークン位置を符号化したベクトルを加算し、縦結合して当初の内部状態 𝑋𝑙=0とする（式 1)。
𝑋0= 𝑋𝑒+ 𝑋𝑝∈ ℝ𝑛×𝑑(1)𝑌𝑙= 𝐴𝑙𝑋𝑙𝑊𝑙𝑣𝑊𝑙𝑜=: 𝐴𝑙𝑋𝑙𝑊𝑙𝑣𝑜∈ ℝ𝑛×𝑑(2)𝑀𝑙= LayerNorm𝑌𝑙+ 𝑋𝑙∈ ℝ𝑛×𝑑(3)𝑁𝑙= ReLU(𝑀𝑙𝑊𝑖𝑛) ∈ ℝ𝑛×4𝑑+(4)𝑍𝑙= 𝑁𝑙𝑊𝑜𝑢 𝑡∈ ℝ𝑛×𝑑(5)𝑋𝑙+1= LayerNorm𝑍𝑙+ 𝑀𝑙∈ ℝ𝑛×𝑑(6)Transformer の各層（𝑙 = 0, . . . , 𝐿−1）は、アテンション層（式 2）と FFN 層（式 4,5）と呼ばれる計算ユニットを交互に適用する。
𝑊𝑙𝑞, 𝑊𝑙𝑘, 𝑊𝑙𝑣, 𝑊𝑙𝑜, 𝑊𝑙𝑖𝑛, 𝑊𝑙𝑜𝑢𝑡は各層で学習されるパラメータであり、
𝐴𝑙はアテンションウェイトの結合を表す1）．式（3,6）の LayerNorm はレイヤー正規化[12]を行う。
ReLU(𝑥) = 𝑚𝑎𝑥 (𝑥, 0)。
最終層後の内部状態 𝑋𝐿は再変換のための行列（多くの場合、埋め込み行列の転置行列 𝐸⊤）により単語へ変換される。
レイヤー正規化を無視すると、𝑋𝐿= 𝑍𝐿− 1+ 𝑀𝐿− 1= 𝑍𝐿− 1+ (𝑌𝐿− 1+ 𝑋𝐿− 1)(7)= (𝑍𝐿− 1+ 𝑌𝐿− 1) + 𝑍𝐿− 2+ 𝑀𝐿− 2= · · · (8)=𝐿− 1𝑙=0(𝑌𝑙+ 𝑍𝑙) + 𝑋0(9)と式変形でき、当初の内部状態 𝑋0にアテンション層の出力 𝑌𝑙（Y ベクトルと呼ぶ）と FFN 層の出力 𝑍𝑙（Z ベクトルと呼ぶ）を足し込む計算フロー（Residual stream と呼ぶ[6]）と捉えられる。



2.2 各計算ユニットがなす部分空間

式（1–6）で示した計算フローを、BERT を例として、各層の次元数とともに図 1 に示す。
内部状態（行列）の各行ベクトルから同じ次元数の Y ベクトルと Z ベクトルへの変換は局所的な空間ごとに見ればアフィン自己準同型写像のように扱える。
図 1 BERT の計算フローY ベクトルは、式（2）よりアテンションウェイトで加重和された内部状態 𝐴𝑙𝑋𝑙を行列 𝑊𝑙𝑉𝑂により1） アテンションヘッドと呼ばれるサブユニットで以下の計算を行う。
𝑊𝑙𝑞, 𝑊𝑙𝑘を分割した 𝑊𝑙,ℎ𝑞, 𝑊𝑙,ℎ𝑘を用いて、まずヘッドごとのウェイト 𝐴𝑙,ℎ= Softmax𝑋𝑊𝑙,ℎ𝑞𝑊𝑙,ℎ𝑇𝑘𝑋𝑇∈ ℝ𝑛×𝑛を計算する。
ソフトマックスは行方向に総和が 1 となるよう適用、その上で次の結合を行う。
𝐴𝑙𝑋𝑙𝑊𝑙𝑣𝑊𝑙𝑜:=[ 𝐴𝑙,1, · · · , 𝐴𝑙,12]𝑋𝑙[𝑊𝑙,1𝑣,· · ·, 𝑊𝑙,12𝑣]𝑇𝑊𝑜線形写像した像と見れる（行ベクトルに右から行列をかける；バイアスを無視）。
この線型写像の像は𝑊𝑙𝑉𝑂の行空間であり、任意の内部状態を変換して得られる Y ベクトルは全てこの部分空間にある。
同様に、Z ベクトルは式（5）より行列 𝑊𝑙𝑜𝑢 𝑡により表現される線型写像の像である部分空間の中にある。
すなわち、Z ベクトルは 𝑊𝑙𝑜𝑢 𝑡の行ベクトルを𝑁𝑙の行ベクトルの成分により線形結合したものであり、𝑊𝑙𝑜𝑢 𝑡の行空間に住む。
𝑁𝑙は活性化関数の出力であり（式 4），𝑊𝑙𝑜𝑢𝑡の行ベクトルの活性・非活性を制御するのでニューロンと呼ばれる[6]。
BERT では各層のニューロンは 4𝑑 = 3072 個である。
ニューロンはスパースとなるが次節で述べる擬似直交と密接な関係がある[13]。
𝑋0（式 1）は、単語埋め込み行列𝐸と位置行列𝑃の行ベクトルの和であり、それぞれのベクトルが張る部分空間を語彙空間と POS 空間と呼ぶ。



2.3 擬似直交性

前節で見た各部分空間の間の幾何的関係を特徴づけるために擬似直交性を導入する（定義は次節)。
二つのベクトルが直交するとは、その内積（またはコサイン類似度）がゼロであることだが、ノイズを許容しコサイン類似度が 0 ± 𝜖 の範囲にあることを擬似直交と呼ぶこととする。
𝑑 次元のベクトル空間にある任意のベクトルは 𝑑 個の（直交）基底の線型結合に分解できるが、ノイズを許容すれば 𝑑 よりもはるかに大きい個数のベクトルを基底のように用いた擬似的な直交分解が可能となる[14] （付録 A)。
[13]は擬似直交な基底が解釈可能な特徴や意味を表現しており、深層学習モデルの内部表現は擬似的な直交基底の重ね合せであるとの立場を示している（superposition 仮説)。
[13]はトイモデルを用いて、学習データがスパースな場合、モデルが擬似直交を用いた表現を学習して、内部表現の次元数以上の特徴量を表現できることを示している。


3 分析：Transformer の部分空間



3.1 擬似直交性を用いた分析手法

分析の目的 Transformer の各層ウェイト行列の行空間の直交性の程度（擬似直交性）を評価する。
定義 𝑑 次元のベクトル空間 𝑉 の 2 つの部分空間𝑈, 𝑊 ⊆ 𝑉 が擬似直交するとは、正規化された任意のベクトル u ∈ 𝑈, w ∈ 𝑊 に対して、−𝜖 ≤ ⟨u, w⟩ ≤ 𝜖 であることとする。
これは次式と同値である。
𝜇𝑤:= maxu∈𝑈| cos(u, w)| ≤ 𝜖 (∀w ∈ 𝑊)(10)𝜇𝑤は w ∈ 𝑊 に対して計算され、u ∈ 𝑈 を動かした時の最大絶対コサイン類似度である。
「完全に」擬似直交する場合には、すべての w ∈ 𝑊 に対して 𝜇𝑤が ±𝜖 の範囲内である。
𝜇𝑤が ±𝜖 の範囲内にある wの比率を「擬似直交の度合い」とする。
分析手順与えられた二つのベクトル群の一方を式（10）における 𝑈、他方を 𝑊 として、𝑊 に含まれる全ての行ベクトル w に対して 𝜇𝑤を算出する。
アテンション層と FFN 層の解釈可能性を調べるために、それぞれの部分空間が語彙空間とどの程度擬似直交しているか評価する必要があるので、それらを𝑊, 語彙空間を 𝑈 として式（10）を適用する。
データ Huggingface の事前学習済みモデル（Pytorch版）[15]より BERT-base と GPT2 を用いる。
いずれも12 層のTransformer モデルであり、前者はBidirectionalencoder、後者は Unidirectional decoder である。
それぞれ単語埋め込み行列 𝐸, 位置ベクトル行列 𝑃 並びにアテンション層の行空間を与える行列として 𝑊𝑙𝑣𝑜と FFN 層の行列として 𝑊𝑙𝑜𝑢 𝑡を用いる。



3.2 予備分析：位置ベクトルと語彙空間

BERTが用いる単語埋め込み行列𝐸は，30522個の 768 次元ベクトルからなる。
すべての組のコサイン類似度の分布は図 2 左の通りほとんどが正で中央値は 0.447 である（クラスタリングすると、同一クラスタの単語は意味的なまとまりを持つ[16])。
これらのベクトルの張る部分空間が語彙空間である。
BERT の 512 個ある位置ベクトルは語彙空間とどのような幾何的関係にあるだろうか。
単語埋め込みと位置ベクトルのコサイン類似度（30522 × 512 組）は、ほぼすべて ±0.1 の範囲内にある。
（図 2 右）図 2 単語埋め込みと位置ベクトルのコサイン類似度POS 空間と語彙空間の擬似直交の度合いを評価するために、定義式（10）により位置ベクトルを 𝑊として算出した 𝜇𝑤の値を図 3 右に昇順に示す。
横軸は個々の位置ベクトルの順位に対応する。
𝜇𝑤の最大値は図の最右で 1.0 であるが、二番目は 0.174であり、𝜖 = 0.15 とする時 𝜇𝑤≤ 𝜖 となる 𝑤 の比率95.9%が POS 空間と語彙空間の擬似直交の度合いとなる。
𝜇𝑤= 1.0 となる組み合わせはトークン[CLS]と位置 0 である2）．POS 空間は[CLS]を除く語彙空間と擬似直交している。
GPT2 に関する同様の分析図 3 POS 空間と語意空間の擬似直交性（図 3 右）は、すべての位置ベクトルが語彙空間と擬似直交することを示す。
1052 個の位置ベクトルに対して算出された 𝜇𝑤の最大値は 0.148 である。


3.3 分析結果：アテンション層と FFN 層

アテンション層と FFN 層について、その部分空間と語彙空間との擬似直交性を調べる。
式（10）に従い、単語埋め込み行列を 𝑈 として、アテンション層に関しては行列 𝑊𝑙𝑉𝑂(𝑙 = 0, . . . , 11)の行ベクトル、FFN 層に関しては 𝑊𝑙𝑜𝑢𝑡(𝑙 = 0, . . . , 11)の行ベクトルに対して、値 𝜇𝑤を算出した。
図 4 は BERT の各 𝑙図 4 アテンション層（左）と FFN 層（右）の語彙空間との擬似直交の度合い（層 𝑙 = 0, . . . , 11 ごと）：BERT-base層ごとにアテンション層（左）と FFN 層（右）について値 𝜇𝑤を昇順に示している。
𝜖 = 0.15 として、アテンション層では 12 層平均で 93.2%が語彙空間と擬似直交する一方、FFN 層では平均で 71.8%の 𝑊𝑜𝑢𝑡の行ベクトルが語彙空間と擬似直交である。
逆に言えば、29.2%において単語埋め込みとのコサイン類似度が 𝜖 以上ということである。
但し、層によるば2） BERT では文頭記号としてトークン[CLS]が追加され、その文中の位置は常に 0 となることから、[POS0]が[CLS]と一致されるよう学習されていることが示唆される。
らつきが大きく低層ほど語彙空間と共有する部分空間が大きい（𝑙 = 0 では 80.0%，𝑙 = 5 では 26.4%)。
同様の傾向が GPT2 でも観察された（図 5）。
アテ図 5 語彙空間との擬似直交の度合い：GPT2ンション層（左）ではほとんどの層において語彙空間と擬似直交する一方、FFN 層（右）は高層において語彙空間との交わりをもつ。
𝜖 = 0.2 とした時、アテンション層の行ベクトルのうち 88.8%（11 層を除くと 92.7%）が擬似直交であるに対して、FFN 層では全層平均では 86.1%が擬似直交であるが、6 層以上に限ると平均 77.1%，10 層以上の平均は 67.5%と高層ほど語彙空間に近い。
BERT との違いは、FFN層において低層と高層で語彙空間との擬似直交の度合いが逆転することと、アテンション層の最終層のみ語彙空間とは擬似直交でないことである。
エンコーダとデコーダの違いと考えられるが、その機序の解明は今後の研究課題である。


4 コンセプトベクトルによる文脈化



4.1 Logit lens が有効な部分空間

擬似直交分析からの示唆は、アテンション層の行空間が語彙空間とほぼ直交する一方、FFN 層はそうではなく、一定数の行ベクトルが単語埋め込みベクトルのいずれかと非ゼロの内積をとることである。
実際に Geva ら[5]は FFN 層から抽出した行ベクトル（本研究ではコンセプトベクトルと呼ぶ）を単語分布に変換し、その 20–40%が解釈可能な意味のまとまり持つことを報告している。
本研究の分析は，Transformer において Logit lens が有効な部分空間（FFN 層）とそうでない部分空間（アテンション層）の存在に説明を与えるものであり、Geva らの研究に数理的根拠を与える。



4.2 内部表現の解釈と文脈化の過程

Z ベクトルがコンセプトベクトルの、Y ベクトルがアテンション層の行ベクトルのそれぞれ線形結合であること、またアテンション層と POS 空間が語彙空間と擬似直交していると見做せることから、式（9）に Logit lens を適用して次式を得る。
𝑋𝐿𝐸𝑇=𝑋𝑒+ 𝑋𝑝+𝐿−1𝑙=0𝑌𝑙+𝐿−1𝑙=0𝑍𝑙𝐸𝑇≈𝑋𝑒+𝐿−1𝑙=0𝑍𝑙𝐸𝑇(11)同式は、入力文の単語埋め込みに対して、各層の Zベクトルを順に加算することで最終層の出力を得る文脈化のプロセスと解釈することができる。
検証のため BERT へ以下の入力文を与えて、FFN 各層出力の Z ベクトルの総和をとり、Logit lens を適用する。
文 1 “The king wears a tie”文 2 “He ties the package with string before mailing it”文 3 “The match ended in a tie, so both teams shared the points”表 1 に文 1 から得られた総和 Z ベクトルに最も近い単語を位置ごとに示す。
位置番号 3 を除き、入力文の単語が top に来ており入力文を復号している。
表 1 コンセプトベクトルによる再構成位置番号 0 1 2 3 4 5 6入力文[CLS] the king wears a tie [SEP]Top [CLS] the king is a tie [SEP]類似度 .32 .39 .34 .30 .45 .16 .28Top 以外の上位単語からは、総和 Z ベクトルが符号化している文脈を観察することができる。
表 2は，3 文に共通する多義語 tie に対応する総和 Z ベクトルを変換した際の上位単語を示す。
文脈に沿った単語群が上位に現れており、総和 Z ベクトルが文脈依存の語義を反映していることを示唆する。
表 2 総和 Z ベクトルに現れる多義語の文脈順位語義 1 語義 2 語義 3文脈衣類（名詞） 縛る（動詞） 引分け（名詞）1 tie tied tie2 ties closed tied3 clothes tying game4 plaid cut tying5 jacket worked rivalry6 shirts tie comparison7 wardrobe cutting conﬂict

5 まとめと考察

語彙空間との擬似直交性の違いから各層に対するLogit lens の有効性の違いを説明し、FFN 層の出力がTransformer の内部状態の文脈化を担っている可能性を示した。
今後の課題は、アテンションヘッドによるコンセプトベクトルの活性化分析により意味合成の機序を解明することと、アテンション層が意味合成の文脈を準備するプロセスを条件付き確率としてモデル化することである。



謝辞

本研究は科研費基盤研究 B(一般) JP23H0369，JSTさきがけ JPMJPR20C9，JST CREST JPMJCR23P4，JSPS KAKENHI 24KJ1202 の助成を受けて行われた。

参考文献


[1] Lena K¨astner and Barnaby Crook. Explaining ai throughmechanistic interpretability. European Journal for Phi-losophy of Science, Vol. 14, No. 4, p. 52, 2024.
[2] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith,and Jacob Steinhardt. Progress measures for grokking viamechanistic interpretability, 2023.
[3] Marco Baroni, Raﬀaella Bernardi, Roberto Zamparelli,et al. Frege in space: A program for compositional dis-tributional semantics. Linguistic Issues in languagetechnology, Vol. 9, pp. 241–346, 2014.
[4] nostalgebraist. interpreting gpt: the logit lens, 2020.www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/.
[5] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Gold-berg. Transformer feed-forward layers build predictionsby promoting concepts in the vocabulary space. arXivpreprint arXiv:2203.14680, 2022.
[6] Javier Ferrando, Gabriele Sarti, Arianna Bisazza, andMarta R. Costa-juss`a. A primer on the inner workingsof transformer-based language models, 2024.
[7] Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Be-rant. Analyzing transformers in embedding space. arXivpreprint arXiv:2209.02535, 2022.
[8] Tomas Mikolov. Eﬃcient estimation of word representa-tions in vector space. arXiv preprint arXiv:1301.3781,Vol. 3781, , 2013.
[9] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, andAndrej Risteski. Linear algebraic structure of word senses,with applications to polysemy. Transactions of the As-sociation for Computational Linguistics, Vol. 6, pp.483–495, 2018.
[10] Kiho Park, Yo Joong Choe, and Victor Veitch. The lin-ear representation hypothesis and the geometry of largelanguage models, 2024.
[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser,and Illia Polosukhin. Attention is all you need. In I. Guyon,U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-wanathan, and R. Garnett, editors, Advances in NeuralInformation Processing Systems, Vol. 30. Curran As-sociates, Inc., 2017.
[12] Jimmy Lei Ba. Layer normalization. arXiv preprintarXiv:1607.06450, 2016.
[13] Nelson Elhage, Tristan Hume, Catherine Olsson, NicholasSchiefer, Tom Henighan, Shauna Kravec, Zac Hatﬁeld-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, RogerGrosse, Sam McCandlish, Jared Kaplan, Dario Amodei,Martin Wattenberg, and Christopher Olah. Toy models ofsuperposition, 2022.
[14] Pentti Kanerva. Hyperdimensional computing: An intro-duction to computing in distributed representation withhigh-dimensional random vectors. Cognitive computa-tion, Vol. 1, pp. 139–159, 2009.
[15] huggingface. Pytorch-transformers, 2019. https://huggingface.co/transformers/v1.2.0/index.html.
[16] 前田晃弘, 鳥居拓馬, 日昇平, 大関洋平. 部分空間法に着想を得た transformer のアテンションヘッドにおける特徴抽出. 言語処理学会第 30 回年次大会, 2024.
[17] Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, and MarkCrowley. Johnson-lindenstrauss lemma, linear and nonlin-ear random projections, random fourier features, and ran-dom kitchen sinks: Tutorial and survey. arXiv preprintarXiv:2108.04172, 2021.
[18] Sanjoy Dasgupta and Anupam Gupta. An elementaryproof of a theorem of johnson and lindenstrauss. RandomStructures & Algorithms, Vol. 22, No. 1, pp. 60–65,2003.




A 擬似直交ベクトルの上限数

𝑑 次元ベクトル空間が含む互いに擬似直交なベクトルの数の試算を試みる。
次元数 𝑑 を固定したとき、コサイン類似度が閾値 𝜖 以下のベクトル群（擬似直交ベクトル）の最大数 𝑁 として、Transformer が利用する擬似直交基底の数を推計する。

A.1 Johnson-Lindenstrauss 補題

先行研究[13]で引用されるこの補題は、高次元空間にある任意の 𝑁 個の点集合を次元削減により低次元空間 ℝ𝑑へ埋め込む際に、その点間距離が 1 ± 𝜖 の範囲内で近似的に保存されることを保証する。
補題の概略（[17]定理 1）は，𝐷 次元ベクトルの集合を𝜒 = {𝑥𝑖∈ ℝ𝐷}𝑁𝑖=1、エラー許容率を 0 ≤ 𝜖 ≤ 1 として、正の整数 𝑑 が𝑑 ≥ Ω𝜖−2log(𝑛)(12)を満たす時、ランダムに生成した写像（表現行列の成分が独立同一な標準正規分布に従う写像） 𝑓 : ℝ𝐷→ ℝ𝑑のもとで、ある確率で(1 − 𝜖)
∥𝑥𝑖− 𝑥𝑗∥22≤ ∥ 𝑓 (𝑥𝑖) − 𝑓 (𝑥𝑗)∥22≤ (1 + 𝜖)∥𝑥𝑖− 𝑥𝑗∥22(13)が成り立つというものである。
Ω はアルゴリズム計算量の漸近的な下限を与える記号である3）。
この定理の主張は、次元数 𝑑 を増やせば、任意のベクトル間の内積を任意の閾値 𝜖 に抑えることが可能であることを示唆する。
そして、次元数 𝑑 が十分に大きければ、ランダムな埋め込みを用いてベクトル間の相対関係（距離と内積）を保存することを意味する。
𝑑, 𝑁, 𝜖 の関係式（12）は 𝑑 を固定した場合、擬似直交ベクトル数 𝑁 の上限が 𝜖 に依存して増加することを示す。

A.2 ランダム抽出による数値実験

Johnson-Lindenstrauss 補題は、ランダム写像により次元削減を行う際の全ての点群の挙動を記述するものであり、特に極値（例外的に大きな、あるいは小さな値）が閾値内に保存されることを保証する。
そこで具体的に極値の挙動を調べるために、次の数値実験を行った。
まず 𝑑 = 768次元のベクトル 𝒗 ∈ ℝ𝑑をランダムに、ベクトルの各成分は独立同一な標準正規分布に従うよう、𝑁 個生成した上で、ノルム 1 に正規化する。
𝑁 個のベクトル間の内積(正規化しているのでコサイン類似度と同じ）の絶対値の最大max𝑖≠ 𝑗 ∈ [ 𝑁 ]|⟨𝒗𝑖, 𝒗𝑗⟩| を求める。
𝑁 = 10000, 20000, 30000の各条件で 100 回の試行を行い、その統計分布を記録した。
図 6 は、各試行ごとの内積最大値を昇順に表示している。
ベクトル数 𝑁 が増加するに伴い、内積最大値がグラフが上方にシフトする傾向が確認できる。
これは極値を得る分布のサンプル数𝑁𝐶2が増加するに伴い、極端な値が大きくなることを反映している。
本実験は、Johnson-Lindenstrauss 補題のランダム写像の代わりにターゲット空間のベクトルを直接ランダム抽出したものであり、同補題の主張する事実を直接再現するものではない、しかし、両者は高次元空間におけるランダムプロセス用いており、いずれもランダムベクトル間3） [定義]ある関数 𝑔(𝑛)について Ω(𝑔 (𝑛))とは、ある正の定数𝑐, 𝑛0が存在して、全ての 𝑛 ≥ 𝑛0に対して 0 ≤ 𝑐(𝑔(𝑛)) ≤ 𝑓 (𝑛)を満たすような 𝑓 (𝑛)の集合である。
図 6 極値（すべてのベクトル間の内積最大値）の分布の内積が高次元空間では分布的に集中するという現象を反映している。


A.3 上限数の理論的推計

Transformer の内部状態に用いるベクトル数は、BERT の場合、 単語埋め込み(30,522 個）、位置ベクトル（512 個），アテンション層の出力ウェイト（768 個 ×12 層）FFN 層の出力ウェイト（3,072 個 ×12 個）であり合計 77,114 個である．GPT2 では、単語埋め込み(50,257 個）、位置ベクトル（1,024 個）でそれ以外は BERT と同じで、合計 97,361 個である。
ここで、𝑑 = 768 の空間で 𝑁 = 100, 000 のベクトルを擬似直交とするための 𝜖 の理論値を次のように求める。
式(12)の条件を満たす 𝜖∗として、[18]（定理 2.1)で提案されている Ω(𝜖−2ln 𝑛) = 8𝜖−2ln(𝑛)を用いると次の値を得る。
これは閾値の下限である。
𝜖∗=8 ln 𝑁𝑑= 0.346 (14)なお、𝑁 = 10000 では 𝜖∗= 0.310 であり、数値実験で見たようにランダムサンプリングしたベクトル群は、Johnson-Lindenstrauss 補題を成立させるための条件を充足している。