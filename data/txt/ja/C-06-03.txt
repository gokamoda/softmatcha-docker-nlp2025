難易度調整可能な多枝選択式読解問題自動生成手法と DirectPreference Optimization による難易度調整精度の向上

富川雄斗

1

 宇都雅輝

11

電気通信大学大学院



 {tomikawa,uto}@ai.lab.uec.ac.jp



概要

近年、深層学習を用いて任意の難易度で読解問題を生成できる技術が提案されている。
しかし、従来手法は、読解対象文中から答えを抜き出して回答する形式の問題のみを対象としており、教育現場で広く使われている多肢選択式問題を生成できない。
加えて、従来手法では訓練データ中の問題の単語列に対する尤度を最大化するように深層学習モデルを訓練しており、難易度調整精度を直接最適化していないため、難易度調整精度に改善の余地が残る。
そこで本研究では、自己回帰型の深層学習モデルを用いた難易度調整可能な多枝選択式問題生成手法を開発するとともに、Direct Preference Optimization により難易度調整精度を最大化する訓練手法を提案する。
1 はじめに読解対象文から問題を自動で生成する読解問題自動生成が教育分野において注目されている[1, 2, 3, 4, 5]。
学習支援や教育評価の文脈で問題生成を利用することを想定すると、各学習者の能力に合った適切な難易度の問題を出題することが望ましいと考えられ、そのためには難易度を調整可能な問題生成手法が必要となる[6, 7]。
このような背景から、近年では難易度調整機能を有する問題生成技術がいくつか提案されている[8, 9, 10, 11, 12, 13]。
しかし、従来手法には以下の課題が残る。
1. 図 1 左で示されるような、読解対象文中から答え単語列を抜き出して回答する形式の問題のみを対象としており、教育現場で広く使われている多枝選択式問題を直接には生成できない。
2. 訓練データ中の問題の単語列に対する尤度を最大化するように問題生成モデルを訓練しており、難易度調整の精度を目的関数として最適化していないため、難易度調整精度に改善の余地が残る。
これらの課題を解決するために、本研究では自己回帰型の深層学習モデルを用いた難易度調整可能な多枝選択式問題生成手法を開発するとともに、DPO(Direct Preference Optimization)[14]に基づいて難易度調整精度を最大化する手法を提案する。
具体的にはまず、自己回帰型生成モデルである Llama 2 [15]を用いて、図 1 右のように、任意の読解対象文と難易度から多枝選択式問題の問題文と選択枝を生成できる手法を提案する。
加えて、LLM の訓練方法として近年広く利用されている DPO に基づいて、難易度調整の精度を目的関数として最適化するようにモデルを訓練する手法を提案する。
2 項目反応理論を用いた難易度調整

可能な問題生成

本研究は、Tomikawa et al. [12]が提案した項目反応理論(Item Response Theory: IRT)[16]を用いた難易度調整可能な抽出型読解問題自動生成手法を基礎とする。
そこで本章では、この手法について説明する。
Tomikawa et al. では、図 1 左のように、読解対象文 𝒄 と所望の難易度 𝑏 から、問題文 𝒒 とそれに対応する答え 𝒂 を生成することを目標としている。
なお、答え 𝒂 は読解対象文 𝒄 中に存在する(𝒂 ∈ 𝒄)形式が想定されている。
このタスクを解く深層学習モデルを構築するためには、これらの 4 つの要素で構成されたデータセット {𝒄𝑖, 𝒒𝑖, 𝒂𝑖, 𝑏𝑖|𝑖 ∈ 1, . . . , 𝐼} が必要となる。
ここで、𝒄𝑖, 𝒒𝑖, 𝒂𝑖, 𝑏𝑖は 𝑖 番目のデータの読解対象文、難易度、問題文、答えをそれぞれ表し，𝐼 はデータ数を表す。
しかし、一般的なデータセットには問題の難易度が含まれていない。
そこでTomikawa et al. はデータセット中の各問題に対してIRT モデルの一つであるラッシュモデル[17, 18]に基づいて難易度を推定し、難易度を含んだデータセットを構築する手法を提案している。
ラッシュモデルに基づいて難易度付きの問題データセットを構

難易度調整可能な多枝選択式読解問題自動生成手法と DirectPreference Optimization による難易度調整精度の向上

富川雄斗

1

 宇都雅輝

11

電気通信大学大学院



 {tomikawa,uto}@ai.lab.uec.ac.jp



概要

近年、深層学習を用いて任意の難易度で読解問題を生成できる技術が提案されている。
しかし、従来手法は、読解対象文中から答えを抜き出して回答する形式の問題のみを対象としており、教育現場で広く使われている多肢選択式問題を生成できない。
加えて、従来手法では訓練データ中の問題の単語列に対する尤度を最大化するように深層学習モデルを訓練しており、難易度調整精度を直接最適化していないため、難易度調整精度に改善の余地が残る。
そこで本研究では、自己回帰型の深層学習モデルを用いた難易度調整可能な多枝選択式問題生成手法を開発するとともに、Direct Preference Optimization により難易度調整精度を最大化する訓練手法を提案する。
1 はじめに読解対象文から問題を自動で生成する読解問題自動生成が教育分野において注目されている[1, 2, 3, 4, 5]。
学習支援や教育評価の文脈で問題生成を利用することを想定すると、各学習者の能力に合った適切な難易度の問題を出題することが望ましいと考えられ、そのためには難易度を調整可能な問題生成手法が必要となる[6, 7]。
このような背景から、近年では難易度調整機能を有する問題生成技術がいくつか提案されている[8, 9, 10, 11, 12, 13]。
しかし、従来手法には以下の課題が残る。
1. 図 1 左で示されるような、読解対象文中から答え単語列を抜き出して回答する形式の問題のみを対象としており、教育現場で広く使われている多枝選択式問題を直接には生成できない。
2. 訓練データ中の問題の単語列に対する尤度を最大化するように問題生成モデルを訓練しており、難易度調整の精度を目的関数として最適化していないため、難易度調整精度に改善の余地が残る。
これらの課題を解決するために、本研究では自己回帰型の深層学習モデルを用いた難易度調整可能な多枝選択式問題生成手法を開発するとともに、DPO(Direct Preference Optimization)[14]に基づいて難易度調整精度を最大化する手法を提案する。
具体的にはまず、自己回帰型生成モデルである Llama 2 [15]を用いて、図 1 右のように、任意の読解対象文と難易度から多枝選択式問題の問題文と選択枝を生成できる手法を提案する。
加えて、LLM の訓練方法として近年広く利用されている DPO に基づいて、難易度調整の精度を目的関数として最適化するようにモデルを訓練する手法を提案する。
2 項目反応理論を用いた難易度調整

可能な問題生成

本研究は、Tomikawa et al. [12]が提案した項目反応理論(Item Response Theory: IRT)[16]を用いた難易度調整可能な抽出型読解問題自動生成手法を基礎とする。
そこで本章では、この手法について説明する。
Tomikawa et al. では、図 1 左のように、読解対象文 𝒄 と所望の難易度 𝑏 から、問題文 𝒒 とそれに対応する答え 𝒂 を生成することを目標としている。
なお、答え 𝒂 は読解対象文 𝒄 中に存在する(𝒂 ∈ 𝒄)形式が想定されている。
このタスクを解く深層学習モデルを構築するためには、これらの 4 つの要素で構成されたデータセット {𝒄𝑖, 𝒒𝑖, 𝒂𝑖, 𝑏𝑖|𝑖 ∈ 1, . . . , 𝐼} が必要となる。
ここで、𝒄𝑖, 𝒒𝑖, 𝒂𝑖, 𝑏𝑖は 𝑖 番目のデータの読解対象文、難易度、問題文、答えをそれぞれ表し，𝐼 はデータ数を表す。
しかし、一般的なデータセットには問題の難易度が含まれていない。
そこでTomikawa et al. はデータセット中の各問題に対してIRT モデルの一つであるラッシュモデル[17, 18]に基づいて難易度を推定し、難易度を含んだデータセットを構築する手法を提案している。
ラッシュモデルに基づいて難易度付きの問題データセットを構図 1: 答え抜き出し形式の問題生成(既存研究でのタスク)と多枝選択式問題生成(本研究でのタスク)築する手順は次のとおりである。
まずデータセット中の各問題 𝒒𝑖を性能の異なる多数の QA システムに出題し、正誤反応データを取得する。
なお、本来は人間の正誤反応データを取得すべきであるが、これには膨大なコストがかかるため、ここでは QA システムを仮想的な学習者とみなして代用している。
次に、このデータからラッシュモデルによって各問題の難易度 𝑏𝑖を推定し、元々のデータセットに 𝑏𝑖を加えることで、難易度を含んだデータセットを構築している。
このように構築した難易度付きデータセットを用いて、読解対象文から所望の難易度に沿った答えを抽出する BERT モデル[19]と読解対象文と答え、および所望の難易度から問題文を生成する T5 モデル[20]を構築することで、難易度調整可能な答えと問題文の生成を行う。
しかし、この手法では多枝選択式問題を生成できない。
また、3.1 で詳述するように難易度を所与とした問題単語列の尤度を最大化するように問題生成モデルが訓練されており、難易度調整の精度を最適化していないため、難易度調整精度に改善の余地が残る。
3 提案手法以上の問題を解決するために、本研究では、Tomikawa et al. の枠組みをベースに、自己回帰型の深層学習モデルを用いた難易度調整可能な多枝選択式問題生成手法を開発するとともに、DPO を用いて難易度調整精度を最大化する手法を開発する。
3.1 教師あり学習に基づく訓練本研究では、図 1 右のように、読解対象文 𝒄 と難易度 𝑏 を入力すると、難易度に応じた問題文 𝒒 と正答選択枝 𝒂、および 3 つの誤答選択枝 𝒅1, 𝒅2, 𝒅3を生成する手法を提案する。
具体的には、表 1 の入力 𝒙の形式で読解対象文 𝒄 と難易度 𝑏 を与えると、表 1の出力 𝒚 の形式で問題文 𝒒、正答選択枝 𝒂，3 つの誤答選択枝 𝒅1, 𝒅2, 𝒅3を生成するように、Llama 2 [15]を学習することで実現する。
なお、表中の { } 内は実際のデータで置き換えられることに注意する。
また、一般的なデータセットには難易度 𝑏 は含まれていないため、Tomikawa et al. と同様の手順で難易度を付与したデータセットを構築する。
難易度調整可能な多枝選択式問題生成を実現するために、提案手法では以下の負の対数尤度を損失関数として最小化するようにモデルの訓練を行う。
Lsft= −𝔼( 𝒙,𝒚 )∼Dlog 𝜋( 𝒚|𝒙, 𝝓sft)(1)ここでDは {𝒄𝑖, 𝒒𝑖, 𝒂𝑖, 𝒅𝑖1, 𝒅𝑖2, 𝒅𝑖3, 𝑏𝑖|𝑖 ∈ 1, . . . , 𝐼 } からなるデータセットであり、𝜋 は Llama 2，𝝓sftは訓練対象のパラメータ、𝜋(𝒚|𝒙, 𝝓sft)は 𝒙 で条件付けされた出力文字列 𝒚 の尤度を表す。
この訓練を行うことによって、与えられた読解対象文と難易度に沿った問題文と選択枝を 𝒚 の形式で生成できるようになる。
3.2 難易度調整精度を目的関数とした追加

訓練

上記の教師あり学習に基づく訓練を行うことで、指定した難易度を反映した多枝選択式問題生成が可能となる。
しかし、この訓練では難易度調整精度を直接的に最大化していない。
そこで本研究では、上記の教師あり学習で訓練された問題生成モデルを、難易度調整精度を最大化するように追加訓練する手法を提案する。
この訓練のために、本研究では近年の LLM の追加学習手法として広く利用されている[21, 22]，DPO [14]を利用する。
DPO を用いた提案手法における損失関数は次のとおりである。
Ldpo= −𝔼( 𝒙,𝒚𝑤,𝒚𝑙)∼Ddpolog 𝜎𝛽 log𝜋( 𝒚𝑤|𝒙, 𝝓dpo)𝜋( 𝒚𝑤|𝒙,ˆ𝝓sft)−𝛽 log𝜋( 𝒚𝑙|𝒙, 𝝓dpo)𝜋( 𝒚𝑙|𝒙,ˆ𝝓sft)(2)

難易度調整可能な多枝選択式読解問題自動生成手法と DirectPreference Optimization による難易度調整精度の向上

富川雄斗

1

 宇都雅輝

11

電気通信大学大学院



 {tomikawa,uto}@ai.lab.uec.ac.jp



概要

近年、深層学習を用いて任意の難易度で読解問題を生成できる技術が提案されている。
しかし、従来手法は、読解対象文中から答えを抜き出して回答する形式の問題のみを対象としており、教育現場で広く使われている多肢選択式問題を生成できない。
加えて、従来手法では訓練データ中の問題の単語列に対する尤度を最大化するように深層学習モデルを訓練しており、難易度調整精度を直接最適化していないため、難易度調整精度に改善の余地が残る。
そこで本研究では、自己回帰型の深層学習モデルを用いた難易度調整可能な多枝選択式問題生成手法を開発するとともに、Direct Preference Optimization により難易度調整精度を最大化する訓練手法を提案する。
1 はじめに読解対象文から問題を自動で生成する読解問題自動生成が教育分野において注目されている[1, 2, 3, 4, 5]。
学習支援や教育評価の文脈で問題生成を利用することを想定すると、各学習者の能力に合った適切な難易度の問題を出題することが望ましいと考えられ、そのためには難易度を調整可能な問題生成手法が必要となる[6, 7]。
このような背景から、近年では難易度調整機能を有する問題生成技術がいくつか提案されている[8, 9, 10, 11, 12, 13]。
しかし、従来手法には以下の課題が残る。
1. 図 1 左で示されるような、読解対象文中から答え単語列を抜き出して回答する形式の問題のみを対象としており、教育現場で広く使われている多枝選択式問題を直接には生成できない。
2. 訓練データ中の問題の単語列に対する尤度を最大化するように問題生成モデルを訓練しており、難易度調整の精度を目的関数として最適化していないため、難易度調整精度に改善の余地が残る。
これらの課題を解決するために、本研究では自己回帰型の深層学習モデルを用いた難易度調整可能な多枝選択式問題生成手法を開発するとともに、DPO(Direct Preference Optimization)[14]に基づいて難易度調整精度を最大化する手法を提案する。
具体的にはまず、自己回帰型生成モデルである Llama 2 [15]を用いて、図 1 右のように、任意の読解対象文と難易度から多枝選択式問題の問題文と選択枝を生成できる手法を提案する。
加えて、LLM の訓練方法として近年広く利用されている DPO に基づいて、難易度調整の精度を目的関数として最適化するようにモデルを訓練する手法を提案する。
2 項目反応理論を用いた難易度調整

可能な問題生成

本研究は、Tomikawa et al. [12]が提案した項目反応理論(Item Response Theory: IRT)[16]を用いた難易度調整可能な抽出型読解問題自動生成手法を基礎とする。
そこで本章では、この手法について説明する。
Tomikawa et al. では、図 1 左のように、読解対象文 𝒄 と所望の難易度 𝑏 から、問題文 𝒒 とそれに対応する答え 𝒂 を生成することを目標としている。
なお、答え 𝒂 は読解対象文 𝒄 中に存在する(𝒂 ∈ 𝒄)形式が想定されている。
このタスクを解く深層学習モデルを構築するためには、これらの 4 つの要素で構成されたデータセット {𝒄𝑖, 𝒒𝑖, 𝒂𝑖, 𝑏𝑖|𝑖 ∈ 1, . . . , 𝐼} が必要となる。
ここで、𝒄𝑖, 𝒒𝑖, 𝒂𝑖, 𝑏𝑖は 𝑖 番目のデータの読解対象文、難易度、問題文、答えをそれぞれ表し，𝐼 はデータ数を表す。
しかし、一般的なデータセットには問題の難易度が含まれていない。
そこでTomikawa et al. はデータセット中の各問題に対してIRT モデルの一つであるラッシュモデル[17, 18]に基づいて難易度を推定し、難易度を含んだデータセットを構築する手法を提案している。
ラッシュモデルに基づいて難易度付きの問題データセットを構図 1: 答え抜き出し形式の問題生成(既存研究でのタスク)と多枝選択式問題生成(本研究でのタスク)築する手順は次のとおりである。
まずデータセット中の各問題 𝒒𝑖を性能の異なる多数の QA システムに出題し、正誤反応データを取得する。
なお、本来は人間の正誤反応データを取得すべきであるが、これには膨大なコストがかかるため、ここでは QA システムを仮想的な学習者とみなして代用している。
次に、このデータからラッシュモデルによって各問題の難易度 𝑏𝑖を推定し、元々のデータセットに 𝑏𝑖を加えることで、難易度を含んだデータセットを構築している。
このように構築した難易度付きデータセットを用いて、読解対象文から所望の難易度に沿った答えを抽出する BERT モデル[19]と読解対象文と答え、および所望の難易度から問題文を生成する T5 モデル[20]を構築することで、難易度調整可能な答えと問題文の生成を行う。
しかし、この手法では多枝選択式問題を生成できない。
また、3.1 で詳述するように難易度を所与とした問題単語列の尤度を最大化するように問題生成モデルが訓練されており、難易度調整の精度を最適化していないため、難易度調整精度に改善の余地が残る。
3 提案手法以上の問題を解決するために、本研究では、Tomikawa et al. の枠組みをベースに、自己回帰型の深層学習モデルを用いた難易度調整可能な多枝選択式問題生成手法を開発するとともに、DPO を用いて難易度調整精度を最大化する手法を開発する。
3.1 教師あり学習に基づく訓練本研究では、図 1 右のように、読解対象文 𝒄 と難易度 𝑏 を入力すると、難易度に応じた問題文 𝒒 と正答選択枝 𝒂、および 3 つの誤答選択枝 𝒅1, 𝒅2, 𝒅3を生成する手法を提案する。
具体的には、表 1 の入力 𝒙の形式で読解対象文 𝒄 と難易度 𝑏 を与えると、表 1の出力 𝒚 の形式で問題文 𝒒、正答選択枝 𝒂，3 つの誤答選択枝 𝒅1, 𝒅2, 𝒅3を生成するように、Llama 2 [15]を学習することで実現する。
なお、表中の { } 内は実際のデータで置き換えられることに注意する。
また、一般的なデータセットには難易度 𝑏 は含まれていないため、Tomikawa et al. と同様の手順で難易度を付与したデータセットを構築する。
難易度調整可能な多枝選択式問題生成を実現するために、提案手法では以下の負の対数尤度を損失関数として最小化するようにモデルの訓練を行う。
Lsft= −𝔼( 𝒙,𝒚 )∼Dlog 𝜋( 𝒚|𝒙, 𝝓sft)(1)ここでDは {𝒄𝑖, 𝒒𝑖, 𝒂𝑖, 𝒅𝑖1, 𝒅𝑖2, 𝒅𝑖3, 𝑏𝑖|𝑖 ∈ 1, . . . , 𝐼 } からなるデータセットであり、𝜋 は Llama 2，𝝓sftは訓練対象のパラメータ、𝜋(𝒚|𝒙, 𝝓sft)は 𝒙 で条件付けされた出力文字列 𝒚 の尤度を表す。
この訓練を行うことによって、与えられた読解対象文と難易度に沿った問題文と選択枝を 𝒚 の形式で生成できるようになる。
3.2 難易度調整精度を目的関数とした追加

訓練

上記の教師あり学習に基づく訓練を行うことで、指定した難易度を反映した多枝選択式問題生成が可能となる。
しかし、この訓練では難易度調整精度を直接的に最大化していない。
そこで本研究では、上記の教師あり学習で訓練された問題生成モデルを、難易度調整精度を最大化するように追加訓練する手法を提案する。
この訓練のために、本研究では近年の LLM の追加学習手法として広く利用されている[21, 22]，DPO [14]を利用する。
DPO を用いた提案手法における損失関数は次のとおりである。
Ldpo= −𝔼( 𝒙,𝒚𝑤,𝒚𝑙)∼Ddpolog 𝜎𝛽 log𝜋( 𝒚𝑤|𝒙, 𝝓dpo)𝜋( 𝒚𝑤|𝒙,ˆ𝝓sft)−𝛽 log𝜋( 𝒚𝑙|𝒙, 𝝓dpo)𝜋( 𝒚𝑙|𝒙,ˆ𝝓sft)(2)表 1: 提案モデルの入力に対応するプロンプトの構造と出力の構成入力(𝒙): Create a question and 4 options with a diﬃculty level of {𝑏} based on the Context. Option 1 is the correct answer and Option 2, 3and 4 are the distractor options. Diﬃculty level -3.0 is the easiest and 3.0 is the most diﬃcult. The output format is “⟨c⟩{Option 1 (CorrectOption)}⟨q⟩{Question}⟨d1⟩{Option 2 (Distractor Option)}⟨d2⟩{Option 3 (Distractor Option)}⟨d3⟩{Option 4 (Distractor Option)}”### Context: {Context}⟨l⟩出力(𝒚 ): ⟨c⟩ {𝒂} ⟨q⟩ {𝒒} ⟨d1⟩ {𝒅1} ⟨d2⟩ {𝒅2} ⟨d3⟩ {𝒅3}ここで、𝜎 はシグモイド関数、ˆ𝝓sftは 1 段階目の教師あり学習で推定されたモデルパラメータ、𝝓dpoは学習されるモデルパラメータを表し、ˆ𝝓sftを初期パラメータとして訓練が進められる。
𝒚𝑤は 𝒚𝑙よりも難易度調整精度が高い問題を表し、Ddpoは DPO のための訓練データセットDdpo= {𝒙𝑖, 𝒚𝑤𝑖, 𝒚𝑙𝑖|𝑖 ∈ 1, . . . , 𝐼}である。
このような訓練データセットDdpoは，𝑖 番目の問題データから表 1 の形式で入力 𝒙 と出力 𝒚𝑤を構築し，𝑖 番目の読解対象文と同じ読解対象文だが異なる難易度の問題データから表 1 の形式で 𝒚𝑙を構築することで得られる。
4 提案手法の有効性評価実験本章では、提案手法の有効性を評価する実験について述べる。
4.1 実験手順多枝選択式読解問題生成の研究では、モデルの学習や性能評価に RACE データセット[23]が広く用いられているため、本実験でも RACE データセットを評価に用いる。
具体的な実験手順は次の通りである1. QA システムの構築：RACE の検証データセットを用いて性能の異なる 69 個の QA システムを構築した。
具体的には、まず 10 個の事前学習済み深層学習モデルを訓練データ数を変えて訓練することで、能力の異なる 100 個の QAシステムを構築した。
次に、性能が極端に低いQA システムのデータはノイズとなり得るため、訓練データセット中の問題に対する正答率が30%以上の QA システムのみを採択した。
その結果、69 個の QA システムが残った。
2. 難易度を付与した訓練データセットの構築：得られた 69 個の QA システムに訓練データセット中の全ての問題を出題し、その正誤反応データを用いてラッシュモデルで各問題の難易度 𝑏𝑖を推定した。
推定した難易度を訓練データセットに加えることで、難易度を付与した訓練データセットを構築した。
なお、4.2 節での分析のために各 QA システムの能力値も推定した。
3. 難易度調整可能な多枝選択式問題生成モデルの作成：3.1 節で述べたように Llama 2 に対して、読解対象文と難易度を入力すると正答選択枝・問題文・3 つの誤答選択枝を出力するモデルを設計し、教師あり学習に基づく訓練を行うことで難易度調整可能な多枝選択式問題生成モデルを作成した。
以降ではこのモデルを 𝑄𝐺sftと呼ぶ。
4. 難易度調整精度を目的関数とした追加訓練：𝑄𝐺sftに対して 3.2 節で述べた DPO による追加訓練を行うことで、難易度調整精度を向上させた問題生成モデルを作成した。
以降ではこのモデルを 𝑄𝐺dpoと呼ぶ。
5. 様々な難易度を指定した問題生成：RACE テストデータセット中の 300 個の読解対象文に対して，-3.0 から 3.0 まで 0.1 刻みで難易度を指定して、表 1 の入力 𝒙 の形式に沿った累計 18,300 個の入力データを作成し、それらを 𝑄𝐺sft，𝑄𝐺dpoにそれぞれ入力することで問題を生成した。
次節では、ここで生成された問題群の難易度を評価する。
4.2 生成した問題の難易度調整精度評価ここでは提案手法の難易度調整精度を評価する。
上記の手順 5 で生成した問題群の難易度を確認するために、生成された問題群に対する 69 個の QAシステム群の正誤反応データからラッシュモデルを用いて推定された難易度と、問題生成時に指定した難易度との関係を分析した。
結果を図 2 に示す。
図の横軸は指定した難易度、縦軸は生成した問題群に対して推定された難易度の平均を表す。
黒線は原点を通る傾き 1 の直線で、黒線に近いほど指定した難易度と生成した問題群の難易度が近いことを表す。
青点は 𝑄𝐺sftが生成した問題群に対する難易度、橙点は 𝑄𝐺dpoが生成した問題群に対する難易度を表す。
この図より、𝑄𝐺sft，𝑄𝐺dpo共に指定した難易度が高くなるほど生成する問題の難易度も高くなることがわかる。
また二つの生成手法を比較すると、青

難易度調整可能な多枝選択式読解問題自動生成手法と DirectPreference Optimization による難易度調整精度の向上

富川雄斗

1

 宇都雅輝

11

電気通信大学大学院



 {tomikawa,uto}@ai.lab.uec.ac.jp



概要

近年、深層学習を用いて任意の難易度で読解問題を生成できる技術が提案されている。
しかし、従来手法は、読解対象文中から答えを抜き出して回答する形式の問題のみを対象としており、教育現場で広く使われている多肢選択式問題を生成できない。
加えて、従来手法では訓練データ中の問題の単語列に対する尤度を最大化するように深層学習モデルを訓練しており、難易度調整精度を直接最適化していないため、難易度調整精度に改善の余地が残る。
そこで本研究では、自己回帰型の深層学習モデルを用いた難易度調整可能な多枝選択式問題生成手法を開発するとともに、Direct Preference Optimization により難易度調整精度を最大化する訓練手法を提案する。
1 はじめに読解対象文から問題を自動で生成する読解問題自動生成が教育分野において注目されている[1, 2, 3, 4, 5]。
学習支援や教育評価の文脈で問題生成を利用することを想定すると、各学習者の能力に合った適切な難易度の問題を出題することが望ましいと考えられ、そのためには難易度を調整可能な問題生成手法が必要となる[6, 7]。
このような背景から、近年では難易度調整機能を有する問題生成技術がいくつか提案されている[8, 9, 10, 11, 12, 13]。
しかし、従来手法には以下の課題が残る。
1. 図 1 左で示されるような、読解対象文中から答え単語列を抜き出して回答する形式の問題のみを対象としており、教育現場で広く使われている多枝選択式問題を直接には生成できない。
2. 訓練データ中の問題の単語列に対する尤度を最大化するように問題生成モデルを訓練しており、難易度調整の精度を目的関数として最適化していないため、難易度調整精度に改善の余地が残る。
これらの課題を解決するために、本研究では自己回帰型の深層学習モデルを用いた難易度調整可能な多枝選択式問題生成手法を開発するとともに、DPO(Direct Preference Optimization)[14]に基づいて難易度調整精度を最大化する手法を提案する。
具体的にはまず、自己回帰型生成モデルである Llama 2 [15]を用いて、図 1 右のように、任意の読解対象文と難易度から多枝選択式問題の問題文と選択枝を生成できる手法を提案する。
加えて、LLM の訓練方法として近年広く利用されている DPO に基づいて、難易度調整の精度を目的関数として最適化するようにモデルを訓練する手法を提案する。
2 項目反応理論を用いた難易度調整

可能な問題生成

本研究は、Tomikawa et al. [12]が提案した項目反応理論(Item Response Theory: IRT)[16]を用いた難易度調整可能な抽出型読解問題自動生成手法を基礎とする。
そこで本章では、この手法について説明する。
Tomikawa et al. では、図 1 左のように、読解対象文 𝒄 と所望の難易度 𝑏 から、問題文 𝒒 とそれに対応する答え 𝒂 を生成することを目標としている。
なお、答え 𝒂 は読解対象文 𝒄 中に存在する(𝒂 ∈ 𝒄)形式が想定されている。
このタスクを解く深層学習モデルを構築するためには、これらの 4 つの要素で構成されたデータセット {𝒄𝑖, 𝒒𝑖, 𝒂𝑖, 𝑏𝑖|𝑖 ∈ 1, . . . , 𝐼} が必要となる。
ここで、𝒄𝑖, 𝒒𝑖, 𝒂𝑖, 𝑏𝑖は 𝑖 番目のデータの読解対象文、難易度、問題文、答えをそれぞれ表し，𝐼 はデータ数を表す。
しかし、一般的なデータセットには問題の難易度が含まれていない。
そこでTomikawa et al. はデータセット中の各問題に対してIRT モデルの一つであるラッシュモデル[17, 18]に基づいて難易度を推定し、難易度を含んだデータセットを構築する手法を提案している。
ラッシュモデルに基づいて難易度付きの問題データセットを構図 1: 答え抜き出し形式の問題生成(既存研究でのタスク)と多枝選択式問題生成(本研究でのタスク)築する手順は次のとおりである。
まずデータセット中の各問題 𝒒𝑖を性能の異なる多数の QA システムに出題し、正誤反応データを取得する。
なお、本来は人間の正誤反応データを取得すべきであるが、これには膨大なコストがかかるため、ここでは QA システムを仮想的な学習者とみなして代用している。
次に、このデータからラッシュモデルによって各問題の難易度 𝑏𝑖を推定し、元々のデータセットに 𝑏𝑖を加えることで、難易度を含んだデータセットを構築している。
このように構築した難易度付きデータセットを用いて、読解対象文から所望の難易度に沿った答えを抽出する BERT モデル[19]と読解対象文と答え、および所望の難易度から問題文を生成する T5 モデル[20]を構築することで、難易度調整可能な答えと問題文の生成を行う。
しかし、この手法では多枝選択式問題を生成できない。
また、3.1 で詳述するように難易度を所与とした問題単語列の尤度を最大化するように問題生成モデルが訓練されており、難易度調整の精度を最適化していないため、難易度調整精度に改善の余地が残る。
3 提案手法以上の問題を解決するために、本研究では、Tomikawa et al. の枠組みをベースに、自己回帰型の深層学習モデルを用いた難易度調整可能な多枝選択式問題生成手法を開発するとともに、DPO を用いて難易度調整精度を最大化する手法を開発する。
3.1 教師あり学習に基づく訓練本研究では、図 1 右のように、読解対象文 𝒄 と難易度 𝑏 を入力すると、難易度に応じた問題文 𝒒 と正答選択枝 𝒂、および 3 つの誤答選択枝 𝒅1, 𝒅2, 𝒅3を生成する手法を提案する。
具体的には、表 1 の入力 𝒙の形式で読解対象文 𝒄 と難易度 𝑏 を与えると、表 1の出力 𝒚 の形式で問題文 𝒒、正答選択枝 𝒂，3 つの誤答選択枝 𝒅1, 𝒅2, 𝒅3を生成するように、Llama 2 [15]を学習することで実現する。
なお、表中の { } 内は実際のデータで置き換えられることに注意する。
また、一般的なデータセットには難易度 𝑏 は含まれていないため、Tomikawa et al. と同様の手順で難易度を付与したデータセットを構築する。
難易度調整可能な多枝選択式問題生成を実現するために、提案手法では以下の負の対数尤度を損失関数として最小化するようにモデルの訓練を行う。
Lsft= −𝔼( 𝒙,𝒚 )∼Dlog 𝜋( 𝒚|𝒙, 𝝓sft)(1)ここでDは {𝒄𝑖, 𝒒𝑖, 𝒂𝑖, 𝒅𝑖1, 𝒅𝑖2, 𝒅𝑖3, 𝑏𝑖|𝑖 ∈ 1, . . . , 𝐼 } からなるデータセットであり、𝜋 は Llama 2，𝝓sftは訓練対象のパラメータ、𝜋(𝒚|𝒙, 𝝓sft)は 𝒙 で条件付けされた出力文字列 𝒚 の尤度を表す。
この訓練を行うことによって、与えられた読解対象文と難易度に沿った問題文と選択枝を 𝒚 の形式で生成できるようになる。
3.2 難易度調整精度を目的関数とした追加

訓練

上記の教師あり学習に基づく訓練を行うことで、指定した難易度を反映した多枝選択式問題生成が可能となる。
しかし、この訓練では難易度調整精度を直接的に最大化していない。
そこで本研究では、上記の教師あり学習で訓練された問題生成モデルを、難易度調整精度を最大化するように追加訓練する手法を提案する。
この訓練のために、本研究では近年の LLM の追加学習手法として広く利用されている[21, 22]，DPO [14]を利用する。
DPO を用いた提案手法における損失関数は次のとおりである。
Ldpo= −𝔼( 𝒙,𝒚𝑤,𝒚𝑙)∼Ddpolog 𝜎𝛽 log𝜋( 𝒚𝑤|𝒙, 𝝓dpo)𝜋( 𝒚𝑤|𝒙,ˆ𝝓sft)−𝛽 log𝜋( 𝒚𝑙|𝒙, 𝝓dpo)𝜋( 𝒚𝑙|𝒙,ˆ𝝓sft)(2)表 1: 提案モデルの入力に対応するプロンプトの構造と出力の構成入力(𝒙): Create a question and 4 options with a diﬃculty level of {𝑏} based on the Context. Option 1 is the correct answer and Option 2, 3and 4 are the distractor options. Diﬃculty level -3.0 is the easiest and 3.0 is the most diﬃcult. The output format is “⟨c⟩{Option 1 (CorrectOption)}⟨q⟩{Question}⟨d1⟩{Option 2 (Distractor Option)}⟨d2⟩{Option 3 (Distractor Option)}⟨d3⟩{Option 4 (Distractor Option)}”### Context: {Context}⟨l⟩出力(𝒚 ): ⟨c⟩ {𝒂} ⟨q⟩ {𝒒} ⟨d1⟩ {𝒅1} ⟨d2⟩ {𝒅2} ⟨d3⟩ {𝒅3}ここで、𝜎 はシグモイド関数、ˆ𝝓sftは 1 段階目の教師あり学習で推定されたモデルパラメータ、𝝓dpoは学習されるモデルパラメータを表し、ˆ𝝓sftを初期パラメータとして訓練が進められる。
𝒚𝑤は 𝒚𝑙よりも難易度調整精度が高い問題を表し、Ddpoは DPO のための訓練データセットDdpo= {𝒙𝑖, 𝒚𝑤𝑖, 𝒚𝑙𝑖|𝑖 ∈ 1, . . . , 𝐼}である。
このような訓練データセットDdpoは，𝑖 番目の問題データから表 1 の形式で入力 𝒙 と出力 𝒚𝑤を構築し，𝑖 番目の読解対象文と同じ読解対象文だが異なる難易度の問題データから表 1 の形式で 𝒚𝑙を構築することで得られる。
4 提案手法の有効性評価実験本章では、提案手法の有効性を評価する実験について述べる。
4.1 実験手順多枝選択式読解問題生成の研究では、モデルの学習や性能評価に RACE データセット[23]が広く用いられているため、本実験でも RACE データセットを評価に用いる。
具体的な実験手順は次の通りである1. QA システムの構築：RACE の検証データセットを用いて性能の異なる 69 個の QA システムを構築した。
具体的には、まず 10 個の事前学習済み深層学習モデルを訓練データ数を変えて訓練することで、能力の異なる 100 個の QAシステムを構築した。
次に、性能が極端に低いQA システムのデータはノイズとなり得るため、訓練データセット中の問題に対する正答率が30%以上の QA システムのみを採択した。
その結果、69 個の QA システムが残った。
2. 難易度を付与した訓練データセットの構築：得られた 69 個の QA システムに訓練データセット中の全ての問題を出題し、その正誤反応データを用いてラッシュモデルで各問題の難易度 𝑏𝑖を推定した。
推定した難易度を訓練データセットに加えることで、難易度を付与した訓練データセットを構築した。
なお、4.2 節での分析のために各 QA システムの能力値も推定した。
3. 難易度調整可能な多枝選択式問題生成モデルの作成：3.1 節で述べたように Llama 2 に対して、読解対象文と難易度を入力すると正答選択枝・問題文・3 つの誤答選択枝を出力するモデルを設計し、教師あり学習に基づく訓練を行うことで難易度調整可能な多枝選択式問題生成モデルを作成した。
以降ではこのモデルを 𝑄𝐺sftと呼ぶ。
4. 難易度調整精度を目的関数とした追加訓練：𝑄𝐺sftに対して 3.2 節で述べた DPO による追加訓練を行うことで、難易度調整精度を向上させた問題生成モデルを作成した。
以降ではこのモデルを 𝑄𝐺dpoと呼ぶ。
5. 様々な難易度を指定した問題生成：RACE テストデータセット中の 300 個の読解対象文に対して，-3.0 から 3.0 まで 0.1 刻みで難易度を指定して、表 1 の入力 𝒙 の形式に沿った累計 18,300 個の入力データを作成し、それらを 𝑄𝐺sft，𝑄𝐺dpoにそれぞれ入力することで問題を生成した。
次節では、ここで生成された問題群の難易度を評価する。
4.2 生成した問題の難易度調整精度評価ここでは提案手法の難易度調整精度を評価する。
上記の手順 5 で生成した問題群の難易度を確認するために、生成された問題群に対する 69 個の QAシステム群の正誤反応データからラッシュモデルを用いて推定された難易度と、問題生成時に指定した難易度との関係を分析した。
結果を図 2 に示す。
図の横軸は指定した難易度、縦軸は生成した問題群に対して推定された難易度の平均を表す。
黒線は原点を通る傾き 1 の直線で、黒線に近いほど指定した難易度と生成した問題群の難易度が近いことを表す。
青点は 𝑄𝐺sftが生成した問題群に対する難易度、橙点は 𝑄𝐺dpoが生成した問題群に対する難易度を表す。
この図より、𝑄𝐺sft，𝑄𝐺dpo共に指定した難易度が高くなるほど生成する問題の難易度も高くなることがわかる。
また二つの生成手法を比較すると、青図 2: 生成する際に指定した難易度と実際に生成された問題の難易度の関係点より橙点の方が黒線に近いことから、𝑄𝐺dpoの方が指定した難易度に近い難易度の問題群を生成できていることがわかる。
生成された問題群に対する難易度推定値と生成時に指定した難易度との平均絶対誤差は、𝑄𝐺sftが 1.75 ，𝑄𝐺dpoが 1.17 であり、この結果からも、𝑄𝐺dpoが指定した難易度により近い問題を生成できていることがわかる。
したがって、提案手法によって難易度を調整した多枝選択式の問題生成が可能であるとともに、DPOを用いた追加訓練を行うことによって難易度調整精度が向上することが確認できた。
4.3 フューショット学習との比較LLM は、少数の入出力例を入力プロンプトに含めたフューショット学習によって様々なタスクを高精度に解くことができる[24]ため、プロンプトで難易度を指定することで難易度調整を実現できる可能性がある。
そこで本研究では、GPT-4o mini を用いて難易度調整を目的としたフューショット学習による問題生成を行い、提案手法と難易度調整精度の比較を行った。
具体的な実験手順は次の通りである。
1. 難易度を付与した RACE の訓練データセット中から難易度が -2.0, 0.0, 2.0 の問題をランダムに一つずつ選び、それぞれのデータに対応する入出力 𝒙, 𝒚 を作成した。
これらを 1 ショット分の例示プロンプトとみなし、同様の手順で 3ショット、 10 ショット、 30 ショット分の例示プロンプトを作成した。
2. RACE のテストデータセットからランダムに表 2: 提案手法とフューショット学習の難易度調整の比較指定した難易度𝑄𝐺sft𝑄𝐺dpoフューショット学習3ショット 10ショット 30ショット-2.0 -0.32 -1.59 -1.46 -1.30 -0.910.0 0.24 0.47 -1.67 -1.10 -0.752.0 0.47 1.55 -1.19 -1.34 -0.79100 個の読解対象文を選択し、各読解対象文に対して難易度を-2.0, 0.0, 2.0 とそれぞれ指定して表 1 の入力 𝒙 の形式で累計 300 個の指示プロンプトを作成した。
3. 手順 1 の各例示プロンプトと手順 2 の指示プロンプトをそれぞれ結合して GPT-4o mini に入力することで、各ショットに対応した問題生成を行った。
これらの生成した問題群を 4.1 節の手順 1 で作成した 69 個の QA システムに解答させ、正誤反応データからラッシュモデルに基づいて難易度を推定した。
表 2 に、この手続きで生成された問題群に対する難易度推定値の平均を、生成時に指定した難易度別に示す。
表には、4.1 節の実験で 𝑄𝐺sft，𝑄𝐺dpoにそれぞれの難易度を指定して生成された問題群に対する難易度推定値の平均も示す。
表より、𝑄𝐺sftや 𝑄𝐺dpoでは指定した難易度が高くなるほど生成した問題の難易度も高くなるのに対し、フューショット学習では指定した難易度と生成した問題の難易度にそのような関係を確認できない。
このことは、IRT の尺度に基づく難易度調整機能を獲得させるためにはフューショット学習では不十分であり、提案手法のようなモデル学習の枠組みが必要であることが示された。
5 おわりに本研究では、自己回帰型の深層学習モデルであるLlama 2 を用いた難易度調整可能な多枝選択式問題生成手法を開発するとともに、DPO により難易度調整精度を最大化する訓練手法を提案した。
RACEデータセットを用いた評価実験により、提案手法では難易度を調整した多枝選択式の問題生成が可能であるとともに、DPO を用いた追加訓練を行うことによって難易度調整精度が向上することを確認した。
さらに、IRT の尺度に基づく難易度調整機能を獲得させるためにはフューショット学習では不十分であり、提案手法のようなモデル学習の枠組みが必要であることも示された。



謝辞

本研究は JSPS 科研費 24H00739，21H00898，19H05663 の助成を受けたものです。

参考文献


[1] Xinya Du, Junru Shao, and Claire Cardie. Learning to ask:Neural question generation for reading comprehension. InProceedings of the 55th Annual Meeting of the Asso-ciation for Computational Linguistics, pp. 1342–1352,2017.
[2] Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, HangboBao, and Ming Zhou. Neural question generation from text:A preliminary study. In Proceedings of the 12th NaturalLanguage Processing and Chinese Computing, pp.662–671, 2018.
[3] Ruslan Mitkov and Le An Ha. Computer-aided generationof multiple-choice tests. In Proceedings of the HLT-NAACL 03 workshop on Building educational appli-cations using natural language processing, NorthAmerican Chapter of the Association for Computa-tional Linguistics, pp. 17–22, 2003.
[4] Andreea Dutulescu, Stefan Ruseti, Denis Iorga, Mihai Das-calu, and Danielle S McNamara. Beyond the obviousmulti-choice options: Introducing a toolkit for distractorgeneration enhanced with NLI ﬁltering. In Proceedingsot the 25th International Conference on Artiﬁcial In-telligence in Education, pp. 242–250, 2024.
[5] Subhankar Maity, Aniket Deroy, and Sudeshna Sarkar. Anovel multi-stage prompting approach for language agnos-tic MCQ generation using GPT. In Proceedings of the46th European Conference on Information Retrieval,pp. 268–277, 2024.
[6] Maomi Ueno and Yoshimitsu Miyazawa. IRT-based adap-tive hints to scaﬀold learning in programming. IEEETransactions on Learning Technologies, Vol. 11,No. 4, pp. 415–428, 2017.
[7] Ghader Kurdi, Jared Leo, Bĳan Parsia, Uli Sattler, andSalam Al-Emari. A systematic review of automatic ques-tion generation for educational purposes. InternationalJournal of Artiﬁcial Intelligence in Education, Vol. 30,pp. 121–204, 2020.
[8] Yifan Gao, Lidong Bing, Wang Chen, Michael R Lyu,and Irwin King. Diﬃculty controllable generation of read-ing comprehension questions. In Proceedings of the28th International Joint Conference on Artiﬁcial In-telligence, pp. 4968–4974, 2018.
[9] Yi Cheng, Siyao Li, Bang Liu, Ruihui Zhao, Sujian Li,Chenghua Lin, and Yefeng Zheng. Guiding the growth:Diﬃculty-controllable question generation through step-by-step rewriting. In Proceedings of the 59th An-nual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Confer-ence on Natural Language Processing, pp. 5968–5978, 2021.
[10] Masaki Uto, Yuto Tomikawa, and Ayaka Suzuki.Diﬃculty-controllable neural question generation for read-ing comprehension using item response theory. In Pro-ceedings of the 18th Workshop on Innovative Use ofNLP for Building Educational Applications, Associa-tion for Computational Linguistics, pp. 119–129, 2023.
[11] 富川雄斗, 鈴木彩香, 宇都雅輝. 項目反応理論に基づく難易度調整可能な読解問題自動生成手法. 電子情報通信学会論文誌 D, Vol. J107-D, No. 2, pp. 53–66,2024.
[12] Yuto Tomikawa, Ayaka Suzuki, and Masaki Uto. Adaptivequestion–answer generation with diﬃculty control usingItem Response Theory and pre-trained transformer models.IEEE Transactions on Learning Technologies, 2024.
[13] Teruyoshi Goto, Yuto Tomikawa, and Masaki Uto. En-hancing diversity in diﬃculty-controllable question gener-ation for reading comprehension via extended T5. Proceed-ings of the 32nd International Conference on Computersin Education, 2024.
[14] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-pher D Manning, Stefano Ermon, and Chelsea Finn. Directpreference optimization: Your language model is secretlya reward model. In Proceedings of Advances in Neu-ral Information Processing Systems 36, pp. 53728–53741, 2023.
[15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.Llama 2: Open foundation and ﬁne-tuned chat models.arXiv:2307.09288, 2023.
[16] Frederic M Lord. Applications of item response the-ory to practical testing problems . Routledge, 2012.
[17] Georg Rasch. Probabilistic models for some intelli-gence and attainment tests. ERIC, 1993.
[18] WJ van der Linden. Handbook of Item Response The-ory: Volume 2: Statistical Tools. CRC Press, 2017.
[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. BERT: Pre-training of deep bidirectional trans-formers for language understanding. In Proceedings ofthe 2019 Conference of the North American Chapterof the Association for Computational Linguistics, pp.4171–4186, 2019.
[20] Colin Raﬀel, Noam Shazeer, Adam Roberts, KatherineLee, Sharan Narang, Michael Matena, Yanqi Zhou, WeiLi, and Peter J Liu. Exploring the limits of transfer learn-ing with a uniﬁed text-to-text transformer. Journal ofmachine learning research, Vol. 21, No. 140, pp. 1–67,2020.
[21] Albert Q Jiang and et al. Mixtral of experts.arXiv:2401.04088, 2024.
[22] Abhimanyu Dubey and et al. The Llama 3 herd of models.arXiv:2307.09288, 2024.
[23] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, andEduard Hovy. RACE: Large-scale reading comprehen-sion dataset from examinations. InProceedings of the2017 Conference on Empirical Methods in NaturalLanguage Processing, pp. 785–794, 2017.
[24] Tom B Brown and et al. Language models are few-shotlearners. arXiv:2005.14165, 2020.