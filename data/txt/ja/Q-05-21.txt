テキスト埋め込みからのテキスト復元における予測制御の援用の効果検証

三好優輝

1

, 宮岡祐弥

1

, 井上正樹

11

慶應義塾大学



概要

テキスト埋め込みから元のテキストを復元する方法を、 Vec2Text[1]とは別のアプローチで考えた。
そのアプローチは、 テキスト生成において予測制御の考えを用いる方法である。
予測制御によるテキスト生成を用いると、 Greedy Search によりテキスト生成した場合よりも性能が向上した。
具体的には、 2 つのテキストの内容が一致する割合が最大 0.120 だけ向上した。



1 序論

テキスト埋め込みモデルとは、 テキストの内容を単一のベクトルに抽出するエンコーダモデルである。
テキスト埋め込みでは、 2 つのテキストの内容の類似度計算が可能であり、 情報検索・質問応答・バイテキストマイニングなど様々な NLP タスクに利用されている[2].この抽出されたテキスト埋め込みから元のテキストを復元することは一般に難しい。
文献[1]では,テキスト埋め込みからテキストを復元する手法としてVec2Text という手法が提案された。
Vec2Text では、 2つのモデルが用いられ、 1 つ目のモデルでテキスト埋め込みから元のテキストの仮説を生成し、 2 つ目のモデルで生成したテキストの修正を行なう。
そして、 2 つ目のモデルにより、 テキストの修正を反復的に行なうことで、 高い精度で元のテキストを復元することが可能になっている。
本研究では、 Vec2Text とは異なるアプローチにより、 テキスト埋め込みからテキストを復元することを目指した。
具体的には、 Vec2Text の 2 つ目のモデルを使用せず、 1 つ目のモデルのテキスト生成方法に予測制御の考え方を用いる方法を考えた。
本稿の構成を示す。
2 章でテキスト埋め込みから元のテキストを復元する “テキスト復元モデル” の構築について、 3 章で予測制御の考え方を用いたテキスト生成方法とその効果について。
4 章で結論を述べる。
表記: 語彙の集合を 𝕍 とおき、 トークンは 𝑤 ∈ 𝕍で表す。
そして、 単一および複数のトークンの列をテキストと呼び、 𝑥 = {𝑤𝑖}𝑛𝑖=1∈ 𝕍𝑛と表す。
テキスト埋め込みモデルは関数 𝜙 : 𝕍𝑛→ ℝ𝑑で表す。


2 準備: テキスト復元モデルの構築

本章では、 テキスト埋め込みから元のテキストを復元するためのモデルとして構築したテキスト復元モデルについて述べる。
テキスト復元モデルは、テキスト埋め込み 𝒆ref∈ ℝ𝑑と生成中のトークン列{𝑤𝑖}𝑛𝑖=1∈ 𝕍𝑛を入力とし、 後続トークンの出現確率分布 𝑃 ∈ ℝ|𝕍 |を出力するモデルである。
2.1 節では、 テキスト復元モデルの訓練について、 2.2 節では、 構築したテキスト復元モデルの性能について述べる。



2.1 テキスト復元モデルの訓練

モデル構造構築したテキスト復元モデルのモデル構造を図 1 に示す.このモデルは,エンコーダ・デコーダモデルからエンコーダ部分を取り除いたものである。
そして、 エンコーダから交差注意機構への入力の代わりとして、 テキスト埋め込み 𝒆ref∈ ℝ𝑑を入力した。
ただし、 テキスト埋め込みの次元 𝑑 は、 デコーダ部分の潜在ベクトル 𝒉𝑖の次元と一致させる必要がある。
本研究では、 基盤のエンコーダ・デコーダモデルとして “sonoisa/t5-base- japanese”1）を使用し、 その潜在ベクトルと同次元のテキスト埋め込みを得られる “sonoisa/sentence-bert-base-ja-mean-tokens-v2”2）というテキスト埋め込みモデルを使用した。
1） https://huggingface.co/sonoisa/t5-base-japanese2） https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2

Tex tEmbedding𝒆!
"#∈ ℝ$Input Text𝑤%%&'(∈ 𝕍(SelfAttentionDecoderAdd & NormCrossAttentionAdd & NormFeedForwardAdd & Norm×𝑁LinearSoftmaxNext Toke n  P ro b ab ili tie s𝑃 ∈ ℝ𝕍𝒉%%&'(∈ ℝ(×$Hidden StatesTokenEmbedderPositionalEncoding図 1 構築したテキスト復元モデル表 1 訓練時のハイパーパラメータの設定ハイパーパラメータ値LoRA 低ランク行列 𝐴 の学習率 𝜌𝐴1.75 × 10−6LoRA 低ランク行列 𝐵 の学習率 𝜌𝐵𝜌𝐴× 24LoRA 低ランク行列 𝐴 のランク 64バッチサイズ 8エポック数 2訓練データ訓練データとして、 文献[3]で使用されている JSNLI データセット3）の訓練セットを使用した。
これは合意関係認識タスクの訓練に使用されるデータセットである。
本研究では、 データセット内の前提文と仮説文の合計 106 万 6010 個のテキストを用いて、 テキスト埋め込み 𝒆refとトークン列{𝑤𝑖}𝑛𝑖=1の２入力と、 後続トークン 𝑤𝑛+1の出力のデータセットを構築した。
訓練方法テキスト復元モデルの訓練には、LoRA[4]を用いた。
デコーダ内の自己注意機構・交差注意機構の全ての線形層に LoRA 層を挿入し、LoRA 層のパタメータのみを訓練した。
訓練時のハイパーパラメータの設定を表 1 に示す。
本研究では、LoRA の低ランク行列 𝐴, 𝐵 を異なる学習率で訓練する LoRA+[5]という手法を使用した。
3） https://huggingface.co/datasets/llm-book/jsnli

2.2 構築したモデルの性能検証

本節では、 2.1 節で構築したテキスト復元モデルの性能について述べる。
検証方法構築したテキスト復元モデルの性能として、 Greedy Search により 1 からテキスト復元を行なった場合に、 どれだけのテキストをどれほどの精度で復元できるのかを調べた。
本研究では、 生成されたテキスト 𝑥genが元のテキストとどの程度異なるかの指標として、 生成テキストのテキスト埋め込み 𝜙(𝑥gen)と目標のテキスト埋め込み 𝒆refのユーグリッド距離𝑑2(𝜙(𝑥gen), 𝒆ref) ≔ ∥𝜙(𝑥gen) − 𝒆ref∥2(1)を用いた4）.検証データには、 日本語言語理解ベンチマークJGLUE[6]の JSTS データセットの検証セットを用いた。
JSTS は 2 つのテキストの意味的類似度計算のデータセットであり、 合計 2914 個のテキストを検証に使用した。
そして、 それぞれのテキストのテキスト埋め込み 𝒆refから元のテキストをどの程度復元できるかを検証した。
検証結果まず、 構築したテキスト復元モデルの性能について述べる前に、 (1)式で定義した指標𝑑2(𝜙(𝑥gen), 𝒆ref)の値とテキストの復元度の関係について調べた。
テキスト距離 𝑑2(𝜙(𝑥gen), 𝒆ref)の振る舞いの例を表 2 に示す。
表 2 より、 2 つのテキストの内容が類似しているときほど、 テキスト距離𝑑2(𝜙(𝑥gen), 𝒆ref)が小さくなることが確かめられた。
そして、 テキスト距離 𝑑2(𝜙(𝑥gen), 𝒆ref)が 5 以下であれば、 おおよそ 2 つのテキストの内容は一致することが分かった。
構築したテキスト復元モデルにより生成されたテキスト 𝑥genと元のテキストの距離 𝑑2(𝜙(𝑥gen), 𝒆ref)の分布を図 2 に示す。
テキスト距離の平均 𝔼(𝑑2)は13.0 であり、 テキスト距離が 5 以下となる割合 𝑝𝑑2≤5は 0. 041 となった。



3 テキスト復元モデルの出力制御

本章では、 2 章で構築したテキスト復元モデルの性能を向上させるためのテキスト復元モデルの出力制御について述べる。
まず、 3.1 節で、 予測制御の考え方を用いたテキスト生成のアルゴリズムについて説4） NLP においては、 ベクトルの違いの指標としてコサイン類似度を用いることが多いが、 本研究ではユーグリッド距離を使用した。

表 2 テキスト距離 𝑑2(𝜙(𝑥gen), 𝒆ref)の値の例元のテキスト生成テキスト 𝑥gen𝑑2(𝜙(𝑥gen), 𝒆ref)子供たちが子供たちが0.0遊んでいます。
遊んでいます。
スケートスケート2.42ボーダーがボーダーは手すりを手すりを滑っています。
滑っています。
3 頭の牛が草の列に5.29草の上に。
3 頭の牛が並んで立って立っています。
います。
棚の中に棚に犬が7.03犬がいます。
棚の棚を棚棚の中にいます。
壁の前にテーブルの前に10.0ソファーとソファーがテーブルがあります。
置いてあります。
図 2 構築したテキスト復元モデルの性能明する。
そして、 3.2 節では、 図 2 の結果をベースラインとし、 テキスト生成方法によってどれだけ性能が向上するかを検証する。

3.1 予測制御を用いたテキスト生成

予測制御とは、 将来の出力を予測しながら、 現在の行動を決定する制御手法である.本研究では,予測制御の考え方を用いたテキスト生成によりテキスト復元の精度を上げることを考えた。
パラメータ予測制御には主に 2 つのパラメータがある。
一つ目は予測ホライゾン 𝐻Pであり、 現在から何ステップ先までの出力を予測するかに対応している。
二つ目は制御ホライゾン 𝐻Cであり、 得られた予測から何ステップ先までの行動を決定するかに対応している。
本研究では、 テキスト生成に予測制御を用いる上で、 予測ホライゾンと制御ホライゾンに加えて、 新たに予測ブランチ列 𝒃 ∈ ℕ𝐻Pとエージェント数 𝑁aとPredictive Horizon𝐻!
= 5Generation Step𝑑!
𝜙 𝑥"#$, 𝒆%#&3Generation Step𝑑!
𝜙 𝑥"#$, 𝒆%#&Control Horizon𝐻"= 2?
1. Prediction Step2. Control Step111222111111111111Predictive Branch Sizes𝒃 = 3, 1, 2, 1, 1: Token図 3 予測制御的なテキスト生成の概要(1)Generation Step𝑑!
𝜙 𝑥"#$, 𝒆%#&2Generation Step𝑑!
𝜙 𝑥"#$, 𝒆%#&?
1. Prediction Step2. Control Step11111111111111Agent num𝑁!
= 2112?
𝟏𝟐𝟏𝟐図 4 予測制御的なテキスト生成の概要(2)いうパラメータを導入した。
まず、 予測ブランチ列𝒃 = [𝑏1, 𝑏2, ..., 𝑏𝐻P]の概要を図 3 に示す。
予測ブランチ列 𝒃 の 𝑖 列目の要素 𝑏𝑖は、 現時点から 𝑖 ステップ先で予測する分岐路の数に対応している。
次に、 エージェント数 𝑁aの概要を図 4 に示す。
エージェント数𝑁aは、 行動決定の際に候補として残しておく路の数に対応している。
アルゴリズム本研究のテキスト生成アルゴリズムは、 図 3, 4 のように予測ステップと制御ステップの 2 つで構成される。
このアルゴリズムの概要について説明する。
予測ステップでは、 入力トークン列から予測ブランチ列 𝒃 に従い、 トークン列を分岐させて推論す

表 3 各メソッドのパラメータ設定メソッド名 𝐻P𝐻C𝒃 𝑁a𝑁a×∏𝑏𝑖∈𝒃𝑏𝑖ベースライン 1 1 [1] 1 1A 1 1 [3] 1 3B 50 1 [3, 1, 1, ..., 1] 1 3C 50 1 [3, 1, 1, ..., 1] 2 6D 50 1 [4, 1, 1, . .., 1] 2 8E 50 1 [3, 1, 1, ..., 1] 3 9F 50 1 [4, 1, 1, ..., 1] 3 12G 50 1 [8, 1, 1, . .., 1] 1 8H 50 1 [4, 1, 1, . .., 1] 2 8I 50 1 [3, 2, 1, ..., 1] 1 6J 50 1 [3, 2, 1, ..., 1] 2 12る。
ただし、 𝑖 ステップ先のトークンの推論では、 出現確率が上位 𝑏𝑖までのトークンを選択する。
そして、 最終的に、 トークン長 𝐻Pの後続トークン列を計𝑁a×∏𝑏𝑖∈𝒃𝑏𝑖個だけ推論する。
制御ステップでは、 予測ステップで得られた計𝑁a×∏𝑏𝑖∈𝒃𝑏𝑖個のトークン列から、 候補として残すトークン列を 𝑁a個だけ選択する。
ここで、 候補として残すトークン列には、 テキスト距離 𝑑2(𝜙(𝑥gen), 𝒆ref)が小さいものを順に選択する。
そして、 選択したそれぞれのトークン列から、 𝐻Cステップ先までのトークン列を生成する。
この予測ステップと制御ステップを繰り返すことにより、 𝑁a個のテキストが生成できる。
そして、 最終的に、 𝑁a個のテキストの中でテキスト距離𝑑2(𝜙(𝑥gen), 𝒆ref)が最小になるテキストを生成する。


3.2 予測制御によるテキスト生成の効果

3.1 節で、 予測制御を用いたテキスト生成について述べた。
本節では、 その効果について述べる。
実験設定メソッド A 〜 J の 10 個のメソッドを考えた。
各メソッドのパラメータ設定を表 3 に示す。
ここで、 𝑁a×∏𝑏𝑖∈𝒃𝑏𝑖は予測ステップにおいて推論するテキストの総数であり、 テキスト生成にかかる計算量に対応している。
また、 ベースラインのパラメータ設定は、 2.2 節の検証の際の設定と同じである。
この実験では、 出力されるトークンの数は最大で50 個となるように設定した。
また、 予測ステップにおいて、 出現確率が 0.9 以上のトークンが存在する場合は、 その生成ステップでは分岐をせず推論した。
実験結果各メソッドにおけるテキスト距離の平均 𝔼(𝑑2), テキスト距離が 5 以下となる割合 𝑝𝑑2≤5を表 4 に示す。
表 4 より、 最良の性能を示したメソッドは F であった。
メソッド F ではテキスト距離が 5 以下となる割合 𝑝𝑑2≤5は 0.161 となり、 ベースラインよ表 4 各メソッドの効果の結果メソッド名 𝔼(𝑑2) 𝑝𝑑2≤5ベースライン 13.0 0.041A 10.7 0.056B 9.3 0.107C 8.8 0.126D 8.4 0.147E 8.5 0.144F 8.1 0.161G 8.1 0.155H 8.4 0.147I 10.0 0.080J 9.1 0.103図 5 メソッド F の性能りも 0.120 だけ高くなった。
メソッド F のテキスト距離 𝑑2(𝜙(𝑥gen), 𝒆ref)の分布を図 5 に示す。
ベースラインの分布である図 2 と比較すると、 メソッド F では分布が大きく左に寄り、 テキスト距離が 1 以下となる割合 𝑝𝑑2≤1は約 0. 02 だけ高いことが分かった。
そして、 メソッド A 〜 F より、 𝑁a×∏𝑏𝑖∈𝒃𝑏𝑖が大きいほど、 おおよそ性能が向上した。
ただし、 メソッドD と E を比較すると、 𝑁a×∏𝑏𝑖∈𝒃𝑏𝑖が小さい D の方が E よりも性能が高くなった。
また、 𝑁a×∏𝑏𝑖∈𝒃𝑏𝑖の値が同じメソッド G と H を比較すると、 1 ステップ目の予測ブランチ 𝑏1が大きい G の方が、 エージェント数 𝑁aが大きい H よりも性能が高くなった。
また、 メソッド B, C と I, J を比較すると、 2 ステップ目の予測ブランチ 𝑏2が大きい I, J の方が B, C よりも性能が低くなった。



4 結論

予測制御によるテキスト生成を用いると、 GreedySearch よりもテキスト距離が 5 以下となる割合が最大 0.120 だけ向上した。
また、 予測ステップで推論するテキスト数が同じ場合、 1 ステップ目の予測ブランチ 𝑏1が大きい方が、 エージェント数 𝑁aが大きいものよりも性能が高くなることが分かった。



参考文献


[1] John Xavier Morris, Volodymyr Kuleshov, VitalyShmatikov, and Alexander M Rush. Text embeddings reveal(almost) as much as text. In The 2023 Conference onEmpirical Methods in Natural Language Processing,2023.
[2] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,Rangan Majumder, and Furu Wei. Improving text em-beddings with large language models. arXiv preprintarXiv:2401.00368, 2023.
[3] 山田育矢ほか. 大規模言語モデル入門. 株式会社技術評論社, 2023.
[4] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, YuanzhiLi, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In InternationalConference on Learning Representations.
[5] Souﬁane Hayou, Nikhil Ghosh, and Bin Yu. Lora+: Eﬃ-cient low rank adaptation of large models. In Forty-ﬁrstInternational Conference on Machine Learning.
[6] 栗原健太郎, 河原大輔, 柴田知秀. Jglue: 日本語言語理解ベンチマーク. 自然言語処理, Vol. 30, No. 1, pp.63–87, 2023.