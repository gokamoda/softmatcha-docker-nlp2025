ReShape Attention による音声と言語の基盤モデルの統合

 叶高朋

1

 小川厚徳

1

 デルクロア・マーク

1

 チェン・ウィリアム

2

福田りょう

1

松浦孝平

1

芦原孝典

1

渡部晋治

21

日本電信電話株式会社 

2

カーネギーメロン大学



{takatomo.kanou, atsunori.ogawa, marc.delcroix, }@ntt. com



{ryo.fukuda, kohei.matsuura, takanori.ashihara}@ntt. com



概要

本論文では、 音声翻訳システムにおいて、 音声基盤モデルである Whisper の分散表現を言語基盤モデルである LLaMA2 の分散表現と結合する ReShapeAttention (RSA)を提案する。
RSA は、 LLaMA2 のTransformer 層の内部に挿入され、 音声とテキストの分散表現を、 同じ特徴次元のサブベクトルに変形し、 2 つの分散表現間で Cross-Attention を実行する。
RSA により、 LLaMA2 と Whisper の勾配グラフは接続され、 音声翻訳システム全体を入力音声に対して最適化できる。
RSA は、 Whisper と LLaMA2 を用いたCascade 音声翻訳システムと比較して、 BLEU スコアを相対的に 8. 5% 向上させた。
さらに、 RSA は正解の書き起こしが得られる場合において Cascade システムよりも音声翻訳精度を向上させる可能性があることが示された。


1 はじめに

基盤モデルは、 大規模なデータで学習された巨大なモデルで、 プロンプトに基づいて、 さまざまな異なるタスクを高い精度で実行できる[1, 2, 3]. また、 音声翻訳のような専門的な下流タスクシステムを開発するための事前学習モデルとして利用することもできる。
音声基盤モデル Whisper と言語基盤モデルLLaMA2 を連結することで、 音声認識と機械翻訳のCascade 型音声翻訳システムを構築できる。
しかし、Cascade システムでは、 Whisper による音声認識誤りの影響を受ける。
加えて、 LLaMA2 は韻律や抑揚、 話者の性別など、 音声から得られる非言語情報を利用することができない。
このような非言語情報は、 性差のある言語の翻訳やセグメントを学習するのに役立つことが知られている[4, 5]. 一方で、 End-to-end(E2E)システムは入力音声を直接翻訳する。
そのため、 音声認識誤りに頑健で、 非言語情報も学習することが可能である。
しかし、 E2E モデルは多くの原言語音声と目的言語テキストのペアデータを学習に必要とする[6, 7].第3の音声翻訳システムとしてHybrid システムがある。
Hybrid システムは、 Cascadeと E2E の両方の長所を得ることを目的とし、 音声と書き起こしの両方を入力として翻訳を行う。
先行研究では、 音声基盤モデルと言語基盤モデルを結合する Cross-Attention 型の接続アダプターが提案された[8]. この接続アダプターは、 学習可能なパラメターを持ち、 基盤モデルの各層に挿入される。
そのため、 基盤モデルが多くの層数を持つ場合、 接続アダプターのパラメータ数も膨大となる。
そして、 多くのパラメータを基盤モデルに挿入することとなり、 破滅的忘却の発生や学習を難しくする可能性がある。
本研究では、 Hybrid システムに焦点を当て、 先行研究の問題を解決するために ReShape Attention（RSA）を提案する。
RSA は異なる次元数の分散表現を写像を行うことなく統合する。
初めに、 RSA では音声基盤モデルと言語基盤モデルの分散表現を同じ次元数の Sub-vector へ分解する。
そして、 Sub-vector レベルで Cross-Attention を計算し、 ２つの分散表現を統合する。
この時、 RSA は写像を行わないため学習可能なパラメータを持たない。
ゆえに、 基盤モデルの層数が増加しても、 学習可能なパラメータ量を増やすことなく基盤モデルを結合可能である。
本研究では、 RSA の有効性を音声翻訳タスクで検証した。
音声基盤モデルには Distil-Whisper [9]を、 言語基盤モデルには LLaMA2-7B [10]を利用した。
音声翻訳のデータセットは MuST-C [11]を用いた。
RSAは Cascade システムや、 先行研究の接続アダプターを持ちいた E2E, Hybrid システムよりも少ないパラメータにも拘わらず、 BLEU スコアを相対的に 8. 5%向上させた。
さらに、 Cascade システムと Hybrid システムにおいて仮に正しい音声書き起こしが得られた場合の性能を比較した。
分析の結果、 接続アダプター

が
音声認識誤りの軽減のみならず、 非言語情報の学習にも有効であることを確認できた。


2 音声翻訳システム

本研究では、 後段の言語処理器への入力が異なる3 つの音声翻訳システムを扱う。
Cascade, E2E, Hybridシステムをそれぞれ図 1 に示す。
Cascade システムは入力音声 𝑋 を変換し、 音声の分散表現 Ex∈ ℝ𝐿×𝐷xと、 書き起こしテキスト 𝑌 ∈V𝑁を得る。
ここで、 𝐿は分散表現の長さ、 𝐷xは次元数、 𝑁 はテキスト長、Vは音声基盤モデルの辞書サイズを表す。
そして、 言語基盤モデルは 𝑌 を入力として受け取り、 自身のトークナイザで再分割して翻訳を行う。
E2E システムでは、 音声の分散表現 Exを言語基盤モデルに入力し翻訳を行う。
そのため、 Exの次元数を言語基盤モデルの次元数 𝐷yを合わせる写像が必要である。
一般的には 𝐷x≠ 𝐷yとなる。
この変換で得られる分散表現を接続分散表現 Eo∈ ℝ𝐿×𝐷yと呼ぶ。
言語基盤モデルは Eoを入力として翻訳を行う。
本研究では、 E2E システムの接続アダプターには Bottleneck 層[12](図 2 (a)) , または、 Transformer層[13]
(SLM)[14](図 2 (b))を用いた。
Bottleneck 層はReLU 活性関数を持つ 2 層のニューラルネットワークである。
最後に Hybrid システム[8]では、 図 1 (c)に示すように、 音声の分散表現 Exとテキストの分散表現Ey∈ ℝ𝑁 ×𝐷yを入力として受け取り内部で統合する。
𝑁 はテキストの長さを表す。
Radhakrishnan らは、Multi-head attention (MHA)を用いて次のように統合を行った[].Ec= Ey+ 𝑓 (Ex, Ey; 𝜃)|  {z  }≜Eo∈ ℝ𝑁 ×𝐷y. (1)ここで、 Ecは、 統合された分散表現を表す。
𝑓 (·)は、 統合機構を表しパラメータ 𝜃 に基づいて、 異なる２つの分散表現を写像し次元数をそろえ、 Cross-Attentionを行う。
そして、 Ecを出力層（FFN）で変換し、 言語の分散表現に加算して接続分散表現 Eoを生成する[8].MHA [8]では統合機構 𝑓 (·)は以下のようになる、Eo= MultiHead(Q, K, V) = [S1, . . . , S𝐻]Wo, (2)where Sℎ= Attention(QWQℎ, KWKℎ, VWVℎ), (3)ここで Attention(·)は注意機構、 MultiHead(·)は MHAである。
Query として(Q = Ey)を、 Key と Value として、 それぞれ(K = Ex, V = Ex)を用いる。
なお、Sℎ∈ ℝ𝑁 ×𝐷は、 MHA の第 ℎ 番目の attention head, 𝐻は head 数を表し、 WQℎ∈ ℝ𝐷y×𝐷, WKℎ, WVℎ∈ ℝ𝐷x×𝐷,Wo∈ ℝ𝐷y×𝐷yは、 それぞれ Query, Key, Value, 統合分散表現の写像重みを表す。
これらの写像行列 WQℎ,WKℎ, WVℎは、 音声と言語の分散表現の次元数を共通の次元数 𝐷 = 𝐷y/ 𝐻 に揃える役割を担う。
MHA アダプターの写像行列 WQℎ, WKℎ, WVℎは、 ランダムに初期化され、 音声と言語の分散表現(ExとEy)は学習の初期段階においてランダムに写像されるため大幅な変更を受ける。
このようなにランダムな写像によって変換された分散表現は、 学習を不安定にし、 学習初期の損失値を大きく増加させる。
そのため不必要に基盤モデルのパラメターを更新し破滅的忘却を引き起こす可能性がある.加えて,Cross-Attention 型の接続アダプターは基盤モデルの各層に挿入されるため、 基盤モデルの大きさに比例して追加される学習パラメータが多くなり、 学習が困難となる。
これは、 年々巨大化する基盤モデルにおいて、 深刻化な問題となりうる[15].

3 ReShape attention による接続

本研究で提案する ReShape attention（RSA）は、 音声の分散表現の情報を写像などで破壊することなく、 そのまま言語基盤モデルに受け渡すことを目標とする。
それにより、 言語基盤モデルは、 テキストの言語情報と音声の非言語情報の両方を考慮した音声翻訳を学習可能となる。
そこで、 写像行列を持たない接続アダプターを提案する。
統合機構 𝑓 (·)(式(1)）は、 音声の分散表現をテキストの分散表現と同じ長さ次元数に整形する。
この時、 本研究では写像を行うことなく、 時間・次元の両方向のアライメントを同時に行う。
この点は、 次元数の違いを写像行列 WQℎ,WKℎ, WVℎで解決する先行研究と違う点である。
まず、 初めに２つの分散表現を Exと Eyを共通の次元数¯𝐷 に変形する。
Reshape 機構 RS(·)は以下のように動作する。
RS(Ex;¯𝐷) ∈ ℝ𝐿 𝐻x×¯𝐷, RS(Ey;¯𝐷) ∈ ℝ𝑁 𝐻y×¯𝐷, (4)ここで、 共通の次元数¯𝐷 は 𝐷xと 𝐷yの最大公約数であり、𝐻x= 𝐷x/¯𝐷, 𝐻y= 𝐷y/¯𝐷. (5)となる。
そして、 Reshape された分散表現に対してAttention の計算を行う。
¯Eo= Attention(RS(Ey;¯𝐷), RS(Ex;¯𝐷), RS(Ex;¯𝐷)).(6)

Self-attentionFFN  LoRASelf-attentionFFN  LoRACross-attention<start of transcription> Self-attentionFFN  LoRASelf-attentionFFN  LoRACross-attentionBridgingTranscription Conv1D+GeLUEmbeddingBridgingEmbeddingTranscription  TranslationTranslationLoRAEmbeddingBridgingCross-attentionBridgingTrainable parametersLLaMA2LLaMA2WhisperSelf-attentionFFN  LoRAConv1D+GeLUWhisperEncoder(c) Hybrid ST　Whisper Encoder　Whisper Decoder　　　　　　　　　　　LLaMA2 Input speech (a) Cascade STConv1D+GeLUtranscriptionTranslationLoRALoRALoRA(b) E2E STInput speech Input speech 図 1 音声翻訳システムの概要図。
Self-attentionFFNWhisper embeddingBridged embedding for LLaMA2(b) Baseline:SLMfor E2E STWhisper embedding Bridged embedding for LLaMA2LinearLinearReLULLaMA2 emb. Whisper emb.Linear Linear LinearScaled Dot-productConcatLinearBridged embedding for LLaMA2(c) Baseline: MHAfor HybridSTLLaMA2 emb.Whisper emb.Scaled Dot-productBridged embedding for LLaMA2(d) Proposed: RSAfor Hybrid STReshape ReshapeReshapeProjection(a) Baseline:Bottleneckfor E2E STReshape図 2 接続アダプターの概要図。
式(6)の Attention(·)により、 接続分散表現¯Eoを変換されていない元の Exと Eyを用いて生成する。
最後に、¯Eoを(𝑁 × 𝐷y)に変形し直して、 式(1)を以下のように適用する。
Eo= 𝜂RS−1(¯Eo; 𝐷y) ∈ ℝ𝑁 ×𝐷y. (7)ここで、 RS−1(·; 𝐷y)は Reshape 機構 RS(·;¯𝐷)を逆変換を表す。
𝜂 ∈ ℝ は、 学習可能なスカラーパラメターで、 異なる分散表現のダイナミックレンジを調整する役割を担い、 初期値は 0 である。
MHA と比べ、ReShape Attention (RSA)は写像行列を用いず次元数の違いを吸収する。
そして、 𝜂 ∈ ℝ により、 段階的に音声の分散表現を加算していくため学習開始時点の損失値を Cascade システムと同様に抑え学習を安定させることができる。

4 実験

本研究では、 MuST-C コーパスを用いて英独音声翻訳実験を行った[11]. MuST-C に含まれる既存のテストセットである “tst-HEと “tst-COMMONを結合して“Original”テストセットとした.またMuST-Cよく知られた音声翻訳データであり、 TED Talk から作成されているため、 WEB から学習データを収集する基盤モデルにおいて、 すでに学習データとして漏洩している可能性が高い。
そこで、 より厳密に評価を行うため、 基盤モデルリリース後に公開された TedTalk1. 5 時間を用いて “New” テストセットを作成して評価に用いた。
1）

4.1 モデル設計

モデル作成は ESPnet [16]を用いた。
音声基盤モデルとして Distill-Whisper を、 言語基盤モデルとしてLLaMA2-7B を使用した。
音声基盤モデルと言語基盤モデルの分散表現の次元数はそれぞれ 𝐷x= 1280と 𝐷y= 4096 である。
音声翻訳システム全体のパラメターサイズは 7. 63B となる。
計算コストを減らすため4bitの量子化を行い半精度計算を行った[17].すべての音声翻訳システムは、 FFN に LoRA アダプターを挿入して適応学習される[18]. LoRA アダプターの 𝑟 と 𝛼 はそれぞれ 64 と 128 に設定した。
初めに、 3 つの音声翻訳システム（図 1, Cascade,E2E, Hybrid. ）を構築した。
Cascade システムは接続アダプターを持たず、 LoRA アダプターのみで学習される。
Whisper は音声を入力として、 書き起こしを正解として、 LLaMA2 は正解書き起こしを入力と1） 論文発表後 TalkList を公開予定

表 1 音声翻訳精度。
No. SystemInputsData WER↓ BLEU↑ MTR↑Trainablefor LLaMA2 Params.↓1 CascadeTextOriginal 7.7 25.4 36.1119.3MNew 11.6 23.1 35.3End-to-end Speech translation2 Bottleneck SpeechOriginal 8.4 21.9 32.5123.2MNew 12.8 16.5 26.03 SLM SpeechOriginal 8.5 22.2 33.0132.5MNew 12.3 22.0 31.5Hybrid Speech translation4 MHA1stSpeech Original 7.1 25.8 36.7136.7M& Text New 12.3 22.0 31.55 MHAAllSpeech Original 7.5 25.5 36.0564.9M& Text New 12.0 23.0 34.86 RSAAllSpeech Original 6.8 26.6 37.1119.3M& Text New 9.7 24.3 35.8して、 翻訳テキストを正解としてそれぞれ学習される。
E2E システムは音声を入力として、 翻訳テキストを正解として学習される。
Hybrid システムでは、音声と正解書き起こしを入力とし、 書き起こしと翻訳テキストを正解とするマルチタスク学習が行われる。
マルチタスク学習の損失は音声認識と翻訳でそれぞれ 0. 3 と 0. 7 で重みづけされる。
Hybrid システムの接続アダプターでは、 MHA と RSA の 2 種類のアダプターを用いた。
まず、 ベースラインとしてMHA接続アダプターを, LLaMA2のEmbed層の後に 1 層のみ差し込んだ MHA1stと、 32 層すべてに差し込んだ MHAAllを用意した。
接続アダプターは、Cross-Attention として動作し Whisper と LLaMA2 の分散表現を統合する。
提案手法である RSA ベースのシステムも同様に作成する。
RSAAllは 32 層すべてに接続アダプターを挿入した( 3 節、 図 2 (d)) . すべてのシステムは、 NVIDIA A100 GPU で 12 時間適応学習され、 最も高い Validation スコアを達成したモデルで評価した。



4.2 音声翻訳結果

音声翻訳実験において、 “Original” と “New” 評価セットの結果を表 1 に示す。
翻訳精度は SacreBLEU(BLEU)[19, 20], METEOR (MTR)[21], COMET [22]の3スコアで評価した.また,各システムの音声認識率（WER)も併せて報告する。
表 1 が示すように提案手法である RSAAll(system6)が他のシステムより良い結果を示した。
また、RSAAllは、 追加の学習パラメータがないにもかかわらず、 他のすべてのシステムに勝る結果を示した。
こ表 2 正解書き起こしを得られた場合の音声翻訳精度。
No. System Input Data BLUE↑ MTR↑ COMET↑1 Cascade TextOriginal 26.7 37.2 81.5New 25.8 36.1 79.37 RSA𝐴𝑙𝑙Speech Original 28.8 40. 2 83.3& Text New 28.1 40.0 83.0れは、 多くの追加学習パラメータ（446MB）を持つMHAAllと比べても、 性能的に優位であり、 基盤モデルに挿入された LoRA アダプターのパラメターのみでも十分な学習が行えることを示唆している。
加えて、 RSAAllは、 Cascade システム(system 1)にも勝っており、 精度と効率性の両方で他のシステムに優れることが分かった。


4.3 正解書き起こしによる翻訳結果

上記の実験での音声認識精度は、 90 ％近い高精度を実現している。
この実験では、 将来的に音声認識精度がさらに向上し、 仮に人と同じ認識精度を実現できたときに、 提案法が有効かどうか検証する。
Cascade と Hybrid システムへの入力が音声と、 その正解書き起こしだった場合に翻訳精度を比較した。
表 2 は、 正解書き起こしを得られた場合の翻訳精度を示す。
提案法の RSAAllは、 仮に正解書き起こしを得られた場合でも Cascade システムに勝る精度を示した。
これは今後、 音声認識精度が向上したとしても Cascade システムに対して優位性を保てることを示している。
また、 提案法が Cascade システムに勝る要因として、 Hybrid システムは音声の分散表現を利用でき、 非言語情報を考慮した音声翻訳ができることが考えられる。
これにより、 Cascade システムよりも学習できる情報が増え、 翻訳精度の改善につながった。



4.4 おわりに

本研究では基盤モデル Whisper と LLaMA2 を統合した音声翻訳システムを構築した。
提案手法の RSAは追加パラメター無しで、 二つの基盤モデルを接合・全体最適化できる。
音声認識誤りに対する頑健性を向上させるのみならず、 非言語情報を音声翻訳に活用することで精度が向上する可能性を示した。
今後の展望として、 より多くのドメインやタスクを通して提案法の有効性を確認する。



参考文献


[1] Alec Radford, Jeﬀ Wu, Rewon Child, David Luan, Dario Amodei,and Ilya Sutskever. Language models are unsupervised multitasklearners. OpenAI blog, Vol. 1, No. 8, p. 9, 2019.
[2] Lo¨ıc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale,Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, BrianEllis, Hady Elsahar, Justin Haaheim, et al. Seamless: Multilingualexpressive and streaming speech translation. arxiv:2312.05187,2023.
[3] Jean-Baptiste Alayrac, Jeﬀ Donahue, Pauline Luc, Antoine Miech,Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Kather-ine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford,Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Mari-anne Monteiro, Jacob L Menick, Sebastian Borgeaud, Andy Brock,Aida Nematzadeh, Sahand Sharifzadeh, Miko l aj Bi´nkowski, Ri-cardo Barreira, Oriol Vinyals, Andrew Zisserman, and Kar´en Si-monyan. Flamingo: a visual language model for few-shot learning.In NeurIPS, Vol. 35, pp. 23716–23736, 2022.
[4] Jie Jiang, Zeeshan Ahmed, Julie Carson-Berndsen, Peter Cahill,and Andy Way. Phonetic representation-based speech translation.In MTSummit, 2011.
[5] William Chen, Takatomo Kano, Atsunori Ogawa, Marc Delcroix,and Shinji Watanabe. Train long and test long:leveraging fulldocument contexts in speech processing. In ICASSP, pp. 13066–13070, 2024.
[6] Ye Jia, Melvin Johnson, Wolfgang Macherey, Ron J. Weiss,Yuan Cao, Chung-Cheng Chiu, Naveen Ari, Stella Laurenzo, andYonghui Wu. Leveraging weakly supervised data to improve end-to-end speech-to-text translation. In ICASSP, pp. 7180–7184,2019.
[7] Matthias Sperber and Matthias Paulik. Speech translation and theend-to-end promise: Taking stock of where we are. In ACL, pp.7409–7421, 2020.
[8] Srijith Radhakrishnan, Chao-Han Huck Yang, Sumeer AhmadKhan, Rohit Kumar, Narsis A. Kiani, David Gomez-Cabrero, andJesper Tegn´er. Whispering LLaMA: A cross-modal generative er-ror correction framework for speech recognition. In EMNLP, pp.10007–10016. ACL, 2023.
[9] Sanchit Gandhi, Patrick von Platen, and Alexander M. Rush. Distil-whisper: Robust knowledge distillation via large-scale pseudo la-belling. CoRR, Vol. abs/2311.00430, , 2023.
[10] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, AmjadAlmahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra,Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Es-iobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-thia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Vik-tor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, JenyaLee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, An-drew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subrama-nian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, YuchenZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur´elienRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.Llama 2: Open foundation and ﬁne-tuned chat models. CoRR,Vol. abs/2307.09288, , 2023.
[11] Mattia Antonino Di Gangi, Roldano Cattoni, Luisa Bentivogli,Matteo Negri, and Marco Turchi. MuST-C: a multilingual speechtranslation corpus. In NAACL-HLT, pp. 2012–2017. ACL, 2019.
[12] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raﬀel, Barret Zoph,Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, DennyZhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, OriolVinyals, Percy Liang, Jeﬀ Dean, and William Fedus. Emergentabilities of large language models. Trans. Mach. Learn. Res.,2022.
[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.Attention is all you need. Proc. NeurIPS, Vol. 30, , 2017.
[14] Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-ChengChiu, Yuan Cao, Nanxin Chen, Yu Zhang, Hagen Soltau, Paul K.Rubenstein, Lukas Zilka, Dian Yu, Golan Pundak, Nikhil Sid-dhartha, Johan Schalkwyk, and Yonghui Wu. SLM: bridge thethin gap between speech and text foundation models. In ASRU,pp. 1–8, 2023.
[15] Zeyu Han, Chao Gao, Jinyang Liu, Jeﬀ Zhang, and Sai Qian Zhang.Parameter-eﬃcient ﬁne-tuning for large models: A comprehensivesurvey. CoRR, Vol. abs/2403.14608, , 2024.
[16] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi,Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, JahnHeymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchin-tala, and Tsubasa Ochiai. ESPnet: End-to-end speech processingtoolkit. In Interspeech, pp. 2207–2211. ISCA, 2018.
[17] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettle-moyer. QLoRA: Eﬃcient ﬁnetuning of quantized LLMs. InNeurIPS, Vol. 36, pp. 10088–10115, 2023.
[18] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick,and Graham Neubig. Towards a uniﬁed view of parameter-eﬃcienttransfer learning. In ICLR. OpenReview.net, 2022.
[19] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.BLEU: a method for automatic evaluation of machine translation.In ACL, pp. 311–318. ACL, 2002.
[20] Matt Post. A call for clarity in reporting BLEU scores. In Pro-ceedings of the Third Conference on Machine Translation,pp. 186–191, Belgium, Brussels, October 2018.
[21] Alon Lavie and Abhaya Agarwal. METEOR: an automatic metricfor MT evaluation with high levels of correlation with humanjudgments. In WMT, pp. 228–231. ACL, 2007.
[22] Ricardo Rei, Craig Stewart, Ana C. Farinha, and Alon Lavie.COMET: A neural framework for MT evaluation. In EMNLP,pp. 2685–2702. ACL, 2020.