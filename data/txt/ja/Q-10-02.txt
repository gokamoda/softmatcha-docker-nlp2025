異種属性の内容的特徴をハイパーグラフにより統合するエンティティ表現学習

西出隆盛 三輪誠



豊田工業大学



{sd21066,makoto-miwa}@toyota-ti.ac.jp



概要

エンティティの内容を表す名前や説明などのテキストで表される様々な異なる種類の属性情報に含まれるエンティティの内容的な特徴を反映したエンティティ表現を獲得する新たなハイパーグラフ表現学習手法を提案する。
具体的には、属性情報をエンティティとは異なるノードとして統合したハイパーグラフを定義し、その上で新たに提案するアテンション機構を利用した異種ハイパーグラフエンコーダと属性情報からエンティティを予測するエンティティ予測損失を導入し、属性情報に含まれるエンティティの内容的な特徴をエンティティ表現に統合する。
PubMed データセットを対象としたノード分類において、提案手法が既存手法と比較して高い分類性能を示すことを確認した。


1 はじめに

エンティティ表現の獲得は、情報検索、推薦システム、分類など多くのタスクにおいて重要である。
例えば、論文エンティティには、図 1 上部のようにタイトルやアブストラクトといったテキストで表される様々な異なる種類の属性情報1）が付随している。
これを反映できれば、属性情報に含まれるエンティティの内容的な特徴をより詳細に捉えた表現の獲得が期待できる。
エンティティやその属性情報のまとまりは、複数のノードをハイパーエッジと呼ばれる集合で接続できるハイパーグラフにより表現できる。
近年、このようなハイパーグラフを対象としたハイパーグラフ表現学習[1]が注目を集めている。
ハイパーグラフ表現学習は、主にノード分類やリンク予測といった特定のタスクに特化した教師あり学習の枠組みで研究が行われてきた[2, 3]。
さらに、事前タスクとし1） 本論文ではエンティティに付随する様々な情報を広く属性と呼ぶ図 1 属性情報を統合したハイパーグラフの構築てタスク非依存な汎用的表現を獲得できる自己教師あり学習が行われており、ハイパーグラフ拡張に基づく対照学習やハイパーエッジ補完などが提案されている[4, 5]。
しかし、既存のハイパーグラフ表現学習手法では、エンティティと属性情報で表される異なるノードやそれらの間のハイパーエッジを区別して扱うことができていない。
図 1 下部のように、属性情報を統合したハイパーグラフでは、エンティティノードに加えて属性情報を表すノードが存在し、エンティティ間の関係だけでなくエンティティと属性ノード間の関係もハイパーエッジとして定義する。
このようなハイパーグラフを扱うためには、異なる種類のノードとハイパーエッジを区別して扱い、エンティティと属性情報間の関係からエンティティの特徴を捉え、それをエンティティ表現に反映する必要がある。
そこで本研究では、こうした属性情報を統合した図 2 提案手法の概要ハイパーグラフにおける異なる種類のノードとハイパーエッジを考慮し、属性情報が表すエンティティの内容的特徴を反映したエンティティ表現を獲得するための新たなハイパーグラフ表現学習手法を提案する。
このようなハイパーグラフに特化したエンコードのために、異なる種類のノードとハイパーエッジに対応した注意機構に基づく異種ハイパーグラフエンコーダとエンティティと属性情報間の関係性を学習するエンティティ予測損失を新たに導入する。
これにより属性情報を統合したハイパーグラフにおけるエンティティの表現学習を実現し、ハイパーグラフによる構造的情報とテキストによる内容的な情報の両方を考慮したエンティティ表現の獲得を目指す。
本手法の有効性の検証として、PubMed 共引用ネットワーク[6, 7]における論文の分類タスクにおける性能評価を行う。
具体的には論文をノードとし、共引用関係をハイパーエッジとする基本構造に、タイトルやアブストラクトなどのテキスト属性をノードとして統合し、既存のハイパーグラフ表現学習手法と比較する。
本研究における貢献は以下である。
• エンティティとは異なるノードとして属性情報を統合したハイパーグラフを対象に、異種のノードとハイパーエッジを区別して処理する新たな異種ハイパーグラフエンコーダを提案した。
• 属性情報に含まれるエンティティの内容的特徴をエンティティ表現に反映させるために、エンティティ予測損失を導入した。
• PubMed データセットを用いたノード分類において、提案手法が既存のハイパーグラフ表現学習手法と比較して高い分類性能を示すことを確認し、属性情報を統合する提案手法の有効性を示した。



2 関連研究

ハイパーグラフ表現学習は、ハイパーグラフから、その構造的・関係的特性を保持した潜在表現を学習することを目的としており、近年では、Hypergraph Neural Networks (HGNNs)[1]に基づく手法が多く提案されている。
HGNNs は、ノードの特徴とハイパーグラフ構造を入力とし、ノードからハイパーエッジ、ハイパーエッジからノードへのメッセージ伝播を通じてハイパーグラフを表現空間にエンコードする。
HNHN [8]はハイパーエッジ専用のエンコーダを導入し、UniGNN [3]はグラフとハイパーグラフの両方に対応する一般化されたメッセージ伝播関数を提案した。
このようなハイパーグラフ表現学習は当初、ノード分類やリンク予測といった特定のタスクに特化した教師あり学習の枠組みで研究が行われてきたが、最近では、タスク非依存な汎用的表現を獲得可能な自己教師あり学習のフレームワークが提案されている。
事前タスクとして、TriCL [4]はハイパーグラフ拡張によってマルチビュー対照学習を行い、HypeBoy [5]はハイパーエッジ補完タスクと呼ばれる生成的自己教師あり学習により、汎用的表現を獲得した。



3 提案手法

異種のノードとハイパーエッジを区別して処理するアテンション機構を利用した異種ハイパーグラフエンコーダと、属性情報からエンティティを予測するエンティティ予測損失から構成される、エンティティと属性情報を表す異なるノードやそれらの間のハイパーエッジを区別して扱う新たなハイパーグラフ表現学習手法を提案する(図 2)。
以下、3.1 節でエンコーダ、3.2 節で対照損失について述べる。



3.1 アテンション機構を利用した異種ハ



イパーグラフエンコーダ

異種ハイパーグラフエンコーダでは、エンティティノードと属性ノードの異なる特性を考慮し、メッセージ伝播を繰り返すことで階層的な特徴抽出を行う。
具体的には、ノードとエッジの埋め込みを用いた初期表現の獲得、ノードとハイパーエッジの双方向のメッセージ伝播、出力変換の 3 つの処理を行い、ノード表現を得る。
3.1.1 ノードとエッジの埋め込みを用いた初期表現の獲得エンティティノードと属性ノードの異なる特性、およびそれらの関係性を捉えるため、以下の 3 種類の埋め込み表現を導入する。
ノードタイプ埋め込み e𝑡：タイトルやアブストラクトなど、各ノードタイプの意味的特徴を表現エンティティ属性埋め込み e𝑘：エンティティノードと属性ノードの異なる役割を表現エッジタイプ埋め込み e𝑟：エンティティ間および、エンティティ-属性間の関係性を表現入力特徴量 X𝑡に対し、非線形変換と埋め込みの加算により初期表現を獲得する。
H(0)𝑡=ELU(LN(W𝑡X𝑡+b𝑡)) +e𝑡+e𝑘(1)3.1.2 ノードとハイパーエッジの双方向メッセージ伝播エッジタイプ 𝑟 ∈ 𝑅 ごとに以下の双方向のメッセージ伝播を行う。
ここで、𝐻𝑒と 𝐻𝑛はアテンションヘッド数を表す。
1)ノード → ハイパーエッジのメッセージ伝播：エッジタイプ埋め込み e𝑟を加算した後、マルチヘッドアテンションにより特徴を集約する。
M(𝑙)𝑒, 𝑟=𝐻𝑒ℎ=1softmaxQℎ1(Kℎ1)⊤√𝑑𝑘Vℎ1(2)2)ハイパーエッジ → ノードのメッセージ伝播：生成されたハイパーエッジ表現からノードへ逆伝播を行う。
H(𝑙+1)= H(𝑙)+𝑟 ∈𝑅𝐻𝑛ℎ=1softmaxQℎ2(Kℎ2)⊤√𝑑𝑘Vℎ2(3)3.1.3 出力変換各ノードタイプごとに異なる MLP を適用し、最終的なノード表現を得る。
Z𝑡= MLP𝑡(H(𝐿 )𝑡)(4)

3.2 エンティティ予測損失

属性情報に含まれるエンティティの内容的特徴を効果的に反映させるため、エンティティと属性の関係、属性の重要度、種類を考慮したエンティティ予測損失を導入する。
3.2.1 エンティティに特化した負例サンプリング各ハイパーエッジ 𝑒 ∈ 𝐸 に対し、含まれる正例エンティティ 𝑣𝑒𝑛∈ 𝑉𝑒𝑒𝑛以外のエンティティノードを負例として用いる。
これにより、エンティティと属性情報との関係性を明示的に学習する。
3.2.2 重要度を考慮した属性情報の集約ハイパーエッジに含まれる属性ノード集合 𝑁𝑒の表現を集約する際、アテンション機構により各属性の重要度を考慮する。
m𝑒=𝑣∈𝑁𝑒𝛼𝑣h𝑣, 𝛼𝑣= softmax(w⊤h𝑣)(5)ここで、𝛼𝑣は属性ノード 𝑣 の重要度を表す。
この重要度の学習により、エンティティの内容を特徴付ける重要な属性情報を選択的に集約する。
3.2.3 属性の種類を考慮した対照損失タイトルやアブストラクトなど、異なる種類の属性情報の特性を考慮するため、属性タイプに応じて適応的に調整される温度パラメータを用いた対照損失を導入する。
𝐿ours= −1|𝐸 |𝑒∈𝐸𝑣𝑒𝑛∈𝑉𝑒𝑒𝑛sim(h𝑒𝑛, m𝑒)𝜏𝑟𝜏𝑡− log𝑣′∈𝑉𝑒𝑛exp(sim(h𝑣′, m𝑒)𝜏𝑟𝜏𝑡)(6)ここで、𝜏𝑟と 𝜏𝑡はそれぞれ関係タイプと属性タイプに応じた温度パラメータを表す。
この損失関数により、属性情報に含まれるエンティティの内容的特徴を表現に反映する。
最終的に、モデルの学習には以下の損失関数を用いる。
𝐿 = 𝐿𝑡𝑟𝑖𝑐𝑙+ 𝛼𝐿𝑜𝑢𝑟 𝑠(7)ここで、𝐿𝑡𝑟𝑖𝑐𝑙は TriCL [4]の対照損失（詳細は付録 B.1），𝛼 は重みパラメータを表す。

4 実験



4.1 実験設定

実験には、PubMed データセット[7]を用いた。
このデータセットは 3,840 件の論文ノードと 7,963件の共引用関係から構成され、NCBI の Entrez APIを通じて 3 種類の属性情報（タイトル、要旨、論文中の化合物名）を取得した。
データの詳細な統計は付録 A.1 に示す。
各属性情報の特徴抽出にはPubMedBERT [9]（出力: 768 次元）を用いた。
ベースラインとなる手法では UniGCNII [3]をエンコーダとして用い、各手法で提案されている損失関数により学習を行った。
ハイパーパラメータは付録 B.3 に示す。
評価は線形評価プロトコル[10]により行い、エンコーダの学習で得られた表現を固定して線形分類器を学習した。
比較手法として使用したベースライン手法は付録 B.2 に示す。
また、評価指標は別のシード値で 5 回実行した平均正解率と標準偏差を用いた。



4.2 ノード分類による評価

まず、エンティティ予測損失を用いない基本設定において、属性情報の有効性の検証と有効な組み合わせの調査のため、異なる属性情報の組み合わせによる比較実験を行った。
表 1 に、ノード分類タスクにおける性能の比較をまとめた。
単一属性ではタイトルが最も高い性能を示し、タイトルに要旨を加えた 2 属性の組み合わせがより高い性能を示した。
これは、タイトルによる主要トピックの表現と要旨による詳細情報が補完的に働いたためと考えられる。
さらに、全ての属性情報を統合することで最高性能となり、各属性が異なる側面から学習に寄与することを確認した。
この結果を踏まえ、全属性を利用した上での提案手法の異なる設定とベースライン手法との比較を表2 に示す。
実験結果から、既存手法に対して、提案手法が高い分類性能を示すことが確認できる。
エンティティのみの結果でもベースライン手法を上回っていることから、提案した異種ハイパーグラフエンコーダがエンティティのみの基本的なハイパーグラフ構造に対しても有効であることを示している。
ま表 1 属性情報ごとのノード分類性能（%）単一属性タイトル 85.01 ± 0.79要旨 84.62 ± 0.73化合物名 83.72 ± 0.62属性の組み合わせタイトル + 要旨 85.07 ± 0.65タイトル + 化合物名 84.94 ± 0.71要旨 + 化合物名 84.28 ± 0.76全属性 85.10 ± 0.85表 2 手法ごとのノード分類性能（%）ベースラインTriCL 82.26 ± 0.47HyperGCL 82.87 ± 0.68HypeBoy 83.11 ± 0.63提案手法エンティティのみ 84.29 ± 0.71+ エンティティ予測損失 84.35 ± 0.81エンティティ + 全属性 85.10 ± 0.85+ エンティティ予測損失 85.27 ± 0.75た、エンティティのみを用いた基本設定に対し、エンティティ予測損失の追加で若干の改善が見られた。
さらに、全属性情報の統合で性能が向上し、エンティティ予測損失と組み合わせて 85.27%を達成した。
これは、エンティティと属性情報を統合的に扱う本手法の有効性を示している。



5 おわりに

本研究では、エンティティに付随する異種属性情報の内容的特徴を反映したエンティティ表現の獲得を目的として、属性情報を統合したハイパーグラフを対象とする新たな表現学習手法を提案した。
PubMed データセットのノード分類タスクにおいて、異なる属性情報の効果を分析した結果、利用した全属性の統合で最も高い性能向上が確認され、各属性が異なる側面から学習に寄与することが示された。
また、既存手法との比較において、提案手法が高い分類性能を示すことを確認し、属性情報の統合の有効性を実証した。
今後は提案手法の実用性と汎用性を向上させるため、より大規模なデータセットへの適用と異なる学術ドメインへの一般化に取り組む。



参考文献


[1] Alessia Antelmi, et al. A survey on hypergraph represen-tation learning. ACM, Vol. 56, No. 1, aug 2023.
[2] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji,and Yue Gao. Hypergraph neural networks. Pro ceedingsof the AAAI Conference on Artiﬁcial Intelligence,Vol. 33, No. 01, pp. 3558–3565, Jul. 2019.
[3] Jing Huang and Jie Yang. Unignn: a uniﬁed framework forgraph and hypergraph neural networks. In Zhi-Hua Zhou,editor, Pro ceedings of the Thirtieth InternationalJoint Conference on Artiﬁcial Intelligence, IJCAI-21, pp. 2563–2569. International Joint Conferences onArtiﬁcial Intelligence Organization, 8 2021. Main Track.
[4] Dongjin Lee and Kijung Shin. Im me, were us, and imus: Tri-directional contrastive learning on hypergraphs. InProceedings of the AAAI Conference on ArtiﬁcialIntelligence, 第 37 巻, pp. 8456–8464, 2023.
[5] Sunwoo Kim, Shinhwan Kang, Fanchen Bu, Soo YongLee, Jaemin Yoo, and Kijung Shin. Hypeboy: Generativeself-supervised representation learning on hypergraphs. InThe Twelfth International Conference on LearningRepresentations, 2024.
[6] Lise Getoor Galileo Mark Namata, Ben London and BertHuang. Query-driven active surveying for collective clas-siﬁcation. InInternational Workshop on Mining andLearning with Graphs, Edinburgh, Scotland, 2012.
[7] Naganand Yadati, Madhav Nimishakavi, Prateek Yadav,Vikram Nitin, Anand Louis, and Partha Talukdar. Hy-pergcn: A new method for training graph convolutionalnetworks on hypergraphs. In H. Wallach, H. Larochelle,A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett,editors, Advances in Neural Information ProcessingSystems, Vol. 32. Curran Associates, Inc., 2019.
[8] Yihe Dong, Will Sawin, and Yoshua Bengio. Hnhn:Hypergraph networks with hyperedge neurons. ICMLGraph Representation Learning and Beyond Work-shop, 2020.
[9] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga,Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, NaveenValluri, Cliﬀ Wong, Matthew Lungren, Tristan Naumann,and Hoifung Poon. Large-scale domain-speciﬁc pretrain-ing for biomedical vision-language processing, 2023.
[10] Petar Veliˇckovi´c, William Fedus, William L. Hamilton,Pietro Li`o, Yoshua Bengio, and R Devon Hjelm. Deepgraph infomax. In International Conference on Learn-ing Representations, 2019.
[11] Tianxin Wei, Yuning You, Tianlong Chen, Yang Shen,Jingrui He, and Zhangyang Wang. Augmentations in hy-pergraph contrastive learning: Fabricated and generative.In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, andKyunghyun Cho, editors, Advances in Neural Informa-tion Processing Systems, 2022.
[12] Ilya Loshchilov and Frank Hutter. Decoupled weight decayregularization, 2019.
[13] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen,Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On thevariance of the adaptive learning rate and beyond, 2021.
[14] Aaron Defazio, Xingyu Yang, Harsh Mehta, KonstantinMishchenko, Ahmed Khaled, and Ashok Cutkosky. Theroad less scheduled, 2024.




A データセットの統計



A.1 PubMed データセット

表 3 にデータセットの基本統計を示す。
表 3 PubMed データセットの統計ノード数Paper 3,840Title 3,840Abstract 3,840PubChem Name 605ハイパーエッジ数共引用関係 7,963エンティティ-属性関係 3,840線形分類器の学習実験においてはデータセットの分割比率を訓練：10%、検証：10%、テスト：80%とした。



B 学習設定詳細



B.1 TriCL の対照損失

エンコーダの学習で用いた TriCL の損失関数は、以下の 3 つの対照損失の重み付き和として定義される。
• ノードレベル損失：同一ハイパーエッジ内のノード間の類似性を最大化• エッジレベル損失：共通ノードを持つハイパーエッジ間の類似性を最大化• メンバーシップ損失：ノードとその所属ハイパーエッジ間の類似性を最大化𝐿𝑡𝑟𝑖𝑐𝑙= 𝛼𝐿𝑛𝑜𝑑𝑒+ 𝛽𝐿𝑒𝑑𝑔𝑒+ 𝛾𝐿𝑚𝑒𝑚(8)ここで、𝛼, 𝛽, 𝛾 は各損失項の重みを表すハイパーパラメータである。


B.2 ベースライン手法

比較実験に使用した既存のハイパーグラフ表現学習手法は以下のようである。
HyperGCL [11]：ハイパーグラフ構造の摂動に基づく対照学習TriCL [4]：3 方向の対照学習を用いたハイパーグラフ表現学習HypeBoy [5]：生成的自己教師あり学習によるハイパーエッジ補完

B.3 ハイパーパラメータ

B.3.1 異種ハイパーグラフエンコーダの設定モデルの主要なハイパーパラメータを表 4 に示す。
表 4 異種ハイパーグラフエンコーダのパラメータ設定パラメータ値埋め込み次元 512中間層の次元 512畳み込み層数 3エッジアテンションヘッド数 8ノードアテンションヘッド数 8特徴変換層数 2Dropout 率 0.2B.3.2 ハイパーパラメータの探索エンコーダの学習のハイパーパラメータについては、以下のパラメータの組み合わせを探索し、検証データにおける性能が最も高いものを採用した。
• エポック数: {200, 400, 600, 700, 800, 900, 1000}• 学習率: {5e-4, 1e-3, 5e-3}• ハイパーグラフ畳み込み層数: {1, 2, 3}• 最適化手法: {AdamW [12], RAdam [13], AdamW(Schedule Free [14]), RAdam (Schedule Free)}全ての属性情報とエンティティ予測損失を用いた最終実験では、以下の設定を使用した：表 5 最終実験におけるハイパーパラメータ設定パラメータ値エポック数 900学習率 1e-3畳み込み層数 3最適化手法 RAdam (Schedule Free)