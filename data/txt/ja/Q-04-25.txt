Leveraging Sentiment Adjectives in Instruction Tuningof LLMs for Zero-Shot Sentiment Classiï¬cation

Yang Zhao

1

â€ƒMasayasu Muraoka

1

â€ƒIssei Yoshida

1

â€ƒBishwaranjan Bhattacharjee

2

â€ƒHiroshi Kanayama

11

IBM Research - Tokyo, 19-21 Nihonbashi Hakozaki-cho, Chuo City, Tokyo, 103-8510, Japan â€ƒ

2

IBM Research, Yorktown Heights, New York 10598, USA



yangzhao@ibm.com â€ƒ{mmuraoka,issei,hkana}@jp.ibm.com â€ƒbhatta@us.ibm.com

AbstractInstruction tuning signiï¬cantly improves the perfor-mance of LLMs in tasks such as sentiment classiï¬cation. Inthis work, we propose a simple yet eï¬ƒcient instruction aug-mentation method which
does not rely on any actual labeledsentiment instances. With just 240 pseudo-instruction in-stances, the proposed method signiï¬cantly improves thesentiment classiï¬cation performance across several LLMson 12 sentiment benchmark datasets, increasing scores by30 points and outperforming LLMs that utilize more com-plex instruction tuning methods by 5.1 points.1 IntroductionSentiment analysis has long been an established areaof research in Natural Language Processing (NLP). Withrecent advances in large
language models (LLMs), im-pressive zero-shot performance in sentiment analysis wasachieved by instruction-tuned LLMs [5, 18, 14]. A typ-ical sentiment Instruction Instance is a tuple with threecomponents (T, I, O):â€¢ Instruction Text (T): Classify the following sentenceinto either positive, neutral or negative sentiment.â€¢ Input (I): A movie journey worth taking.â€¢ Output (O): The sentiment is positive.where the instruction text (T) refers to the user instruction.It usually speciï¬es the desired outputs; the input (I) refers tothe input sentence or document for the sentiment task; theoutput (O) refers to the ground truth answer cor respondingto the instruction text.Previously, many sentiment analysis studies have utilizedactual training instances in sentiment benchmark datasetsas Input (I) and corresponding labels as Output (O) forinstruction tuning. For example, the work [21] instruction-tuned LLMs across various
NLP tasks, including four sen-timent datasets, while the work [3] further expanded thisapproach to more than 1,800 NLP tasks. Considering sen-timent classiï¬cation spans diverse domains such as ï¬nance,restaurants, movies, and politics, obtaining a large numberof domain-speciï¬c labeled instances for instruction tuningis labor-intensive and ineï¬ƒcient.To enhance this aspect, we propose a simple-yet-eï¬ƒcientinstruction augmentation method to construct sentimentaladjective-based pseudo instructions that do not rely
on anytraining instances in sentiment benchmark datasets. Subse-quently, we instruction-tune Llama2-7b,13b,70b base mod-els and the Falcon-40b base model and evaluate their zero-shot performance on 12 sentiment benchmark datasets.The results show that instruction-tuned models signiï¬-cantly outperform the base models by 30 points and otherinstruction-tuned models by an average of 5.1 points.2 Sentimental Adjective-based In-struction ConstructionWe herein describe the steps to construct pseudo in-stances using
sentimental adjectives. Section 2.1 outlinesthe process for collecting diverse sentiment instruction text(T) from various corpora. Section 2.2 details the stepsof constr ucting instruction using sentimental adjectives forInput (I) and Output (O).2.1 Instruction Text (T) CollectionUser instructions exhibit a wide variety of paraphras-ing. To increase the diversity, we collect sentiment in-struction text (T) from ï¬ve widely-used instruction datasetswritten by either human annotators or LLMs, as follows:(1) SuperNI [20], which contains 96k instructions writ-ten by humans covering 1600+ NLP tasks. (2) Alpaca1ï¼‰[16], which contains 52k instructions generated by GPT-3(davinci-003). (3) Self-instruct [19], which contains 82kinstructions generated by GPT-3 (vanilla). (4) UnnaturalInstructions [8], which contains 68k instructions generatedby GPT-3 (davinci-002). (5) Baize [22], which contains210k instruction instances created by prompting ChatGPTand letting it converse with itself. We extracted all
theinstruction text (T) from these datasets and retained in-struction texts only if they contain the terms â€˜sentimentâ€™,â€˜positiveâ€™, â€˜negativeâ€™, and â€˜neutralâ€™. Finally, 110 diversesentiment instruction text (T) are yielded, and we empir-ically determine to use 80 for training and 30 for testingduring instruction tuning. For the aspect-based sentimentclassiï¬cation task, we add with respect to the TARGET tothe instruction text and replace TARGET with the speciï¬caspect.2.2 Sentimental Adjective (I, O) PairInspired by the concept of evaluative adjectives in lin-guistics, we describe the four steps to automatically collectpairs of instruction input (I) and output (O). Evaluativeadjectives often express value judgments and convey opin-ions, emotions, or subjective interpretations. For instance,adjectives like beautiful imply a positive sentiment, whileawful suggests a negative one. We refer to our collectedadjectives as sentimental adjectives.Step
1. Collect sentimental adjective candidatesWe start by collecting adjectives from SentiWordNet 3.02ï¼‰[2] where each sense of an adjective word ğ‘¤ is assigned twoscores: a positive score (ğ‘†ğ‘ğ‘œğ‘ ) and a negative score (ğ‘†ğ‘›ğ‘’ğ‘”)where 0 â‰¤ ğ‘†ğ‘˜â‰¤ 1 and ğ‘˜ âˆˆ {ğ‘ğ‘œğ‘ , ğ‘›ğ‘’ğ‘”}. The selectioncriteria is:1. Choose all words where at least one of its senses meetsthe criteria: ğ‘†ğ‘ğ‘œğ‘ â‰¥ ğ‘Ÿ and ğ‘†ğ‘›ğ‘’ğ‘”= 0.0 to compilepositive word list ğ¿1ğ‘ğ‘œğ‘ 2. Choose all words where at least one of its senses meetsthe criteria:: ğ‘†ğ‘›ğ‘’ğ‘”â‰¥ ğ‘Ÿ and ğ‘†ğ‘ğ‘œğ‘ = 0.0 to compile1ï¼‰ https://github.com/gururise/AlpacaDataCleaned/2ï¼‰ https://github.com/aesuli/SentiWordNet. It is under CC BY-SA4.0 license.negative word list ğ¿1ğ‘›ğ‘’ğ‘”3. Choose all words where at least one of its senses meetsthe criteria: ğ‘†ğ‘ğ‘œğ‘ = 0.0 and ğ‘†ğ‘›ğ‘’ğ‘”= 0.0 to compileneutral word list ğ¿1ğ‘›ğ‘’ğ‘¢We empirically determine the threshold ğ‘Ÿ to trade oï¬€
be-tween the number and quality of adjectives. Please seeTable 3 in Appendix for ğ¿1.Step 2. Align with sentiment word sense.This step aims to reï¬ne the adjective lists in Step 1. Forinstance, one sense of the word â€˜freshâ€™ meets the criteriağ‘†ğ‘›ğ‘’ğ‘”â‰¥ 0.75 and ğ‘†ğ‘ğ‘œğ‘ = 0.0, this word is therefore includedin the negative list ğ¿1ğ‘›ğ‘’ğ‘”. However, â€freshâ€ often conveysa non-negative meaning, typically referring to somethingnew or unused. including this word in negative list mayconfuse the model during instruction tuning. To addressthis, we utilize pre-deï¬ned positive (ğ‘‰ğ‘ğ‘œğ‘ ) and negative(ğ‘‰ğ‘›ğ‘’ğ‘”) vocabularies in the paper [10]. Words in lists ğ¿1ğ‘ğ‘œğ‘ and ğ¿1ğ‘›ğ‘’ğ‘”are excluded if they do not appear in ğ‘‰ğ‘ğ‘œğ‘ andğ‘‰ğ‘›ğ‘’ğ‘”, respectively. Words in ğ¿1ğ‘›ğ‘’ğ‘¢are removed if theyappear in eitherğ‘‰ğ‘ğ‘œğ‘ orğ‘‰ğ‘›ğ‘’ğ‘”. This process results in threereï¬ned lists: ğ¿2ğ‘ğ‘œğ‘ , ğ¿2ğ‘›ğ‘’ğ‘”, and ğ¿2ğ‘›ğ‘’ğ‘¢. Please see Table
4 inAppendix for ğ¿2.Step 3. Rank word by frequency.This step focuses on selecting more domain-agnostic wordsby leveraging frequency information. We use EnglishWikipedia3ï¼‰to obtain word frequency for ranking adjec-tives in each list in descending order based on their fre-quency. If a adjective in ğ¿2ğ‘ğ‘œğ‘ , ğ¿2ğ‘›ğ‘’ğ‘”, and ğ¿2ğ‘›ğ‘’ğ‘¢is not inthe wiki frequency list, its frequency would be set to zero.After ranking, frequent words such as best, great, and im-portant appear at the top of the positive list, whereas theoriginal words in the list are legendary, solid, and gallant.We note the ranked lists as ğ¿3ğ‘ğ‘œğ‘ , ğ¿3ğ‘›ğ‘’ğ‘”, and ğ¿3ğ‘›ğ‘’ğ‘¢. Pleasesee Table 5 in Appendix for ğ¿3.Step 4. Add negation words.This step helps LLMs to better handle sentences containingnegation words. We add the negation word not directlybefore adjectives (e.g., not beautiful)
for ğ‘‹% of instancesin only ğ¿3ğ‘ğ‘œğ‘ and ğ¿3ğ‘›ğ‘’ğ‘”. Subsequently, adjectives withnegation from the positive list are transferred to the negativelist and vice versa. This process yields the ï¬nal lists: ğ¿4ğ‘ğ‘œğ‘ ,3ï¼‰ https://jwsmythe.com/tools/wordlist/wikipedia-word-frequency-master/results/enwiki-2023-04-13.txtğ¿4ğ‘›ğ‘’ğ‘”, and ğ¿4ğ‘›ğ‘’ğ‘¢(where ğ¿4ğ‘›ğ‘’ğ‘¢= ğ¿3ğ‘›ğ‘’ğ‘¢). Please see Table 6in Appendix for ğ¿4.After completing steps 1 to 4, we take the ï¬rst instructiontext T from 80 instruction texts in Section 2.1, the ï¬rstadjective from ğ¿4ğ‘ğ‘œğ‘ and positive to form the ï¬rst tuple (T,I, O); Continue this process until the 80th instruction textis taken. Then, we obtained 80 tuples for the positive class,80 tuples for the negative class, 80 tuples for the neuralclass respectively.3 Experiment3.1 Experimental SetupThe constructed 240 tuples are split into 80% for thetraining set and 20% for the development set. We set thethreshold ğ‘Ÿ in SentiWordNet 3.0
in Step 1 to 0.75, andnegation word percentage ğ‘‹ to 10%, according to perfor-mance on the development set. For training, we followthe paper [17] by utilizing an auto-regressive objective andzeroing out the loss on tokens from the user prompt, in-cluding instruction text and input, while backpropagatingonly on instruction output. Of the 110 instruction texts, weuse 80 for model training and development, and remaining30 for testing. Dur ing training, we employ the eï¬ƒcientparameter tuning technique, LoRA [9], with a LoRA rankof 8 and LoRA alpha of 32. We set learning rate to 2e-4and batch size to 2. During inference, we follow previouswork [6] to load models in the 8-bit mode which signiï¬-cantly speeds up the inference and has negligible impacton the ï¬nal performance. We set the maximum
number ofgenerated tokens to 20. All the experiments are conductedusing one A100 GPU.Evaluation MetricSince all the instruction texts we collected explicitly spec-ify the output space as positive, negative, or neutral label,we adopt the following metric for calculating instance-wise accuracy: 1) Score 1 if the output string containsthe g round-truth label and does not contain other classesâ€™ground-truth labels (case insensitive); 2) Score 0, other-wise. We observed a high correlation score between thehuman annotator and this automatic metric, So we decidedto use this metric for all datasets.3.2 DatasetWe experiment with 7 general sentiment classiï¬cationdatasets, i.e., SST-2 [15], IMDB, Yelp, Amazon datasetsfrom [11], Airline4ï¼‰Debate5ï¼‰, ï¬nancial phrasebank [13] aswell as 5 aspect-based sentiment classiï¬cation datasets6ï¼‰from the Workshop on Semantic Evaluation (SemEval) in2014, 2015, and 2016.Table 1 shows the statistics of
each dataset. We pairedeach sentence from the sentiment benchmark datasets with30 instr uction texts for testing. For instance, in the case ofSST-2, this resulted in 1,821 Ã— 30 = 54,630 instances usedfor testing instruction-tuned models. The same procedurewas applied to the other datasets.Table 1: Statistics of sentiment classiï¬cation datasets.Dataset Domain Size # Class AspectSST-2 Movie 1,821 2 noYelp Restaurant 1,000 2 noAmazon (Amaz) Product 1,000 2 noIMDB Movie 1,000 2 noAirline Operation 1,000 3 noDebate (Deba) Politics 1,000 3 noPhraseBank (PB) Finance 970 3 noSemEval-14lap Laptop 543 3 yesSemEval-14res Restaurant 994 3 yesSemEval-15res Restaurant 485 3 yesSemEval-15hot Hotel 215 3 yesSemEval-16res Restaurant 514 3 yes3.3 ModelsWe instruction-tuned Llama2 base model [17], andfalcon-40b base model [1] using our constructed 240 in-struction tuples (T, I, O), noted as
base+ours. In addition,we consider the following comparison methods:base+ours w/o adjective Previous works, such as [12],have pointed out that some instruction-tuned models donot fully utilize instructions, and that the impressive perfor-mance gains from instruction tuning may stem from modelslearning superï¬cial patterns, such as the output space andformat. To verify this, we replaced the sentimental adjec-tives with empty strings to ablate the input, while keepingthe
instruction text and output format unchanged.4ï¼‰ https://www.kaggle.com/datasets/crowdï¬‚ower/twitter-airline-sentiment. 1k instances is used only.5ï¼‰ https://www.kaggle.com/datasets/crowdï¬‚ower/ï¬rst-gop-debate-twitter-sentiment. 1k instances is used only.6ï¼‰ https://github.com/kevinscaria/InstructABSA/tree/main/DatasetTable 2: Accuracy of zero-shot sentiment classiï¬cation on 12 benchmark datasets. Best results associated with the samebase model are in bold.Dataset SST-2 Yelp Amaz IMDB Deba Airline PB 14hap 14res 15res 15hot 16res Ave.â–³ Lexicon-match baseline 59.2 64.7 69.6 69.0 55.4 63.6 56.0 68.9 81.5
74.4 74.9 78.4 67.9#1 llama2-7b-base 49.4 51.2 40.8 46.7 34.7 37.4 27.3 40.0 61.5 53.8 61.5 61.6 47.2#2 llama2-7b-chat 78.8 88.8 83.5 86.2 61.6 69.9 60.5 75.5 83.6 78.9 71.5 75.7 76.2#3 base+ours w/o adjective 38.9 35.5 32.3 37.9 35.2 35.6 29.9 18.3 13.8 24.9 16.4 13.4 27.7#4 base+ours 89.5 96.1 94.1 94.4 62.0 67.6 53.5 82.1 88.4 86.3 84.4 89.4 82.3$1 llama2-13b-base 47.0 52.5 43.4 49.3 36.1 40.8 41.7 46.1 61.2 56.7 58.0 57.9 49.2$2 llama2-13b-chat 71.2 79.0 75.0 77.9 62.9 69.5 59.1 68.9 76.9 71.4 63.1 65.4 70.0$3 base+ours w/o adjective 49.6 50.4 44.4 50.7 38.7 43.1 26.3 28.0 35.1 41.9 33.9 24.9 38.9$4 base+ours 80.5 88.4 75.9 86.1 63.1 69.8 62.0 62.9 81.6 78.1 73.0 77.2 74.9&1 llama2-70b-base 55.8 42.7 43.7 48.1 34.5
39.1 31.6 44.9 45.1 47.0 44.3 54.8 44.3&2 llama2-70b-chat 81.9 90.0 87.6 88.6 64.8 72.6 68.8 74.5 80.9 77.1 72.3 67.8 77.2&3 base+ours w/o adjective 72.4 80.5 75.8 77.4 43.6 51.1 29.4 64.3 77.1 72.3 71.1 67.0 65.2&4 base+ours 92.5 97.9 95.8 96.3 63.0 71.1 55.3 80.4 89.0 85.6 88.3 85.0 83.4â™¢1 falcon-40b-base 69.9 72.1 61.8 63.1 36.6 42.5 27.5 50.1 65.8 66.3 60.7 67.2 57.0â™¢2 falcon-40b-instr. 78.9 89.2 80.0 83.2 51.5 55.2 40.3 74.7 86.3 81.3 83.3 85.3 74.1â™¢3 base+ours w/o adjective 63.6 58.7 46.4 53.4 36.0 38.9 23.8 35.7 56.5 51.7 51.0 53.2 47.4â™¢4 base+ours 92.0 91.2 87.8 88.1 55.0 62.0 43.2 77.8 84.1 80.6 80.3 85.3 77.3lexicon-match baseline We add a sentiment lexiconmatch-based model [7], which directly utilizes the pres-ence of positive (e.g.,
great, good, and nice) and negativewords (e.g., sad, bad, and worse) to determine the senti-ment polarities. This aims to determine if good perfor-mance can be achieved through simple sentimental wordmatching, without injecting these sentimental adjectivesvia instruction tuning.llama2 chat model The Llama2 chat model began super-vised ï¬ne-tuning with instructions from 1.8K NLP tasks[4]. The model was further ï¬ne-tuned on 27,540 annotatedinstructions and millions of human preference data via re-inforcement learning. We believe this provides a powerfulbaseline, even for our sentiment classiï¬cation task.falcon chat model It is also known as the Falcon-40B-Instruct model7ï¼‰, which is ï¬ne-tuned on hundreds of thou-sands of QA and dialog instances from Quora, Stack Over-ï¬‚ow, and MedQuAD questions.4 Result and AnalysisTable 2 shows comparison results and our observationsare as follows:(1) Our instruction-tuned models (base+ours)
outperformall base models by 30 points and even all chat modelsby 5.1 points on average. Moreover, our instr uction-tunedLlama2-70B model achieves the best average performance,7ï¼‰ https://huggingface.co/tiiuae/falcon-40b-instructsuggesting that model size remains an important factor inthe eï¬€ectiveness of instr uction tuning.(2) The results of base+ours w/o adjective show sig-niï¬cant performance degradation for Llama2-7B (#3),Llama2-13B ($3), and Falcon-40B (â™¢). While the â€empty-inputâ€ instruction tuning boosts Llama2-70Bâ€™s perfor-mance to some extent (&3), combining it with our senti-mental adjectives achieves the best performance (&4). Thisveriï¬es that the improvements are largely not attributed tolearning the output space formats, such as positive andnegative labels, as reported
by previous work [12].(3) To investigate whether our base+ours models simplymemorize sentimental adjectives for making predictions,we added a sentiment lexicon match-based model for com-parison. The results show that our models signiï¬cantlyoutperform this baseline (â–³), indicating that incorporatingsentimental adjectives into LLMs through instruction tun-ing equips the models to handle not only straightforwardsentiment lexicon-based cases but also more challengingcases lacking explicit sentiment lexicons.5 ConclusionIn this work, we create pseudo sentimental instructionsto ï¬ne-tune LLMs. Experiments show signiï¬cant perfor-mance gains on various sentiment benchmarks. Notably, itrequires no ground-truth training data and generalizes wellacross domains. Future work will extend this approach toï¬ne-grained emotion classiï¬cation.



References


[1] Ebtesam Almazrouei, Hamza Alobeidli, AbdulazizAlshamsi, Alessandro Cappelli, Ruxandra Cojocaru,MÂ´erouane Debbah,Â´Etienne Goï¬ƒnet, Daniel Hesslow,Julien Launay, Quentin Malartic, et al. The fal-con series of open language models. arXiv preprintarXiv:2311.16867, 2023.
[2] Stefano Baccianella, Andrea Esuli, Fabrizio Sebastiani,et al. Sentiwordnet 3.0: an enhanced lexical resource forsentiment analysis and opinion mining. In Lrec, Vol. 10,pp. 2200â€“2204, 2010.
[3] Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, et al. Scalinginstruction-ï¬netuned language models. arXiv preprintarXiv:2210.11416, 2022.
[4] Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, et al. Scalinginstruction-ï¬netuned language models. Journal of Ma-chine Learning Research, Vol. 25, No. 70, pp. 1â€“53,2024.
[5] Xiang Deng, Vasilisa Bashlovkina, Feng Han, SimonBaumgartner, and Michael Bendersky. LLMs to the Moon?Reddit market sentiment analysis with large language mod-els. In Companion Proceedings of the ACM Web Con-ference 2023, pp. 1014â€“1019, 2023.
[6] Tim Dettmers, Mike Lewis, Younes Belkada, and LukeZettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication fortransformers at scale. Advances in Neural InformationProcessing Systems, Vol. 35, pp. 30318â€“30332, 2022.
[7] Eric Gilbert. Vader: A parsimonious rule-based model forsentiment analysis of social media text. In Proceedings ofthe international AAAI conference on web and socialmedia, Vol. 8, pp. 216â€“225, 2014.
[8] Or Honovich, Thomas Scialom, Omer Levy, and TimoSchick. Unnatural instructions: Tuning language mod-els with (almost) no human labor. In Proceedings of the61st Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), Toronto,Canada, July 2023.
[9] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. Lora: Low-rank adaptation of large language mod-els. arXiv preprint arXiv:2106.09685, 2021.
[10] Minqing Hu and Bing Liu. Mining and summarizingcustomer reviews. In Proceedings of the tenth ACMSIGKDD international conference on Knowledge dis-covery and data mining, pp. 168â€“177, 2004.
[11] Dimitrios Kotzias, Misha Denil, Nando De Freitas, andPadhraic Smyth. From group to individual labels us-ing deep features. InProceedings of the 21th ACMSIGKDD international conference on knowledge dis-covery and data mining, pp. 597â€“606, 2015.
[12] Po-Nien Kung and Nanyun Peng. Do models really learnto follow instructions? an empirical study of instruc-tion tuning. In Anna Rogers, Jordan Boyd-Graber, andNaoaki Okazaki, editors, Proceedings of the 61st An-nual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pp. 1317â€“1328,Toronto, Canada, July 2023. Association for Computa-tional Linguistics.
[13] P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala.Good debt or bad debt: Detecting semantic orientations ineconomic texts. Journal of the Association for Infor-mation Science and Technology, Vol. 65, , 2014.
[14] Kevin Scaria, Himanshu Gupta, Siddharth Goyal,Saurabh Arjun Sawant, Swaroop Mishra, and Chitta Baral.Instructabsa: Instruction learning for aspect based senti-ment analysis. arXiv preprint arXiv:2302.08624, 2023.
[15] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher D. Manning, Andrew Ng, and ChristopherPotts. Recursive deep models for semantic compositional-ity over a sentiment treebank. In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing, pp. 1631â€“1642, Seattle, Wash-ington, USA, October 2013. Association for Computa-tional Linguistics.
[16] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang, andTatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.
[17] Hugo Touvron, Louis Mar tin, Kevin Stone, Peter Albert,Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.Llama 2: Open foundation and ï¬ne-tuned chat models.arXiv preprint arXiv:2307.09288, 2023.
[18] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, JackHessel, Tushar Khot, Khyathi Raghavi Chandu, DavidWadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy,et al. How far can camels go? exploring the state ofinstruction tuning on open resources. arXiv preprintarXiv:2306.04751, 2023.
[19] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-isa Liu, Noah A. Smith, Daniel Khashabi, and Han-naneh Hajishirzi. Self-instruct: Aligning language modelswith self-generated instructions. In Proceedings of the61st Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), Toronto,Canada, July 2023.
[20] Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-labashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik,Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunk-umar, David Stap, et al. Super-naturalinstructions: Gener-alization via declarative instructions on 1600+ nlp tasks.In Proceedings of the 2022 Conference on EmpiricalMethods in Natural Language Processing, pp. 5085â€“5109, 2022.
[21] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai,and Quoc V Le. Finetuned language models are zero-shotlearners. arXiv preprint arXiv:2109.01652, 2021.
[22] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.Baize: An open-source chat model with parameter-eï¬ƒcient tuning on self-chat data. arXiv preprintarXiv:2304.01196, 2023.

positive words negative words neutral wordssophisticated contemptible last-ditchmagna-cum-laude bogus alategorgeous salt ï¬‚ooredboss unfree quadrilateralheaven-sent hidden fortyexhaustive inhumane french-speakingsuperb humble combinedhealthy false client-server... ... ...Table 3: Step 1. Collect sentiment al adjectives candidates.positive words negative words neutral wordssophisticated contemptible alategorgeous bogus quadrilateralsuperb inhumane fortyhealthy false french-speakingmeticulous precarious combinedperfect upset client-serversweet numb trojancoherent indelicate diagonal... ... ...Table 4: Step 2. Align with sentiment word
sense.positive words negative words neutral wordsbest dead newgreat poor moreimportant diï¬ƒcult nationalgood unable mostbetter bad manysupreme wild americangolden cold earlygreatest oï¬€ensive high... ... ...Table 5: Step 3. Rank word by frequency.positive words negative words neutral wordsbest dead newgreat poor moreimportant diï¬ƒcult nationalgood unable mostbetter bad manysupreme wild americangolden cold earlynot oï¬€ensive not greatest high... ... ...Table 6: Step 4. Add negation words.