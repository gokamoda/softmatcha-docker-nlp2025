Leveraging Sentiment Adjectives in Instruction Tuningof LLMs for Zero-Shot Sentiment Classiﬁcation

Yang Zhao

1

 Masayasu Muraoka

1

 Issei Yoshida

1

 Bishwaranjan Bhattacharjee

2

 Hiroshi Kanayama

11

IBM Research - Tokyo, 19-21 Nihonbashi Hakozaki-cho, Chuo City, Tokyo, 103-8510, Japan  

2

IBM Research, Yorktown Heights, New York 10598, USA



yangzhao@ibm.com  {mmuraoka,issei,hkana}@jp.ibm.com  bhatta@us.ibm.com

AbstractInstruction tuning signiﬁcantly improves the perfor-mance of LLMs in tasks such as sentiment classiﬁcation. Inthis work, we propose a simple yet eﬃcient instruction aug-mentation method which
does not rely on any actual labeledsentiment instances. With just 240 pseudo-instruction in-stances, the proposed method signiﬁcantly improves thesentiment classiﬁcation performance across several LLMson 12 sentiment benchmark datasets, increasing scores by30 points and outperforming LLMs that utilize more com-plex instruction tuning methods by 5.1 points.1 IntroductionSentiment analysis has long been an established areaof research in Natural Language Processing (NLP). Withrecent advances in large
language models (LLMs), im-pressive zero-shot performance in sentiment analysis wasachieved by instruction-tuned LLMs [5, 18, 14]. A typ-ical sentiment Instruction Instance is a tuple with threecomponents (T, I, O):• Instruction Text (T): Classify the following sentenceinto either positive, neutral or negative sentiment.• Input (I): A movie journey worth taking.• Output (O): The sentiment is positive.where the instruction text (T) refers to the user instruction.It usually speciﬁes the desired outputs; the input (I) refers tothe input sentence or document for the sentiment task; theoutput (O) refers to the ground truth answer cor respondingto the instruction text.Previously, many sentiment analysis studies have utilizedactual training instances in sentiment benchmark datasetsas Input (I) and corresponding labels as Output (O) forinstruction tuning. For example, the work [21] instruction-tuned LLMs across various
NLP tasks, including four sen-timent datasets, while the work [3] further expanded thisapproach to more than 1,800 NLP tasks. Considering sen-timent classiﬁcation spans diverse domains such as ﬁnance,restaurants, movies, and politics, obtaining a large numberof domain-speciﬁc labeled instances for instruction tuningis labor-intensive and ineﬃcient.To enhance this aspect, we propose a simple-yet-eﬃcientinstruction augmentation method to construct sentimentaladjective-based pseudo instructions that do not rely
on anytraining instances in sentiment benchmark datasets. Subse-quently, we instruction-tune Llama2-7b,13b,70b base mod-els and the Falcon-40b base model and evaluate their zero-shot performance on 12 sentiment benchmark datasets.The results show that instruction-tuned models signiﬁ-cantly outperform the base models by 30 points and otherinstruction-tuned models by an average of 5.1 points.2 Sentimental Adjective-based In-struction ConstructionWe herein describe the steps to construct pseudo in-stances using
sentimental adjectives. Section 2.1 outlinesthe process for collecting diverse sentiment instruction text(T) from various corpora. Section 2.2 details the stepsof constr ucting instruction using sentimental adjectives forInput (I) and Output (O).2.1 Instruction Text (T) CollectionUser instructions exhibit a wide variety of paraphras-ing. To increase the diversity, we collect sentiment in-struction text (T) from ﬁve widely-used instruction datasetswritten by either human annotators or LLMs, as follows:(1) SuperNI [20], which contains 96k instructions writ-ten by humans covering 1600+ NLP tasks. (2) Alpaca1）[16], which contains 52k instructions generated by GPT-3(davinci-003). (3) Self-instruct [19], which contains 82kinstructions generated by GPT-3 (vanilla). (4) UnnaturalInstructions [8], which contains 68k instructions generatedby GPT-3 (davinci-002). (5) Baize [22], which contains210k instruction instances created by prompting ChatGPTand letting it converse with itself. We extracted all
theinstruction text (T) from these datasets and retained in-struction texts only if they contain the terms ‘sentiment’,‘positive’, ‘negative’, and ‘neutral’. Finally, 110 diversesentiment instruction text (T) are yielded, and we empir-ically determine to use 80 for training and 30 for testingduring instruction tuning. For the aspect-based sentimentclassiﬁcation task, we add with respect to the TARGET tothe instruction text and replace TARGET with the speciﬁcaspect.2.2 Sentimental Adjective (I, O) PairInspired by the concept of evaluative adjectives in lin-guistics, we describe the four steps to automatically collectpairs of instruction input (I) and output (O). Evaluativeadjectives often express value judgments and convey opin-ions, emotions, or subjective interpretations. For instance,adjectives like beautiful imply a positive sentiment, whileawful suggests a negative one. We refer to our collectedadjectives as sentimental adjectives.Step
1. Collect sentimental adjective candidatesWe start by collecting adjectives from SentiWordNet 3.02）[2] where each sense of an adjective word 𝑤 is assigned twoscores: a positive score (𝑆𝑝𝑜𝑠) and a negative score (𝑆𝑛𝑒𝑔)where 0 ≤ 𝑆𝑘≤ 1 and 𝑘 ∈ {𝑝𝑜𝑠, 𝑛𝑒𝑔}. The selectioncriteria is:1. Choose all words where at least one of its senses meetsthe criteria: 𝑆𝑝𝑜𝑠≥ 𝑟 and 𝑆𝑛𝑒𝑔= 0.0 to compilepositive word list 𝐿1𝑝𝑜𝑠2. Choose all words where at least one of its senses meetsthe criteria:: 𝑆𝑛𝑒𝑔≥ 𝑟 and 𝑆𝑝𝑜𝑠= 0.0 to compile1） https://github.com/gururise/AlpacaDataCleaned/2） https://github.com/aesuli/SentiWordNet. It is under CC BY-SA4.0 license.negative word list 𝐿1𝑛𝑒𝑔3. Choose all words where at least one of its senses meetsthe criteria: 𝑆𝑝𝑜𝑠= 0.0 and 𝑆𝑛𝑒𝑔= 0.0 to compileneutral word list 𝐿1𝑛𝑒𝑢We empirically determine the threshold 𝑟 to trade oﬀ
be-tween the number and quality of adjectives. Please seeTable 3 in Appendix for 𝐿1.Step 2. Align with sentiment word sense.This step aims to reﬁne the adjective lists in Step 1. Forinstance, one sense of the word ‘fresh’ meets the criteria𝑆𝑛𝑒𝑔≥ 0.75 and 𝑆𝑝𝑜𝑠= 0.0, this word is therefore includedin the negative list 𝐿1𝑛𝑒𝑔. However, ”fresh” often conveysa non-negative meaning, typically referring to somethingnew or unused. including this word in negative list mayconfuse the model during instruction tuning. To addressthis, we utilize pre-deﬁned positive (𝑉𝑝𝑜𝑠) and negative(𝑉𝑛𝑒𝑔) vocabularies in the paper [10]. Words in lists 𝐿1𝑝𝑜𝑠and 𝐿1𝑛𝑒𝑔are excluded if they do not appear in 𝑉𝑝𝑜𝑠and𝑉𝑛𝑒𝑔, respectively. Words in 𝐿1𝑛𝑒𝑢are removed if theyappear in either𝑉𝑝𝑜𝑠or𝑉𝑛𝑒𝑔. This process results in threereﬁned lists: 𝐿2𝑝𝑜𝑠, 𝐿2𝑛𝑒𝑔, and 𝐿2𝑛𝑒𝑢. Please see Table
4 inAppendix for 𝐿2.Step 3. Rank word by frequency.This step focuses on selecting more domain-agnostic wordsby leveraging frequency information. We use EnglishWikipedia3）to obtain word frequency for ranking adjec-tives in each list in descending order based on their fre-quency. If a adjective in 𝐿2𝑝𝑜𝑠, 𝐿2𝑛𝑒𝑔, and 𝐿2𝑛𝑒𝑢is not inthe wiki frequency list, its frequency would be set to zero.After ranking, frequent words such as best, great, and im-portant appear at the top of the positive list, whereas theoriginal words in the list are legendary, solid, and gallant.We note the ranked lists as 𝐿3𝑝𝑜𝑠, 𝐿3𝑛𝑒𝑔, and 𝐿3𝑛𝑒𝑢. Pleasesee Table 5 in Appendix for 𝐿3.Step 4. Add negation words.This step helps LLMs to better handle sentences containingnegation words. We add the negation word not directlybefore adjectives (e.g., not beautiful)
for 𝑋% of instancesin only 𝐿3𝑝𝑜𝑠and 𝐿3𝑛𝑒𝑔. Subsequently, adjectives withnegation from the positive list are transferred to the negativelist and vice versa. This process yields the ﬁnal lists: 𝐿4𝑝𝑜𝑠,3） https://jwsmythe.com/tools/wordlist/wikipedia-word-frequency-master/results/enwiki-2023-04-13.txt𝐿4𝑛𝑒𝑔, and 𝐿4𝑛𝑒𝑢(where 𝐿4𝑛𝑒𝑢= 𝐿3𝑛𝑒𝑢). Please see Table 6in Appendix for 𝐿4.After completing steps 1 to 4, we take the ﬁrst instructiontext T from 80 instruction texts in Section 2.1, the ﬁrstadjective from 𝐿4𝑝𝑜𝑠and positive to form the ﬁrst tuple (T,I, O); Continue this process until the 80th instruction textis taken. Then, we obtained 80 tuples for the positive class,80 tuples for the negative class, 80 tuples for the neuralclass respectively.3 Experiment3.1 Experimental SetupThe constructed 240 tuples are split into 80% for thetraining set and 20% for the development set. We set thethreshold 𝑟 in SentiWordNet 3.0
in Step 1 to 0.75, andnegation word percentage 𝑋 to 10%, according to perfor-mance on the development set. For training, we followthe paper [17] by utilizing an auto-regressive objective andzeroing out the loss on tokens from the user prompt, in-cluding instruction text and input, while backpropagatingonly on instruction output. Of the 110 instruction texts, weuse 80 for model training and development, and remaining30 for testing. Dur ing training, we employ the eﬃcientparameter tuning technique, LoRA [9], with a LoRA rankof 8 and LoRA alpha of 32. We set learning rate to 2e-4and batch size to 2. During inference, we follow previouswork [6] to load models in the 8-bit mode which signiﬁ-cantly speeds up the inference and has negligible impacton the ﬁnal performance. We set the maximum
number ofgenerated tokens to 20. All the experiments are conductedusing one A100 GPU.Evaluation MetricSince all the instruction texts we collected explicitly spec-ify the output space as positive, negative, or neutral label,we adopt the following metric for calculating instance-wise accuracy: 1) Score 1 if the output string containsthe g round-truth label and does not contain other classes’ground-truth labels (case insensitive); 2) Score 0, other-wise. We observed a high correlation score between thehuman annotator and this automatic metric, So we decidedto use this metric for all datasets.3.2 DatasetWe experiment with 7 general sentiment classiﬁcationdatasets, i.e., SST-2 [15], IMDB, Yelp, Amazon datasetsfrom [11], Airline4）Debate5）, ﬁnancial phrasebank [13] aswell as 5 aspect-based sentiment classiﬁcation datasets6）from the Workshop on Semantic Evaluation (SemEval) in2014, 2015, and 2016.Table 1 shows the statistics of
each dataset. We pairedeach sentence from the sentiment benchmark datasets with30 instr uction texts for testing. For instance, in the case ofSST-2, this resulted in 1,821 × 30 = 54,630 instances usedfor testing instruction-tuned models. The same procedurewas applied to the other datasets.Table 1: Statistics of sentiment classiﬁcation datasets.Dataset Domain Size # Class AspectSST-2 Movie 1,821 2 noYelp Restaurant 1,000 2 noAmazon (Amaz) Product 1,000 2 noIMDB Movie 1,000 2 noAirline Operation 1,000 3 noDebate (Deba) Politics 1,000 3 noPhraseBank (PB) Finance 970 3 noSemEval-14lap Laptop 543 3 yesSemEval-14res Restaurant 994 3 yesSemEval-15res Restaurant 485 3 yesSemEval-15hot Hotel 215 3 yesSemEval-16res Restaurant 514 3 yes3.3 ModelsWe instruction-tuned Llama2 base model [17], andfalcon-40b base model [1] using our constructed 240 in-struction tuples (T, I, O), noted as
base+ours. In addition,we consider the following comparison methods:base+ours w/o adjective Previous works, such as [12],have pointed out that some instruction-tuned models donot fully utilize instructions, and that the impressive perfor-mance gains from instruction tuning may stem from modelslearning superﬁcial patterns, such as the output space andformat. To verify this, we replaced the sentimental adjec-tives with empty strings to ablate the input, while keepingthe
instruction text and output format unchanged.4） https://www.kaggle.com/datasets/crowdﬂower/twitter-airline-sentiment. 1k instances is used only.5） https://www.kaggle.com/datasets/crowdﬂower/ﬁrst-gop-debate-twitter-sentiment. 1k instances is used only.6） https://github.com/kevinscaria/InstructABSA/tree/main/DatasetTable 2: Accuracy of zero-shot sentiment classiﬁcation on 12 benchmark datasets. Best results associated with the samebase model are in bold.Dataset SST-2 Yelp Amaz IMDB Deba Airline PB 14hap 14res 15res 15hot 16res Ave.△ Lexicon-match baseline 59.2 64.7 69.6 69.0 55.4 63.6 56.0 68.9 81.5
74.4 74.9 78.4 67.9#1 llama2-7b-base 49.4 51.2 40.8 46.7 34.7 37.4 27.3 40.0 61.5 53.8 61.5 61.6 47.2#2 llama2-7b-chat 78.8 88.8 83.5 86.2 61.6 69.9 60.5 75.5 83.6 78.9 71.5 75.7 76.2#3 base+ours w/o adjective 38.9 35.5 32.3 37.9 35.2 35.6 29.9 18.3 13.8 24.9 16.4 13.4 27.7#4 base+ours 89.5 96.1 94.1 94.4 62.0 67.6 53.5 82.1 88.4 86.3 84.4 89.4 82.3$1 llama2-13b-base 47.0 52.5 43.4 49.3 36.1 40.8 41.7 46.1 61.2 56.7 58.0 57.9 49.2$2 llama2-13b-chat 71.2 79.0 75.0 77.9 62.9 69.5 59.1 68.9 76.9 71.4 63.1 65.4 70.0$3 base+ours w/o adjective 49.6 50.4 44.4 50.7 38.7 43.1 26.3 28.0 35.1 41.9 33.9 24.9 38.9$4 base+ours 80.5 88.4 75.9 86.1 63.1 69.8 62.0 62.9 81.6 78.1 73.0 77.2 74.9&1 llama2-70b-base 55.8 42.7 43.7 48.1 34.5
39.1 31.6 44.9 45.1 47.0 44.3 54.8 44.3&2 llama2-70b-chat 81.9 90.0 87.6 88.6 64.8 72.6 68.8 74.5 80.9 77.1 72.3 67.8 77.2&3 base+ours w/o adjective 72.4 80.5 75.8 77.4 43.6 51.1 29.4 64.3 77.1 72.3 71.1 67.0 65.2&4 base+ours 92.5 97.9 95.8 96.3 63.0 71.1 55.3 80.4 89.0 85.6 88.3 85.0 83.4♢1 falcon-40b-base 69.9 72.1 61.8 63.1 36.6 42.5 27.5 50.1 65.8 66.3 60.7 67.2 57.0♢2 falcon-40b-instr. 78.9 89.2 80.0 83.2 51.5 55.2 40.3 74.7 86.3 81.3 83.3 85.3 74.1♢3 base+ours w/o adjective 63.6 58.7 46.4 53.4 36.0 38.9 23.8 35.7 56.5 51.7 51.0 53.2 47.4♢4 base+ours 92.0 91.2 87.8 88.1 55.0 62.0 43.2 77.8 84.1 80.6 80.3 85.3 77.3lexicon-match baseline We add a sentiment lexiconmatch-based model [7], which directly utilizes the pres-ence of positive (e.g.,
great, good, and nice) and negativewords (e.g., sad, bad, and worse) to determine the senti-ment polarities. This aims to determine if good perfor-mance can be achieved through simple sentimental wordmatching, without injecting these sentimental adjectivesvia instruction tuning.llama2 chat model The Llama2 chat model began super-vised ﬁne-tuning with instructions from 1.8K NLP tasks[4]. The model was further ﬁne-tuned on 27,540 annotatedinstructions and millions of human preference data via re-inforcement learning. We believe this provides a powerfulbaseline, even for our sentiment classiﬁcation task.falcon chat model It is also known as the Falcon-40B-Instruct model7）, which is ﬁne-tuned on hundreds of thou-sands of QA and dialog instances from Quora, Stack Over-ﬂow, and MedQuAD questions.4 Result and AnalysisTable 2 shows comparison results and our observationsare as follows:(1) Our instruction-tuned models (base+ours)
outperformall base models by 30 points and even all chat modelsby 5.1 points on average. Moreover, our instr uction-tunedLlama2-70B model achieves the best average performance,7） https://huggingface.co/tiiuae/falcon-40b-instructsuggesting that model size remains an important factor inthe eﬀectiveness of instr uction tuning.(2) The results of base+ours w/o adjective show sig-niﬁcant performance degradation for Llama2-7B (#3),Llama2-13B ($3), and Falcon-40B (♢). While the ”empty-input” instruction tuning boosts Llama2-70B’s perfor-mance to some extent (&3), combining it with our senti-mental adjectives achieves the best performance (&4). Thisveriﬁes that the improvements are largely not attributed tolearning the output space formats, such as positive andnegative labels, as reported
by previous work [12].(3) To investigate whether our base+ours models simplymemorize sentimental adjectives for making predictions,we added a sentiment lexicon match-based model for com-parison. The results show that our models signiﬁcantlyoutperform this baseline (△), indicating that incorporatingsentimental adjectives into LLMs through instruction tun-ing equips the models to handle not only straightforwardsentiment lexicon-based cases but also more challengingcases lacking explicit sentiment lexicons.5 ConclusionIn this work, we create pseudo sentimental instructionsto ﬁne-tune LLMs. Experiments show signiﬁcant perfor-mance gains on various sentiment benchmarks. Notably, itrequires no ground-truth training data and generalizes wellacross domains. Future work will extend this approach toﬁne-grained emotion classiﬁcation.



References


[1] Ebtesam Almazrouei, Hamza Alobeidli, AbdulazizAlshamsi, Alessandro Cappelli, Ruxandra Cojocaru,M´erouane Debbah,´Etienne Goﬃnet, Daniel Hesslow,Julien Launay, Quentin Malartic, et al. The fal-con series of open language models. arXiv preprintarXiv:2311.16867, 2023.
[2] Stefano Baccianella, Andrea Esuli, Fabrizio Sebastiani,et al. Sentiwordnet 3.0: an enhanced lexical resource forsentiment analysis and opinion mining. In Lrec, Vol. 10,pp. 2200–2204, 2010.
[3] Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, et al. Scalinginstruction-ﬁnetuned language models. arXiv preprintarXiv:2210.11416, 2022.
[4] Hyung Won Chung, Le Hou, Shayne Longpre, BarretZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, et al. Scalinginstruction-ﬁnetuned language models. Journal of Ma-chine Learning Research, Vol. 25, No. 70, pp. 1–53,2024.
[5] Xiang Deng, Vasilisa Bashlovkina, Feng Han, SimonBaumgartner, and Michael Bendersky. LLMs to the Moon?Reddit market sentiment analysis with large language mod-els. In Companion Proceedings of the ACM Web Con-ference 2023, pp. 1014–1019, 2023.
[6] Tim Dettmers, Mike Lewis, Younes Belkada, and LukeZettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication fortransformers at scale. Advances in Neural InformationProcessing Systems, Vol. 35, pp. 30318–30332, 2022.
[7] Eric Gilbert. Vader: A parsimonious rule-based model forsentiment analysis of social media text. In Proceedings ofthe international AAAI conference on web and socialmedia, Vol. 8, pp. 216–225, 2014.
[8] Or Honovich, Thomas Scialom, Omer Levy, and TimoSchick. Unnatural instructions: Tuning language mod-els with (almost) no human labor. In Proceedings of the61st Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), Toronto,Canada, July 2023.
[9] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. Lora: Low-rank adaptation of large language mod-els. arXiv preprint arXiv:2106.09685, 2021.
[10] Minqing Hu and Bing Liu. Mining and summarizingcustomer reviews. In Proceedings of the tenth ACMSIGKDD international conference on Knowledge dis-covery and data mining, pp. 168–177, 2004.
[11] Dimitrios Kotzias, Misha Denil, Nando De Freitas, andPadhraic Smyth. From group to individual labels us-ing deep features. InProceedings of the 21th ACMSIGKDD international conference on knowledge dis-covery and data mining, pp. 597–606, 2015.
[12] Po-Nien Kung and Nanyun Peng. Do models really learnto follow instructions? an empirical study of instruc-tion tuning. In Anna Rogers, Jordan Boyd-Graber, andNaoaki Okazaki, editors, Proceedings of the 61st An-nual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pp. 1317–1328,Toronto, Canada, July 2023. Association for Computa-tional Linguistics.
[13] P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala.Good debt or bad debt: Detecting semantic orientations ineconomic texts. Journal of the Association for Infor-mation Science and Technology, Vol. 65, , 2014.
[14] Kevin Scaria, Himanshu Gupta, Siddharth Goyal,Saurabh Arjun Sawant, Swaroop Mishra, and Chitta Baral.Instructabsa: Instruction learning for aspect based senti-ment analysis. arXiv preprint arXiv:2302.08624, 2023.
[15] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher D. Manning, Andrew Ng, and ChristopherPotts. Recursive deep models for semantic compositional-ity over a sentiment treebank. In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing, pp. 1631–1642, Seattle, Wash-ington, USA, October 2013. Association for Computa-tional Linguistics.
[16] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, YannDubois, Xuechen Li, Carlos Guestrin, Percy Liang, andTatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.
[17] Hugo Touvron, Louis Mar tin, Kevin Stone, Peter Albert,Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.Llama 2: Open foundation and ﬁne-tuned chat models.arXiv preprint arXiv:2307.09288, 2023.
[18] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, JackHessel, Tushar Khot, Khyathi Raghavi Chandu, DavidWadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy,et al. How far can camels go? exploring the state ofinstruction tuning on open resources. arXiv preprintarXiv:2306.04751, 2023.
[19] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-isa Liu, Noah A. Smith, Daniel Khashabi, and Han-naneh Hajishirzi. Self-instruct: Aligning language modelswith self-generated instructions. In Proceedings of the61st Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), Toronto,Canada, July 2023.
[20] Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-labashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik,Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunk-umar, David Stap, et al. Super-naturalinstructions: Gener-alization via declarative instructions on 1600+ nlp tasks.In Proceedings of the 2022 Conference on EmpiricalMethods in Natural Language Processing, pp. 5085–5109, 2022.
[21] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai,and Quoc V Le. Finetuned language models are zero-shotlearners. arXiv preprint arXiv:2109.01652, 2021.
[22] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.Baize: An open-source chat model with parameter-eﬃcient tuning on self-chat data. arXiv preprintarXiv:2304.01196, 2023.

positive words negative words neutral wordssophisticated contemptible last-ditchmagna-cum-laude bogus alategorgeous salt ﬂooredboss unfree quadrilateralheaven-sent hidden fortyexhaustive inhumane french-speakingsuperb humble combinedhealthy false client-server... ... ...Table 3: Step 1. Collect sentiment al adjectives candidates.positive words negative words neutral wordssophisticated contemptible alategorgeous bogus quadrilateralsuperb inhumane fortyhealthy false french-speakingmeticulous precarious combinedperfect upset client-serversweet numb trojancoherent indelicate diagonal... ... ...Table 4: Step 2. Align with sentiment word
sense.positive words negative words neutral wordsbest dead newgreat poor moreimportant diﬃcult nationalgood unable mostbetter bad manysupreme wild americangolden cold earlygreatest oﬀensive high... ... ...Table 5: Step 3. Rank word by frequency.positive words negative words neutral wordsbest dead newgreat poor moreimportant diﬃcult nationalgood unable mostbetter bad manysupreme wild americangolden cold earlynot oﬀensive not greatest high... ... ...Table 6: Step 4. Add negation words.