Enhancing the JNLI Dataset and Evaluating Model Performanceon Improved Data

Jun Liang Hiroaki Fujimoto Masahiro Fukuyori



Fujitsu Limited, AI Laboratory, Japan



{liang-jun, fujimoto.hiroak, fukuyori}@fujitsu.com

AbstractThe Japanese Natural Language Inference (JNLI) datasetis a valuable resource for NLI research. However, we foundit contains inconsistencies and lacks structural diversity.This paper presents a two-pronged approach to addressthese limitations: A rigorous correction of errors and thecreation of a new, expanded dataset with diverse sentencestructures. We detail our iterative correction methodology,leveraging Large Language Model (LLM) predictions andmanual review. The new dataset introduces variations insentence type (noun, verb, adjective/quantiﬁer), enrichingthe data. Furthermore, we evaluate the performance ofour internally created LLM model Takane on the original,corrected, and newly created JNLI datasets, demonstratingsuperior performance compared to
existing state-of-the-artmodels.1 IntroductionNatural Language Inference (NLI) remains a challengingyet crucial task in Natural Language Processing (NLP). In aNLI task, the goal is to determine the semantic relationshipbetween a pair of sentences: a premise and a hypothesis.Premise: This is the given sentence, the statement that pro-vides context or background information. Think of it as theestablished fact or assertion. Hypothesis: This is the sen-tence that needs to be evaluated in relation to the premise.It is a claim or statement that is being tested against thepremise. The task is to determine the relationship betweenthe premise and the hypothesis. This relationship is typi-cally categorized into one of three classes/labels: Entail-ment: The hypothesis is logically implied by
the premise.In other words, if the premise is true, the hypothesis mustalso be true. There is a clear logical connection. Contra-diction: The hypothesis directly contradicts the premise.If the premise is true, the hypothesis must be false. Theyare opposing statements. Neutral: There is no clear logicalrelationship between the premise and the hypothesis. Thetruth of one doesn’t necessarily aﬀect the truth of the other.They are independent statements.The creation and generation of high-quality, diversedatasets is helpful to evaluate a LLM’s performance onlogical thinking. The Japanese Natural Language Infer-ence (JNLI) dataset[1] serves as an important resource forJapanese NLI research. However, its limitations, includinginconsistencies and a lack of structural diversity, necessi-tate improvements. This paper addresses these
limitationsby presenting:• A reﬁned JNLI dataset through error correction;• A novel, expanded JNLI dataset with diverse sentencestructures.We then evaluate the performance of Takane on these im-proved datasets and compare its performance against ex-isting state-of-the-art models, demonstrating the eﬀective-ness of our data enhancement strategies.2 MethodologyIn this section, data correction and data creation arewritten as below:2.1 Dataset CorrectionOf 88 randomly selected original JNLI ground truth(GT) sentence pairs reviewed by human annotators, only 24(about 27%) were judged logically correct; the remainderwere deemed incorrect or inappropriate. Thus, our datasetcorrection process begins by identifying inconsistencies inthe original JNLI validation set. Originally, a sentencepair is chosen as GT if 6 out of 10 annotators give the samelabel. In
our procedure, we ask 5 annotators to manuallycheck the sentence pairs without telling them the GT in theold dataset. We try to keep the fairness of the judgementand identify the types of the potential errors by settingdiﬀerent judge agreement rules. We represent three diﬀer-ent conﬁdence percentages for diﬀerent inference labels:entailment, neutral, contradiction. 60% judge agreement:means for one sentence pair, 3 out of 5 annotators have thesame judgement. 80% judge agreement: means for onesentence pair, 4 out of 5 annotators have the same judge-ment. 100% judge agreement: means for one sentencepair, 5 out of 5 annotators have the same judgement.Firstly, we conform to the original paper[2] preparationprocedure, ﬁnding that the sentence pair
is made as the cap-tion of a photo. The annotators are asked to give sentencepairs which can be used to describe this photo. Besidesthat, the relation between the two sentences in the sentencepair should be one of the three diﬀerent inference labels:entailment, neutral, contradiction.Figure 1: The review ﬂow to determine the validity andlabel of a sentence pairFigure 1 depicts a review process for evaluating the ap-propriateness of two sentences as captions for a photo. Theprocess begins by determining if the contents of the twosentences are "close" enough to be considered appropriatecaptions for the same photo. Then, we sequentially deter-mine the appropriate label. Please check the Appendix A.1for more details.2.2 Dataset CreationThe original
work only asked annotators to describe thephoto without any prerequisite. To address the lack ofstructural diversity in the original JNLI dataset, we cre-ate a new expanded JNLI dataset. This dataset focuseson expanding the var iety of sentence str uctures. For eachpremise, we generate three hypothesis sentences, each ex-hibiting diﬀerent types of transformation: noun, verb, andadjective/quantiﬁer. Each hypothesis is carefully crafted torepresent one of the three types of transformation. Pleasecheck the Appendix A.2 for more details. The creation ofthis dataset aims to provide a more challenging and rep-resentative benchmark for LLM training, avoiding overlysimplistic examples that could lead to inﬂated performancemetrics.2.2.1 Basic PolicyThe basic policies of the newly created dataset creationare:• Conform to
the original paper preparation procedureThe sentence pair is given as the caption of a photo,and three diﬀerent transformations of sentences arecreated without the photo.• Wide varietyCreate text with three diﬀerent transformations onone source text.• Ensuring data qualityTo ensure data quality and prevent overﬁtting toeasily processed data, we instruct annotators to createa more complex dataset that challenges even humanjudgment at ﬁrst glance. Learning on complex datacan also be very diﬃcult, but complex is good forevaluation.2.2.2 Creation ProcedureThe new dataset creation procedure is as shown below:1. Create a new premise that can be used as a caption fora photo which is not from the original text.2. Create three hypotheses following the new premise,one for
each of entailment, contradiction, or neutralaccording to the instructions and ideas on the fol-lowing creation ﬂow in Figure 1. The newly createdsentence pair is totally diﬀerent from the originallyprovided sentence pair on content.3. Enhance each hypothesis with three transformations(noun, verb, adjective/quantiﬁer) for entailment, con-tradiction, or neutral.4. Check if the created data label: Entailment, Contra-diction, and Neutral is correct by validating it againthrough the ﬂow in Figure 1. Thus, there are ninepremise-hypothesis sentence pairs for one photo.Please check the Appendix A.2 for the details.3 Experimental Setup and ResultsWe evaluate our Takane model with the current state ofart models on the original dataset, corrected dataset, andnewly created dataset. We also evaluate the inﬂuence
ofdiﬀerences in judge agreements on inference results.3.1 Models Accuracy on Three DatasetsTo evaluate the impact of our dataset enhancements, weconducte an experiment on these three datasets. Mean-while, we also compare Takane model with current state-ot-art models.The models are evaluated on the original JNLI validationset, correctd JNLI valication dataset, and newly createdJNLI dataset.Model Original Corrected NewTakane 0.890 0.923 0.770Command-R-plus-08-2024 0.689 0.693 0.672Command-R-plus 0.644 0.645 0.626GPT-4-0613 0.832 0.840 0.696GPT-4 0.835 0.840 0.698GPT-3.5 0.816 0.840 0.709GPT-3.5-Turbo 0.719 0.719 0.663Table 1: Model accuracy on these three types of datasetsGPT series models are based on underlying principles[3] and [4]. Command R+ models are from https://docs.cohere.com/v2/docs/command-r-plus. Takaneis described in https://pr.fujitsu.com/jp/news/2024/09/30.html. The accuracy for each model is re-ported in
Table 1. To ensure a fair comparison, the "Cor-rected Dataset" results use 80% annotator judge agree-ment―a more robust threshold rather than the original60%. This decision is supported by our experiments onJudge Agreement in the next section. The Takane modeloutperforms all other models.3.2 Eﬀect of Diﬀerent Judge AgreementsSince Takane reaches the best performance on the cor-rected datasets, we also evaluate the model on the newlycreated dataset with three diﬀerent annotator judge agree-ments.This experiment also shows that Takane performs theModel 60% 80% 100%Takane 0.878 0.923 0.945Command-R-plus-08-2024 0.667 0.693 0.711Command-R-plus 0.627 0.645 0.664GPT-4-0613 0.807 0.840 0.877GPT-4 0.807 0.840 0.878GPT-3.5 0.807 0.840 0.877GPT-3.5-Turbo 0.686 0.719 0.767Table 2: Model accuracy on diﬀerent judge agreementsbest among all
the models.Furthermore, we evalute the precision, recall, and F1-score of "entailment", "neutral", "contradiction" in eachjudge agreement.As shown in Figure 2, precision, recall, F1-score, andaccuracy all increase with higher annotator judge agree-ment (60%, 80%, 100%). However, the performance im-provement from 60% to 80% annotator judge agreement isgreater than the improvement from 80% to 100%. There-fore, 80% annotator judge agreement represents the opti-mal balance between data quality and performance gains.3.3 Analysis Ratios of Inference LabelsWe also analyze the ratios the three inference labels ineach judge agreement in the corrected dataset, ﬁnding thatthe "neutral" label takes up roughly 50% of the valid datasetaccording to the result in Figure 3. To equally evaluate thelabels of the
inference, it is better to make a balanceddataset to validate the performance of the models.4 DiscussionThe results presented in Table 1 and Table 2 reveal a sig-niﬁcant impact of data quality and structural diversity onthe performance of NLI models. Takane consistently out-performs state-of-the-art models across all three datasets,demonstrating the eﬀectiveness of our data enhancementapproach. The augmentation in accuracy observed whenmoving from the original to the corrected dataset (Table1) quantiﬁes the negative impact of inconsistencies in theoriginal JNLI dataset. This rise highlights the importanceof addressing these errors for reliable model evaluation.Furthermore, the performance diﬀerence between the cor-rected and the new dataset demonstrates the signiﬁcantimpact of str uctural diversity. The introduction of variedsentence
structures, as detailed in Section 2.2, creates aFigure 2: Takane model’s Precision, Recall, F1-score among diﬀerent judge agreementsFigure 3: Inference label ratios analysis across diﬀerent annotator judge agreementsmore challenging and realistic benchmark for evaluatingNLI models. The fact that all models’ performance de-creases on the structurally diverse dataset, while Takaneis still outperforming other models, suggests that currentstate-of-the-art models still struggle with more complexsyntactic str uctures.The analysis of diﬀerent inter-annotator judge agreementlevels (Table 2) reinforces this observation. The consistentincrease in accuracy judging by Table 2 with increasingagreement thresholds (from 60% to 100%) is expected, ashigher ag reement levels indicate more nuanced and chal-lenging examples. However, Takane consistently outper-forms other models at all these three
agreement levels,demonstrating its robustness. The detailed precision, re-call, and F1-score analysis (Figure 2) further illuminatesthe model’s strengths and weaknesses across diﬀerent la-bels at each agreement level.The analysis of the original valid dataset inference labeldistribution reveals a notable class imbalance, with neutralexamples consistently comprising approximately 50% ofthe dataset across all agreement levels. This imbalance mayinﬂuence model evaluation, potentially leading to inﬂatedperformance metrics for the other certain inference labels.5 ConclusionThis study demonstrated the crucial role of data qualityand structural diversity in advancing Japanese NLI. Ourtwo-pronged approach for rigorous error correction andthe creation of a structurally diverse expanded dataset sig-niﬁcantly improved the JNLI dataset. Both data correctionand data creation resulted in superior performance for ourTakane
model compared to existing state-of-the-art mod-els. Analysis revealed the impact of data inconsistenciesand label imbalance.Future research should focus on several key areas. First,expanding the dataset further, particularly addressing thelabel imbalance and incorporating a wider range of sen-tence structures, is crucial. Second, investigating alterna-tive error correction methods and exploring the use of ac-tive learning techniques for dataset expansion could furtherenhance data quality. Third, evaluating the transferabilityof our improved dataset to other downstream NLP tasksis essential to assess its broader impact. Finally, a deeperlinguistic analysis of the factors contributing to the diﬃ-culty of Japanese NLI tasks will inform the design of futuredatasets and models, leading to more robust and reliablesystems for Japanese language understanding.

AcknowledgementsWe are extremely grateful to Shigeyuki Odashima andSusumu Tokumoto for their consistent support throughoutthis research. We also thank Shigeyuki Odashima and PelatGuillaume for their proofreading.

References


[1] Kentaro Kurihara, Daisuke Kawahara, and Tomohide Shi-bata. JGLUE: Japanese general language understand-ing evaluation. In Proceedings of the Thirteenth Lan-guage Resources and Evaluation Conference, Mar-seille, France, 2022. European Language Resources As-sociation.
[2] Samuel R. Bowman, Gabor Angeli, Christopher Potts, andChristopher D. Manning. A large annotated corpus for learn-ing natural language inference, 2015.
[3] Tom B. Brown et al. Language models are few-shot learners,2020.
[4] OpenAI et al. Gpt-4 technical report, 2024.

A AppendixThe details of data correction and data creation are asshown below.A.1 Data Correction ProcessIn this section, review ﬂowchart is described to betterhelp understand data correction process.• If the contents are close: The process moves to checkfor entailment. Entailment means one sentence log-ically implies the other. We have created entailmentcheck steps (the details will be shown in the datasetreview ﬂow: Figure 1). If the sentence pair is judgedas "entailment", then the ﬂow stops. If
the sentencepair is judged as "not entailment", the process thenchecks for "contradiction". Contradiction means thesentences oppose each other. The process followsthe contradiction check steps. If the contradiction isfound, the result is "contradiction"; if not, the ﬁnaloutput is "neutral".• If the contents are far: The process determines thesentences are inappropriate as datasets, resulting inthe output "Invalid" as data inappropriation. In short,the ﬂowchart outlines a decision tree for classifyingsentence pairs (intended as image captions) into oneof four
labels: entailment, contradiction, neutral, orinvalid. The core logic relies on assessing the se-mantic closeness of the sentences and then applyingspeciﬁc rules for entailment and contradiction deter-mination. The determination steps on entailement andcontradiction will be shown in Figure 1.A.2 Data Creation ProcessThis section details the creation of the expanded JNLIdataset, focusing on increased structural diversity to createmore challenging examples for NLI model evaluation. Theprocess adhered to these principles:1. Adherence to Original Methodology: Sentencepairs are created as
if they are photo captions, butwithout providing the actual photos.2. Structural Diversity: Three sentence transforma-tions (noun, verb, adjective/quantiﬁer) are applied toeach premise.3. Data Quality Assurance: Annotators are instructedto create complex examples challenging even humanjudgment.The procedure of data creation is descr ibed in 2.2.2.A.2.1 Data Creation ExampleFor one original sentence pair, we expand it into threelabels. Three transformations are made for each label. Thenewly created premise should be totally diﬀerent from theoriginal premise.Entailment• Noun: Premise: 猫が芋虫にじゃれています。
Hypothesis: 生き物が芋虫にじゃれています。
• Verb: Premise: 猫が物陰からネズミを狙っています。
Hypothesis: 猫がネズミを見ています。
• Adjective/Quantiﬁer: Premise: 映画館で男女のペアが話をしている。
Hypothesis: 映画館で夫婦が話をしている。
Contradiction• Noun: Premise: 公園で女の子がボール遊びをしている。
Hypothesis: 公園で少年がボール遊びをしている。
• Verb: Premise: 公園で女の子がかけっこをしている。
Hypothesis: 公園で女の子が泣いている。
• Adjective/Quantiﬁer: Premise: 公園で幼い女の子がブランコに乗っている。
Hypothesis: 公園で老婆がブランコにのっている。
Neutral• Noun: Premise: 映画館で男女のペアが話をしている。
Hypothesis: 映画館で夫婦が話をしている。
• Verb: Premise: 映画館で夫婦が話をしている。
Hypothesis: 映画館で夫婦がけんかをしている。
• Adjective/Quantiﬁer: Premise: 映画館で若いカップルがポップコーンを買っている。
Hypothesis:映画館で若いカップルがキャラメル味のポップコーンを買っている。