Transformer LLM ã®å†…éƒ¨æŒ™å‹•æ”¹å–„ï¼šéš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®æ•°å€¤çš„åæŸæ€§ã®å‘ä¸Š

æŸ´ç”° åœ­æ‚Ÿ

1

é«˜æ©‹ è‰¯å…

1

çŸ¢é‡ ä¸€æ¨¹

1

æ å®°æˆ

1

æ± ç”° èˆª

1

éˆ´æœ¨ æ½¤

1,2,31

æ±åŒ—å¤§å­¦

2

ç†åŒ–å­¦ç ”ç©¶æ‰€

3

å›½ç«‹æƒ…å ±å­¦ç ”ç©¶æ‰€



shibata.keigo.p1@dc.tohoku.ac.jp



æ¦‚è¦

Transformer ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€å„å±¤ã®äºˆæ¸¬åˆ†å¸ƒã®æ„å‘³çš„åæŸæ€§ãŒç¤ºã•ã‚Œã‚‹ä¸€æ–¹ã§ã€éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®æ•°å€¤çš„åæŸæ€§ãŒç¢ºèªã•ã‚Œã¦ãŠã‚‰ãšã€ä¸¡è€…ã«ä¹–é›¢ãŒã‚ã‚‹ã“ã¨ãŒå•é¡Œã§ã‚ã‚‹ã€‚
æœ¬ç ”ç©¶ã§ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«è‡ªå·±è’¸ç•™ã‚’é©ç”¨ã—ã€éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®æ•°å€¤çš„åæŸæ€§ã‚’æ”¹å–„ã™ã‚‹ã€‚
è‡ªå·±è’¸ç•™ã¯ã€åŒä¸€ãƒ¢ãƒ‡ãƒ«å†…ã§æ·±ã„å±¤ã‹ã‚‰æµ…ã„å±¤ã¸çŸ¥è­˜ã‚’è’¸ç•™ã—ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®åæŸæ€§ã‚’é«˜ã‚ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã‚ã‚‹ã€‚
ææ¡ˆæ‰‹æ³•ã«ã‚ˆã‚Šã€æœ€çµ‚å±¤ä»˜è¿‘ã§ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®åæŸæ€§ãŒå‘ä¸Šã—ãŸã€‚



1 ã¯ã˜ã‚ã«

Transformer ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰ã€æ„æ€æ±ºå®šéç¨‹ã®è©³ç´°ãªè§£æã‚„è§£é‡ˆãŒå¯èƒ½ã§ã‚ã‚‹ã€‚
ç‰¹ã«ã€å„å±¤ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’èªå½™æ¬¡å…ƒã«å°„å½±ã™ã‚‹ã“ã¨ã§ã€Transformer å†…éƒ¨ã§æ¬¡å˜èªäºˆæ¸¬ã®æ¨è«–éç¨‹ã‚’å¯è¦–åŒ–ã§ãã‚‹[1, 2]ã€‚
ã“ã®æ‰‹æ³•ã«ã‚ˆã‚Šã€å„å±¤ã§äºˆæ¸¬ã•ã‚Œã‚‹å˜èªãŒæœ€çµ‚å±¤ã§äºˆæ¸¬ã•ã‚Œã‚‹å˜èªã«æ„å‘³çš„ã«è¿‘ã¥ã„ã¦ã„ãå‚¾å‘ãŒè¦³æ¸¬ã§ãã‚‹ã€‚
ã“ã‚Œã‚‰ã®ç ”ç©¶ã‹ã‚‰å„å±¤ã®äºˆæ¸¬åˆ†å¸ƒã®æ„å‘³çš„ãªåæŸæ€§ã¯ç¤ºå”†ã•ã‚Œã‚‹ãŒã€ä¸€æ–¹ã§ã€éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«è‡ªä½“ã®æ•°å€¤çš„ãªåæŸæ€§ã¯ç¢ºèªã•ã‚Œã¦ã„ãªã„ã€‚
å®Ÿéš›ã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ç”¨ã„ãŸèª¿æŸ»ã«ã‚ˆã‚Šã€éš ã‚ŒçŠ¶æ…‹ã¯ä¸­é–“å±¤ã§ã¯ã»ã¨ã‚“ã©å¤‰åŒ–ã›ãšã€æœ€çµ‚å±¤ä»˜è¿‘ã§å¤§ããå¤‰åŒ–ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹[3]ã€‚
ã“ã®çµæœã¯ã€ä¸­é–“å±¤ã®ãƒ¢ãƒ‡ãƒ«ã¸ã®å¯„ä¸ãŒå°ã•ã„ä¸€æ–¹ã§ã€æœ€çµ‚å±¤ä»˜è¿‘ã§ã¯éš ã‚ŒçŠ¶æ…‹ãŒå¤§ããå¤‰åŒ–ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ãŠã‚Šã€äºˆæ¸¬åˆ†å¸ƒã«åŸºã¥ãçµæœã¨ã¯ç•°ãªã‚‹æ€§è³ªã‚’æŒã¤ã€‚
ç›´æ„Ÿçš„ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æµ…ã„å±¤ã§ã¯å¤§ã¾ã‹ãªæ¬¡å˜èªäºˆæ¸¬ã‚’è¡Œã†ãŸã‚ã«éš ã‚ŒçŠ¶æ…‹ãŒå¤§ããå¤‰åŒ–ã—ã€æ·±ã„å±¤ã§ã¯äºˆæ¸¬ã‚’å¾®èª¿æ•´ã™ã‚‹æ®µéšã«ç§»è¡Œã™ã‚‹ãŸã‚ã€éš ã‚ŒçŠ¶æ…‹ã®å¤‰åŒ–ã¯å°ã•ããªã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã‚‹ï¼ˆå›³ 1ï¼‰ã€‚
ã“ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®æ•°å€¤çš„ãªé·ç§»ã‚’ã€Œéš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«InputLayer 0Layer 1Layer 2Layer 3ğ’‰ğŸğ’‰ğŸğ’‰ğŸInputğ’‰ğ¥ğšğ¬ğ­ğ’‰ğ¥ğšğ¬ğ­ğŸğŸğŸNot ConvergedInputğ’‰ğ¥ğšğ¬ğ­ğŸğŸğŸå›³ 1: Transformer ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®é·ç§»ã€‚
äº‹å‰å­¦ç¿’æ¸ˆã¿ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆé’ç‚¹ï¼‰ã§ã€ä¸­é–“å±¤ãŒéš ã‚ŒçŠ¶æ…‹ã®å¤‰åŒ–ã«å¯„ä¸ã›ãšã«ã€æœ€çµ‚å±¤ã§å¤§ããªå¤‰åŒ–ãŒèµ·ã“ã£ã¦ã„ã‚‹ã€‚
ä¸€æ–¹ã§ã€è‡ªå·±è’¸ç•™å¾Œã®ãƒ¢ãƒ‡ãƒ«ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆèµ¤ç‚¹ï¼‰ã¯ã€æ·±ã„å±¤ã»ã©éš ã‚ŒçŠ¶æ…‹ã®å¤‰åŒ–ãŒå°ã•ããªã£ã¦ãŠã‚Šã€åæŸæ€§ãŒæ”¹å–„ã—ã¦ã„ã‚‹ã€‚ã®åæŸæ€§ã€ã¨å®šç¾©ã™ã‚‹ã€‚
æœ¬ç ”ç©¶ã®ç›®æ¨™ã¯ã€äºˆæ¸¬åˆ†å¸ƒã®æ„å‘³çš„åæŸæ€§ã¨éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®æ•°å€¤çš„ãªåæŸæ€§ã®ä¹–é›¢ã«å¯¾å‡¦ã—ã€éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®æ•°å€¤çš„åæŸæ€§ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šè§£é‡ˆæ€§ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
æ„å‘³çš„åæŸæ€§ã‚’ä¿ã¡ãªãŒã‚‰ã€éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®æ•°å€¤çš„ãªåæŸæ€§ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«ã€ä¸­é–“è¡¨ç¾ã‚’åˆ©ç”¨ã—ã¦ã€åŒä¸€ãƒ¢ãƒ‡ãƒ«å†…ã§çŸ¥è­˜è’¸ç•™ã‚’è¡Œã†è‡ªå·±è’¸ç•™[4]ã‚’äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦è¡Œã†ã€‚
ã“ã®çµæœã€æœ€çµ‚å±¤ä»˜è¿‘ã§ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®ç§»å‹•é‡ãŒæ¸›å°‘ã—ã€éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®åæŸæ€§ãŒæ”¹å–„ã•ã‚ŒãŸï¼ˆå›³ 3ï¼‰ã€‚
ã•ã‚‰ã«ã€æ„å‘³çš„åæŸæ€§ã¨ã®æ•´åˆæ€§ãŒç¢ºèªã•ã‚ŒãŸï¼ˆå›³ 7)ã€‚



2 éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®åæŸæ€§

Transformer ã«ã‚ˆã‚‹ LLM ãŒæ–‡ç« ã‚’ç”Ÿæˆã™ã‚‹å‡¦ç†ã¯ã€æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹è¨ˆç®—ï¼ˆnext token prediction)ã®è‡ªå·±å›å¸°çš„ãªå‡¦ç†ã§æˆã‚Šç«‹ã£ã¦ã„ã‚‹ã€‚
æœ¬ç ”ç©¶ã§ã¯ã€æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹å‡¦ç†ã‚’ã€å…¥åŠ›å±¤ã®å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‡ºç™ºç‚¹ã¨ã—ã¦ã€å‡ºåŠ›â€• 1704 â€•å±¤ã®æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆã®è¿‘å‚ï¼‰ã‚’åˆ°é”ç‚¹ã¨ã—ãŸå„å±¤ã®éš ã‚Œãƒ™ã‚¯ãƒˆãƒ«ã«ã‚ˆã‚‹åŸ‹ã‚è¾¼ã¿ç©ºé–“å†…ã®ç§»å‹•ã¨æ‰ãˆã‚‹ã€‚
ã“ã®æ™‚ã€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“å†…ã®ç§»å‹•è»Œè·¡ï¼ˆå†…éƒ¨æŒ™å‹•ï¼‰ã«é–¢ã—ã¦ã€ãƒ©ãƒ³ãƒ€ãƒ ã«ç§»å‹•ã™ã‚‹ã‚ˆã‚Šã‚‚åæŸæ€§ã®æ€§è³ªãŒã‚ã‚‹ã“ã¨ãŒæœ›ã¾ã—ã„ã€‚
åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“å†…ã«ãŠã„ã¦ã€å„å±¤ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®ç§»å‹•è»Œè·¡ã«é–¢ã™ã‚‹åæŸæ€§ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã™ã‚‹ã€‚
å®šç¾© 1 (éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®ç§»å‹•é‡). ãƒ¢ãƒ‡ãƒ«ã®å±¤æ•°ã‚’ ğ¿ ã¨ã™ã‚‹ã€‚
å±¤ ğ‘™ï¼ˆ0 â‰¤ ğ‘™ â‰¤ ğ¿ï¼‰ã®å…¥å‡ºåŠ›ã‚’ â„ğ‘™âˆ’1, â„ğ‘™ã¨ã—ï¼ˆå±¤ 0 ã®å…¥åŠ›ã¯ Input ã¨ã™ã‚‹ï¼‰ã€å±¤ ğ‘™ ã§ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®ç§»å‹•é‡ã‚’æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã™ã‚‹ã€‚
ğ·ğ‘™= 1 âˆ’1 + cosine_similarity(â„ğ‘™âˆ’1, â„ğ‘™)2(1)ã“ã“ã§ã€cosine_similarity ã¯ ï¼’ ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’æ±‚ã‚ã‚‹é–¢æ•°ã§ã‚ã‚Šã€ğ·ğ‘™ã¯ 0 â‰¤ ğ·ğ‘™â‰¤ 1 ã®ç¯„å›²ã«åã¾ã‚‹ã€‚
å®šç¾© 2 (éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®åæŸæ€§). å±¤ç•ªå· ğ‘™ï¼ˆ0 â‰¤ ğ‘™ â‰¤ ğ¿ï¼‰ã«ãŠã‘ã‚‹ç§»å‹•é‡ ğ·ğ‘™ã®å˜èª¿æ¸›å°‘æ€§ã‚’è©•ä¾¡ã™ã‚‹æŒ‡æ¨™ ğ‘†modelã‚’å¼ 2 ã«å®šç¾©ã™ã‚‹ã€‚
ğ·ğ‘™ãŒå˜èª¿æ¸›å°‘ã«è¿‘ã„ã»ã©ã€ğ‘†modelã®å€¤ã¯ 1 ã«è¿‘ã¥ãã€‚
ğ‘†model= 1 âˆ’î›ğ¿ğ‘™=1max(0, ğ·ğ‘™âˆ’ ğ·ğ‘™âˆ’1)î›ğ¿ğ‘™=1|ğ·ğ‘™âˆ’ ğ·ğ‘™âˆ’1|(2)

3 è‡ªå·±è’¸ç•™ï¼ˆSelf Distillationï¼‰

è‡ªå·±è’¸ç•™ã¯ã€æ·±ã„å±¤ã‚’æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã€æµ…ã„å±¤ã‚’ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€æ·±ã„å±¤ã‹ã‚‰æµ…ã„å±¤ã¸çŸ¥è­˜è»¢ç§»ã‚’æ‰‹æ³•ã§ã‚ã‚‹ã€‚
[4]ã€‚
åˆ¥ã€…ã®æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã€ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨æ„ã—ã¦æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã¸çŸ¥è­˜è»¢ç§»ã™ã‚‹çŸ¥è­˜è’¸ç•™ã«æ¯”ã¹ã¦ã€ä½ã‚³ã‚¹ãƒˆã§é«˜ã„æ€§èƒ½ã‚’é”æˆã§ãã‚‹ã€‚
æœ¬ç ”ç©¶ã§ç”¨ã„ã‚‹ã€ãƒ¢ãƒ‡ãƒ«ã®è‡ªå·±è’¸ç•™æ–¹æ³•ã‚’å›³ 2 ã«ç¤ºã™ã€‚
æå¤±é–¢æ•°ã¯ã€æœ€çµ‚å±¤ã®äºˆæ¸¬åˆ†å¸ƒã¨ã€å„å±¤ã®äºˆæ¸¬åˆ†å¸ƒã® KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®é‡ã¿ä»˜ãå’Œï¼ˆå¼ 4ï¼‰ã¨ã€éš£æ¥ã—ãŸéš ã‚ŒçŠ¶æ…‹ã®äºŒä¹—ãƒãƒ«ãƒ ã®é‡ã¿ä»˜ãå’Œï¼ˆå¼ 5ï¼‰ã§ã‚ã‚‹ã€‚
é‡ã¿ ğ‘¤ğ‘™ã¯ã€ç·å’ŒãŒ 1ã€æ·±ã„å±¤ã»ã©å¤§ãããªã‚‹ã‚ˆã†ã«è¨­å®šã—ãŸï¼ˆå¼ 6ï¼‰ã€‚
å¼ 4 ã¯ã€å„å±¤ã®èªå½™åˆ†å¸ƒãŒæ•™å¸«ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹æœ€çµ‚å±¤ã®èªå½™åˆ†å¸ƒã«ä¼¼ã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®é …ã§ã‚ã‚‹ã€‚
æœ¬æ¥ã®è‡ªå·±è’¸ç•™ã®æ–‡è„ˆã§ã¯ã€æœ€çµ‚å±¤ã®èªå½™åˆ†å¸ƒ ğ‘ğ¿ã¨ã® KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’è¨­å®šã™ã‚‹ãŒã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ› ğ‘targetã¨ã® KL ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã¨ã™ã‚‹ã“ã¨ã§ã€æ€§èƒ½åŠ£åŒ–ã‚’é˜²ãã€‚
å¼ 5 ã¯ã€éš£æ¥ã—ãŸéš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®è·é›¢ã‚’è¿‘ã¥ã‘ã‚‹ã“ã¨ã§ã€éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’åæŸã•ã›ã‚‹ã“ã¨ã‚’æ„å›³ã—ã¦ã„ã‚‹ã€‚
Layer 0 Layer 1Layer ğ‘³LM Head  LM Head Layer ğ‘³-1LM Head LM Head  LM Head Layer 0 Layer 1Layer ğ‘³Layer ğ‘³-1LM Head Te ac he r modelğœ»ğŸğœ»ğŸStudent modelğ’‰ğŸğ’’ğŸğ’’ğŸğ’’ğ‘³$ğŸğ’’ğ­ğšğ«ğ ğğ­ğ’’ğ‘³ğ’‰ğ‘³$ğŸğ’‰ğŸğ’‰ğ‘³å›³ 2: è‡ªå·±è’¸ç•™ã®æå¤±é–¢æ•°ğœ = ğœ1+ ğ›¼ğœ2(3)ğœ1=ğ¿î›•ğ‘™=0ğ‘¤ğ‘™KL(ğ‘ğ‘™, ğ‘target)(4)ğœ2=ğ¿î›•ğ‘™=1ğ‘¤ğ‘™||â„ğ‘™âˆ’ â„ğ‘™âˆ’1||2||â„ğ‘™âˆ’1||2(5)ğ‘¤ğ‘™=exp(ğ‘˜ğ‘™)î›ğ¿ğ‘–=0exp(ğ‘˜ğ‘–)(6)ãŸã ã—ã€ğ‘˜ ã¯é‡ã¿ä¿‚æ•°ã§ã‚ã‚Šã€äººæ‰‹ã«ã‚ˆã‚Šæ±ºå®šã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚


4 å®Ÿé¨“

å®Ÿé¨“ã§ã¯ã€é€šå¸¸ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ã€ãã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦è¿½åŠ ã§è‡ªå·±è’¸ç•™ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®åæŸæ€§ã®é•ã„ã‚’èª¿ã¹ã€è‡ªå·±è’¸ç•™ã‚’ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦è¿½åŠ ã§è¡Œã†ã“ã¨ã§ã€åæŸæ€§ãŒæ”¹å–„ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚

4.1 å®Ÿé¨“è¨­å®š

ãƒ¢ãƒ‡ãƒ«ã€‚
Llama3[5]ã® 1B, 3Bï¼ŒQwen2[6]ã® 0.5Bï¼Œ1.5Bï¼Œ3B ã‚’ä½¿ç”¨ã—ãŸã€‚
Qwen2 ã®å®Ÿé¨“çµæœã¯ä»˜éŒ² Aã«è¨˜è¼‰ã—ãŸã€‚
è©•ä¾¡å°ºåº¦ã€‚
éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®åæŸæ€§ã¯ã€ç§»å‹•é‡ï¼ˆå®šç¾© 1ï¼‰ğ·ğ‘™ã®å˜èª¿æ¸›å°‘æ€§ï¼ˆå®šç¾© 2ï¼‰ã§è©•ä¾¡ã™ã‚‹ã€‚
ã“ã®æ™‚ã€éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¸»æˆåˆ†åˆ†æã—ã€åæŸæ€§ã®å®šæ€§çš„ãªåˆ†æã‚‚åˆã‚ã›ã¦å®Ÿæ–½ã™ã‚‹ã€‚
äºˆæ¸¬åˆ†å¸ƒã®æ„å‘³çš„ãªåæŸæ€§ã¯ã€ä¸­é–“å±¤ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’ LM ãƒ˜ãƒƒãƒ‰ã«é€šã—ãŸèªå½™ç©ºé–“ã®åˆ†å¸ƒã‹ã‚‰ã€å„å±¤ã®ã‚¿ã‚¹ã‚¯ã®äºˆæ¸¬æ€§èƒ½ã¨ã€æ¨è«–éç¨‹ã®å¯è¦–åŒ–ã‚ˆã‚Šè©•ä¾¡ã™ã‚‹ã€‚
è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ã«ã¯ 2ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã€‚
é•·æ–‡ã®æœ€å¾Œã®å˜èªã‚’äºˆæ¸¬ã—åºƒç¯„å›²ã®æ–‡è„ˆç†è§£åŠ›ã‚’æ¸¬ã‚‹ LAMBADA[7]ã‹ã‚‰æ¬¡å˜èªäºˆæ¸¬ã®ç²¾åº¦ã€wikipedia ã‹ã‚‰æ§‹ç¯‰ã•ã‚ŒãŸWikitext[8]ã‹ã‚‰ Perplexityï¼ˆPPLï¼‰ã‚’æ¸¬ã‚‹ã€‚
è©•ä¾¡ã«â€• 1705 â€•ã¯ï¼Œlm-evaluation-harness ãƒ©ã‚¤ãƒ–ãƒ©ãƒª[9]ã‚’ä½¿ç”¨ã—ãŸã€‚



4.2 å®Ÿé¨“æ‰‹é †

ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã® Llama-3.2-1B, Llama-3.2-3B ã«å¯¾ã—ã¦ã€Fineweb-Edu [10]ã‹ã‚‰æŠ½å‡ºã—ãŸ 10M ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å¼ 3 ã®æå¤±é–¢æ•°ã§è‡ªå·±è’¸ç•™ã‚’è¡Œã£ãŸãƒ¢ãƒ‡ãƒ«ã‚’SelfDistillationLlama ã¨ã™ã‚‹ã€‚
ãƒãƒƒãƒã‚µã‚¤ã‚º 128, å­¦ç¿’ç‡ã¯ 1e-4ã€ã‚³ã‚µã‚¤ãƒ³ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã€å¼ 3 ã®ä¿‚æ•° ğ›¼ ã¯1, å¼ 6 ã®ä¿‚æ•° ğ‘˜ ã¯ 0.25 ã¨ã—ãŸã€‚


4.3 å®Ÿé¨“çµæœ

4.3.1 éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®åæŸæ€§ã®è©•ä¾¡Llama3.2-1B ã¨ Llama3.2-3B ã¨ã€ãã‚Œãã‚Œã‚’è‡ªå·±è’¸ç•™ã—ãŸéš›ã®å„å±¤ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®ç§»å‹•é‡ï¼ˆå®šç¾© 1ï¼‰ã‚’å›³ 3 ã«ç¤ºã™ã€‚
åæŸæ€§ï¼ˆå®šç¾© 2ï¼‰ã®æŒ‡æ¨™ã§ã¯ã€1B ç´šã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ ğ‘†Llama3.2âˆ’1B= 0.56, ğ‘†SelfLlamaâˆ’1B=0.90 ã«ã€ 3B ç´šã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ ğ‘†Llama3.2âˆ’3B= 0.66,ğ‘†SelfLlamaâˆ’3B= 0.89 ã¸ã¨æ”¹å–„ãŒã¿ã‚‰ã‚ŒãŸã€‚
ç‰¹ã«ã€å›³ 3ã‹ã‚‰ã€æœ€çµ‚å±¤ã§ã®ç§»å‹•é‡ãŒå¤§å¹…ã«æ”¹å–„ã•ã‚ŒãŸã“ã¨ãŒã‚ã‹ã‚‹ã€‚
1B ç´šã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®æœ€çµ‚å±¤ã§ã®ç§»å‹•é‡ã¯ 0.27 ã§ã‚ã‚Šã€å‰ã®å±¤ã‹ã‚‰ç´„ 4.14 å€ã«å¢—åŠ ã—ã¦ã„ã‚‹ã€‚
ä¸€æ–¹ã€SelfDistillationLlama ã§ã¯ã€æœ€çµ‚å±¤ã§ã®ç§»å‹•é‡ãŒ 0.058 ã¾ã§æ¸›å°‘ã—ã€å‰ã®å±¤ã‹ã‚‰ã®å¢—åŠ ã¯ 1.69 å€ã«ç•™ã¾ã‚‹ã€‚
3B ç´šã®ãƒ¢ãƒ‡ãƒ«ã§ã‚‚åŒæ§˜ã®æ”¹å–„ãŒç¢ºèªã§ãã‚‹ã€‚
ã“ã®çµæœã¯ã€è‡ªå·±è’¸ç•™ã«ã‚ˆã‚Šæœ€çµ‚å±¤ã§ã®éå‰°ãªå¤‰åŒ–ãŒæŠ‘åˆ¶ã•ã‚Œã€éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®åæŸæ€§ãŒå‘ä¸Šã—ãŸã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
Llama3.2-1B ã¨ã® SelfDistillationLlama-1B ã®å„å±¤ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¸»æˆåˆ†åˆ†æã«ã‚ˆã£ã¦ 3 æ¬¡å…ƒã«å¯è¦–åŒ–ã—ãŸçµæœã‚’å›³ 4 ã«ç¤ºã™ã€‚
Llama3.2-1B ã¯æœ€çµ‚å±¤ã§çŠ¶æ…‹ãŒå¤§ããå¤‰åŒ–ã™ã‚‹æ§˜å­ãŒç¢ºèªã§ãã‚‹ï¼ˆå›³ 4a)ã€‚
ä¸€æ–¹ã€SelfDistillationLlama-1B ã§ã¯ã€æœ€çµ‚å±¤ä»˜è¿‘ã§éš ã‚ŒçŠ¶æ…‹ãŒåæŸã—ã¦ã„ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ãŠã‚Šï¼ˆå›³ 4bï¼‰ï¼Œå®šæ€§çš„ã«ã‚‚éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®åæŸæ€§ãŒæ”¹å–„ã•ã‚ŒãŸã“ã¨ãŒã‚ã‹ã‚‹ã€‚
4.3.2 æ„å‘³çš„ãªåæŸæ€§ã®è©•ä¾¡å›³ 5 ã«ã€lambada ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹å„å±¤ã®æ¬¡å˜èªäºˆæ¸¬æ€§èƒ½ã®è©•ä¾¡ã€å›³ 6 ã«ã€wikitext ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹ Perplexityï¼ˆPPLï¼‰ã®è©•ä¾¡ã‚’ç¤ºã™ã€‚
1B ç´šã€3B ç´šã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šã‚‚æœ€çµ‚å±¤ä»˜è¿‘ã§æ€§èƒ½å‘ä¸ŠãŒç¢ºèªã•ã‚ŒãŸã€‚
å…·ä½“çš„ã«ã¯ã€1Bç´šã®ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€Llama3.2-1B ã¯æ¬¡å˜èªäºˆæ¸¬ã®æ€§èƒ½ãŒ 15 å±¤ï¼ˆæœ€çµ‚å±¤ï¼‰ã¨ 14 å±¤ã§ç´„ 38%æ¸›å°‘ã—ã¦ã„0 3 6 9 12 15Layer0.10.20.30.4DistanceLlama3.2-1BSelfDistillationLlama-1B(a) Llama1B ç´šãƒ¢ãƒ‡ãƒ«ã€‚
0 3 6 9 12 15 18 21 24 27Layer0.00.10.20.30.4DistanceLlama3.2-3BSelfDistillationLlama-3B(b) Llama3B ç´šãƒ¢ãƒ‡ãƒ«ã€‚
å›³ 3: éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®ç§»å‹•é‡ã®é·ç§»ã€‚
Input(a) Llama3.2-1BInput(b) SelfDistillationLlama-1Bå›³ 4: Llama1B ç´šãƒ¢ãƒ‡ãƒ«ã®å„å±¤ã®éš ã‚ŒçŠ¶æ…‹ã®å¯è¦–åŒ–ã€‚
ã‚‹ã€‚
ä¸€æ–¹ã€SelfDistillationLlama-1B ã¯ã€æ¬¡å˜èªäºˆæ¸¬ã®ç²¾åº¦ãŒ 15 å±¤ã¨ 14 å±¤ã§ã®æ¸›å°‘ç‡ãŒ 9%ã«æŠ‘ãˆã‚‰ã‚Œã¦ãŠã‚Šã€æœ€çµ‚å±¤ã‚’é™¤ãå…¨ã¦ã®å±¤ã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šé«˜ã„ç²¾åº¦ã‚’ç¶­æŒã—ã¦ã„ã‚‹ã€‚
ï¼ˆå›³5aï¼‰æœ€çµ‚å±¤ã®ç²¾åº¦ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸‹å›ã£ãŸè¦å› ã¨ã—ã¦ã¯ã€å°‘æ•°ã®ãƒˆãƒ¼ã‚¯ãƒ³ã§äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå·±è’¸ç•™ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ãŸã“ã¨ãŒå½±éŸ¿ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚
ã¾ãŸã€wikitext ã® PPL ã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šã‚‚ä¸€è²«ã—ã¦å°ã•ãªå€¤ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
å›³ 7 ã«ã€Llama1B ç´šã®ãƒ¢ãƒ‡ãƒ«ã«æ–‡ç« ã‚’ä¸ãˆãŸéš›ã®å„å±¤ã®æ¨è«–éç¨‹ã®å¯è¦–åŒ–ãŠã‚ˆã³éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®ç§»å‹•é‡ã‚’ç¤ºã™ã€‚
Llama3.2-1B ã§ã¯ã€å±¤ãŒæ·±ããªã‚‹ã«ã¤ã‚Œã¦å˜èªã®æ„å‘³çš„ãªåæŸæ€§ãŒç¢ºèªã§ãã‚‹ä¸€æ–¹ã§ã€æœ€çµ‚å±¤ã«ãŠã„ã¦éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ãŒå¤§ããå¤‰åŒ–ã—ã¦ãŠã‚Šã€æ„å‘³çš„åæŸæ€§ã¨éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®æ•°å€¤çš„åæŸæ€§ã®é–“ã«ä¹–é›¢ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚
ã“ã‚Œã«å¯¾ã—ã€è‡ªå·±è’¸ç•™ã‚’é©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã¯ã€éš ã‚ŒçŠ¶æ…‹ã®æ•°å€¤çš„ãªåæŸæ€§ã‚‚ç¢ºèªã§ãã‚‹ã€‚


5 é–¢é€£ç ”ç©¶

Transformer ã®åå¾©æ¨è«–ã€‚
Transformer ã®æœ€çµ‚å±¤ã®è¡¨ç¾ã¯ã€æµ…ã„å±¤ã‹ã‚‰æ·±ã„å±¤ã«ã‹ã‘ã¦ã®éš ã‚ŒçŠ¶æ…‹ã‹ã‚‰æ®µéšçš„ã«æ§‹ç¯‰ã•ã‚Œã¦ã„ã‚‹[1, 2, 11]. ã“ã‚Œã¯ã€ResNetsâ€• 1706 â€•0 3 6 9 12 15Layer0.00.20.40.6AccuracyLlama3.2-1BSelfDistillationLlama-1B(a) Llama1B ç´šãƒ¢ãƒ‡ãƒ«ã€‚
0 3 6 9 12 15 18 21 24 27Layer0.00.20.40.6AccuracyLlama3.2-3BSelfDistillationLlama-3B(b) Llama3B ç´šãƒ¢ãƒ‡ãƒ«ã€‚
å›³ 5: Lambada ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹æ¬¡å˜èªäºˆæ¸¬ã®æ€§èƒ½ï¼0 3 6 9 12 15Layer10410810121016PerplexityLlama3.2-1BSelfDistillationLlama-1B(a) Llama1B ç´šãƒ¢ãƒ‡ãƒ«ã€‚
0 3 6 9 12 15 18 21 24 27Layer10410810121016PerplexityLlama3.2-3BSelfDistillationLlama-3B(b) Llama3B ç´šãƒ¢ãƒ‡ãƒ«ã€‚
å›³ 6: wikitext ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹ Perplexityï¼ã®æ®‹å·®çµåˆãŒã€è‡ªç„¶ã«éš ã‚ŒçŠ¶æ…‹ã®åå¾©çš„ãªæ›´æ–°ã‚’ä¿ƒã™[12, 13]ã¨ã„ã†çµæœãŒ Transformer ã§ã‚‚é©ç”¨å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚
ã“ã®åå¾©æ¨è«–ã¯ã€å±¤ã‚’å‰Šé™¤ã™ã‚‹ã¨ã€å¾Œç¶šã®å±¤ãŒæŒ™å‹•ã‚’å¤‰åŒ–ã•ã›ã¦è‡ªå·±ä¿®å¾©ã™ã‚‹è¦³ç‚¹ã‹ã‚‰ã‚‚æ”¯æŒã•ã‚Œã¦ã„ã‚‹[14]ã€‚
ãƒ¢ãƒ‡ãƒ«ã®å†—é•·æ€§ã€‚
Transformer ã®ä¸­é–“å±¤ãŒã€å…¥åŠ›ã«è¿‘ã„åˆæœŸå±¤ã‚„å‡ºåŠ›ã«è¿‘ã„æœ€çµ‚å±¤ä»˜è¿‘ã®å±¤ã«æ¯”ã¹ã¦æ€§èƒ½ã«å½±éŸ¿ã—ãªã„ã“ã¨ã‚’ã€å±¤ã®å‰Šé™¤ã‚„å…¥ã‚Œæ›¿ãˆã«ã‚ˆã£ã¦å®Ÿé¨“çš„ã«ç¤ºã—ã¦ã„ã‚‹[15,
16, 3]ã€‚
ã¾ãŸã€Pruning ã«ã‚ˆã£ã¦ã€BERT ã®ç´„ 85%ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå†—é•·ã§ã‚ã‚‹ã“ã¨[17]ã‚„ã€66B ç´šã®ãƒ¢ãƒ‡ãƒ«ã§æ€§èƒ½ã®åŠ£åŒ–ã‚’æœ€å°ã«æŠ‘ãˆãªãŒã‚‰ã€Attention head ã® 70%, Feed Forward å±¤ã® 20%ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã§ãã‚‹ã“ã¨[18]ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ã€‚
Transformer ã«å†—é•·æ€§ãŒç”Ÿã¾ã‚Œã‚‹ã“ã¨ã‚’æŠ‘åˆ¶ã™ã‚‹ãŸã‚ã«ã€äº‹å‰å­¦ç¿’æ™‚ã«å±¤ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã§å‰Šé™¤ã™ã‚‹ LayerDrop [19, 20]ã‚„ã€å±¤ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹LayerShuï¬„e [21]ãªã©ã®æ‰‹æ³•ãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹ã€‚
æ—©æœŸçµ‚äº†ï¼ˆearly exitï¼‰ã€‚
ä¸­é–“å±¤ã®éš ã‚ŒçŠ¶æ…‹ã‹ã‚‰æœ€çµ‚å±¤ã®è¡¨ç¾ã‚’ç›´æ¥äºˆæ¸¬ã™ã‚‹ã“ã¨ã§æ—©æœŸçµ‚äº†ã‚’å¯èƒ½ã« quietly  at  the quietly  at  Levine quietly -at  latter quiet /by frequency quiet near ä½™ quietly  amidst è· quietly tracted  obscuredubu  nearby  upper tucked ached ucked sitting  nearby  nearby beneath  nearby  nearby beneath top tree beneath  tops  top atop  the  top on  the  top on  the  top at  the  top1412108642000.20.40.60.81ProbabilityInput TokensLayers(a) Llama3.2-1B quietly  the  same quiet  the  top,  the  edges quiet  the  edges on  the  tree in  the 
center on  the  top in  the  top in  the  top in  the  edge on  the  top in the  top on  the  top on  the  top on  the  top on  the  top at  the  top141210864200.10.20.30.40.50.60.70.80.9ProbabilityInput TokensLayers(b) SelfDistillationLlama-1B0 3 6 9 12 150.00.2Distance(c) at0 3 6 9 12 150.00.2Distance(d) the0 3 6 9 12 150.10.2Distance(e) topå›³ 7: Llama1B ã®ãƒ¢ãƒ‡ãƒ«ã«ã€ŒThe curious cat climbed upthe tall tree, carefully balancing on each thin branch, untilit suddenly noticed a small bird sitting quietly at the top.ã€ã®æœ€å¾Œã® 3 ãƒˆãƒ¼ã‚¯ãƒ³ã§ã‚ã‚‹ã€Œat the topã€ã‚’äºˆæ¸¬ã•ã›ãŸéš›ã®æ¨è«–éç¨‹ã®å¯è¦–åŒ–ã¨å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹éš›ã®éš ã‚ŒçŠ¶æ…‹ã®ç§»å‹•é‡ã€‚
ã™ã‚‹æ‰‹æ³•ãŒææ¡ˆã•ã‚Œã¦ãŠã‚Šã€æ¨è«–ã®é«˜é€ŸåŒ–ãŒå¯èƒ½ã§ã‚ã‚‹ã€‚
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¸­é–“å±¤ã®éš ã‚ŒçŠ¶æ…‹ãŒæœ€çµ‚å±¤ã®è¡¨ç¾ã«è¿‘ã¥ãã‚ˆã†ã«ç¶™ç¶šå­¦ç¿’ã•ã›ã‚‹æ‰‹æ³•[22, 23]ã‚„ã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å¤–éƒ¨ã«å­¦ç¿’å¯èƒ½ãªç·šå½¢å±¤ãªã©ã‚’é€šã˜ã¦ä¸­é–“å±¤ã®éš ã‚ŒçŠ¶æ…‹ã‹ã‚‰æœ€çµ‚å±¤ã®è¡¨ç¾ã‚’äºˆæ¸¬ã™ã‚‹æ‰‹æ³•[24, 25]ãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹ã€‚



6 ãŠã‚ã‚Šã«

æœ¬ç ”ç©¶ã§ã¯ã€Llama ã‚„ Qwen ã¨ã„ã£ãŸäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ãŒæœ€çµ‚å±¤ã§å¤§ããå¤‰åŒ–ã—ã€æ•°å€¤çš„åæŸæ€§ã‚’æŒãŸãªã„ã¨ã„ã†èª²é¡Œã«ç€ç›®ã—ãŸã€‚
ã“ã®èª²é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã€æ·±ã„å±¤ã‹ã‚‰æµ…ã„å±¤ã¸çŸ¥è­˜ã‚’è’¸ç•™ã™ã‚‹è‡ªå·±è’¸ç•™ã‚’é©ç”¨ã—ã€éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®æ•°å€¤çš„åæŸæ€§ãŒæ”¹å–„ã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚
ä»Šå¾Œã®èª²é¡Œã¨ã—ã¦ã€åæŸæ€§ã‚’é«˜ã‚ãŸå¾Œã®å±¤å‰Šé™¤ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®è»½é‡åŒ–ã‚’æ¤œè¨ã™ã‚‹ã€‚
ã¾ãŸã€éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ãŒåæŸæ€§ã‚’æŒã¤ã‚ˆã†ãªæ–°ã—ã„äº‹å‰å­¦ç¿’æ‰‹æ³•ã®é–‹ç™ºã«ã‚‚å–ã‚Šçµ„ã¿ãŸã„ã€‚
æœ¬ç ”ç©¶ã§å¾—ã‚‰ã‚ŒãŸçŸ¥è¦‹ãŒã€ã‚ˆã‚Šè³ªã®é«˜ã„ LLMé–‹ç™ºæŠ€è¡“ã¸ã®ä¸€åŠ©ã¨ãªã‚‹ã“ã¨ã‚’æœŸå¾…ã™ã‚‹ã€‚
â€• 1707 â€•



è¬è¾

æœ¬ç ”ç©¶ã¯ã€JST ãƒ ãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆå‹ç ”ç©¶é–‹ç™ºäº‹æ¥­JPMJMS2011-35 (fundamental research)ã€ãŠã‚ˆã³ã€æ–‡éƒ¨ç§‘å­¦çœã®è£œåŠ©äº‹æ¥­ã€Œç”Ÿæˆ AI ãƒ¢ãƒ‡ãƒ«ã®é€æ˜æ€§ãƒ»ä¿¡é ¼æ€§ã®ç¢ºä¿ã«å‘ã‘ãŸç ”ç©¶é–‹ç™ºæ‹ ç‚¹å½¢æˆã€ã®æ”¯æ´ã‚’å—ã‘ãŸã‚‚ã®ã§ã™ã€‚ã¾ãŸã€æœ¬ç ”ç©¶ã¯ä¹å·å¤§å­¦æƒ…å ±åŸºç›¤ç ”ç©¶é–‹ç™ºã‚»ãƒ³ã‚¿ãƒ¼ç ”ç©¶ç”¨è¨ˆç®—æ©Ÿã‚·ã‚¹ãƒ†ãƒ ã®ä¸€èˆ¬åˆ©ç”¨ã‚’åˆ©ç”¨ã—ãŸã‚‚ã®ã§ã™ã€‚æœ¬ç ”ç©¶æˆæœã®ä¸€éƒ¨ã¯ã€ãƒ‡ãƒ¼ã‚¿æ´»ç”¨ç¤¾ä¼šå‰µæˆãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  mdx[26]ã‚’åˆ©ç”¨ã—ã¦å¾—ã‚‰ã‚ŒãŸã‚‚ã®ã§ã™ã€‚

å‚è€ƒæ–‡çŒ®


[1] Nostalgebraist. Interpreting gpt: The logit lens. https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt- the-logit- lens, 2020.
[2] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Bider-man, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. CoRR, Vol.abs/2303.08112, , 2023.
[3] Qi Sun, Marc Pickett, Aakash Kumar Nain, and Llion Jones. Transformer layers as painters. CoRR, Vol.abs/2407.09298, , 2024.
[4] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your ownteacher: Improve the performance of convolutional neural networks via self distillation. In 2019 IEEE/CVFInternational Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -November 2, 2019, pp. 3712â€“3721. IEEE, 2019.
[5] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, AurÃ©lienRodriguez, Austen Gregerson, Ava Spataru, Baptiste RoziÃ¨re, Bethany Biron, Binh Tang, Bobbie Chern,Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Chr istopheTouret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, DanielSong, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano,Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric MichaelSmith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail,GrÃ©goire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron,Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, JaewonLee, Jan Geï¬€ert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock,Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, JoannaBitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan VasudenAlwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heaï¬eld, Kevin Stone, and et al. The llama 3 herdof models. CoRR, Vol. abs/2407.21783, , 2024.
[6] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang,Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, JunyangLin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, PengWang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, TianhaoLi, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei,Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, ZeyuCui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, Vol. abs/2407.10671, ,2024.
[7] Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raï¬€aella Bernardi, SandroPezzelle, Marco Baroni, Gemma Boleda, and Raquel FernÃ¡ndez. The LAMBADA dataset: Word predictionrequiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Associationfor Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: LongPapers. The Association for Computer Linguistics, 2016.
[8] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In 5thInternational Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,2017, Conference Track Proceedings. OpenReview.net, 2017.
[9] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPoï¬, Charles Foster, LaurenceGolding, Jeï¬€rey Hsu, Alain Le Noacâ€™h, Haonan Li, Kyle McDonell, Niklas Muennighoï¬€, Chris Ociepa, JasonPhang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, BenWang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 07 2024.
[10] Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the ï¬nest collectionof educational content, 2024.
[11] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feed-forward layers buildpredictions by promoting concepts in the vocabulary space. In Yoav Goldberg, Zornitsa Kozareva, and YueZhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural LanguageProcessing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 30â€“45.Association for Computational Linguistics, 2022.
[12] Klaus Greï¬€, Rupesh Kumar Srivastava, and JÃ¼rgen Schmidhuber. Highway and residual networks learnunrolled iterative estimation. In 5th International Conference on Learning Representations, ICLR2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
[13] Stanislaw Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. Residualconnections encourage iterative inference. In 6th International Conference on Learning Representa-tions, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceed-ings. OpenReview.net, 2018.
[14] Cody Rushing and Neel Nanda. Explorations of self-repair in language models. In Forty-ï¬rst InternationalConference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net,2024.
[15] Vedang Lad, Wes Gurnee, and Max Tegmark. The remarkable robustness of llms: Stages of inference? CoRR,Vol. abs/2406.19384, , 2024.
[16] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and WeipengChen. Shortgpt: Layers in large language models are more redundant than you expect. CoRR, Vol.abs/2403.03853, , 2024.
[17] Fahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy in pretrainedtransformer models. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,November 16-20, 2020, pp. 4908â€“4926. Association for Computational Linguistics, 2020.
[18] Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati, Katrin Kirchhoï¬€, and Dan Roth.Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale.In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023,Toronto, Canada, July 9-14, 2023, pp. 11833â€“11856. Association for Computational Linguistics, 2023.
[19] Minjia Zhang and Yuxiong He. Accelerating training of transformer-based language models with progressivelayer dropping. In Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference onNeural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
[20] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structureddropout. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
[21] Matthias Freiberger, Peter Kun, Anders Sundnes LÃ¸vlie, and Sebastian Risi. Layershuï¬„e: Enhancing robust-ness in vision transformers by randomizing layer execution order. CoRR, Vol. abs/2407.04513, , 2024.
[22] Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, AnasMahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A Aly, Beidi Chen, and Carole-Jean Wu.Layerskip: Enabling early exit inference and self-speculative decoding. In Lun-Wei Ku, Andre Martins, andVivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024,pp. 12622â€“12642. Association for Computational Linguistics, 2024.
[23] Fulai Nan, Jin Wang, and Xuejie Zhang. An on-device machine reading comprehension model with adaptive fastinference. In Wei Lu, Shujian Huang, Yu Hong, and Xiabing Zhou, editors, Natural Language Processingand Chinese Computing - 11th CCF International Conference, NLPCC 2022, Guilin, China,September 24-25, 2022, Proceedings, Part I, Vol. 13551 of Lecture Notes in Computer Science,pp. 850â€“862. Springer, 2022.
[24] Ariel Gera, Roni Friedman, Oï¬r Arviv, Chulaka Gunasekara, Benjamin Sznajder, Noam Slonim, and EyalShnarch. The beneï¬ts of bad advice: Autocontrastive decoding across model layers. In Anna Rogers, Jordan L.Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14,2023, pp. 10406â€“10420. Association for Computational Linguistics, 2023.
[25] Alexander Yom Din, Taelin Karidi, Leshem Choshen, and Mor Geva. Jump to conclusions: Short-cuttingtransformers with linear transformations. In Nicoletta Calzolari, Min-Yen Kan, VÃ©ronique Hoste, AlessandroLenci, Sakriani Sakti, and Nianwen Xue, editors,Proceedings of the 2024 Joint International Confer-ence on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024,20-25 May, 2024, Torino, Italy, pp. 9615â€“9625. ELRA and ICCL, 2024.
[26] Toyotaro Suzumura, Akiyoshi Sugiki, Hiroyuki Takizawa, Akira Imakura, Hiroshi Nakamura, Kenjiro Taura,Tomohiro Kudoh, Toshihiro Hanawa, Yuji Sekiya, Hiroki Kobayashi, Yohei Kuga, Ryo Nakamura, RenheJiang, Junya Kawase, Masatoshi Hanai, Hiroshi Miyazaki, Tsutomu Ishizaki, Daisuke Shimotoku, DaisukeMiyamoto, Kento Aida, Atsuko Takefusa, Takashi Kurimoto, Koji Sasayama, Naoya Kitagawa, Ikki Fujiwara,Yusuke Tanimura, Takayuki Aoki, Toshio Endo, Satoshi Ohshima, Keiichiro Fukazawa, Susumu Date, andToshihiro Uchibayashi. mdx: A cloud platform for supporting data science and cross-disciplinary researchcollaborations. In2022 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, IntlConf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing,Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech),pp. 1â€“7, 2022.â€• 1708 â€•




A Qwen2 ã®å®Ÿé¨“çµæœ

Qwen2 ã® 0.5B, 1.5B, 3B ã§ Llama3 ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã®å®Ÿé¨“ã‚’è¡Œã£ãŸçµæœã‚’å›³ 8, å›³ 9, å›³ 10 ã«ãã‚Œãã‚Œç¤ºã™ã€‚
0 3 6 9 12 15 18 21Layer0.00.20.4DistanceQwen2-0.5BSelfDistillationQwen(a) Qwen2-0.5B0 3 6 9 12 15 18 21 24 27Layer0.00.20.4DistanceQwen2-1.5BSelfDistillationQwen(b) Qwen2-1.5B0 3 6 9 12 15 18 21 24 27 30 33Layer0.00.20.4DistanceQwen2.5-3BSelfDistillationQwen(c) Qwen2-3Bå›³ 8: éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®ç§»å‹•é‡ã®é·ç§»ã€‚
0 3 6 9 12 15 18 21Layer0.00.20.4AccuracyQwen2-0.5BSelfDistillationQwen(a) Qwen2-0.5B0 3 6 9 12 15 18 21 24 27Layer0.00.20.40.6AccuracyQwen2-1.5BSelfDistillationQwen(b) Qwen2-1.5B0 3 6 9 12 15 18 21 24 27 30 33Layer0.00.20.40.6AccuracyQwen2.5-3BSelfDistillationQwen(c) Qwen2-3Bå›³ 9: Lambada ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹æ¬¡å˜èªäºˆæ¸¬ã®æ€§èƒ½ï¼0 3 6 9 12 15 18 21Layer102104106108PerplexityQwen2-0.5BSelfDistillationQwen(a) Qwen2-0.5B0 3 6 9 12 15 18 21 24 27Layer102104106108PerplexityQwen2-1.5BSelfDistillationQwen(b) Qwen2-1.5B0 3 6 9 12 15 18 21 24 27 30 33Layer103106109PerplexityQwen2.5-3BSelfDistillationQwen(c) Qwen2-3Bå›³ 10: wikitext ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹ Perplexityï¼â€• 1709 â€•